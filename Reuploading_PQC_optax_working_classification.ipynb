{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPczKXEpvfaeQdeLtXKvfnS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zakuta/D-QRL/blob/main/Reuploading_PQC_optax_working_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install equinox\n",
        "# !pip install tensorcircuit\n",
        "# !pip install qiskit\n",
        "# !pip install tensorcircuit\n",
        "# !pip install cirq\n",
        "# !pip install openfermion\n",
        "# !pip install gymnax\n",
        "# !pip install brax\n",
        "# !pip install distrax"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aiL423KliZM",
        "outputId": "5062a40b-3cee-4f83-c3b5-9cc272a39630"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: jaxtyping in /usr/local/lib/python3.10/dist-packages (0.2.25)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from jaxtyping) (1.25.2)\n",
            "Requirement already satisfied: typeguard<3,>=2.13.3 in /usr/local/lib/python3.10/dist-packages (from jaxtyping) (2.13.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.1 in /usr/local/lib/python3.10/dist-packages (from jaxtyping) (4.9.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4PKynPtlaNQ",
        "outputId": "61ca9cb7-8a13-4f90-fef2-15685fa1866d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorcircuit.translation:Please first ``pip install -U qiskit`` to enable related functionality in translation module\n"
          ]
        }
      ],
      "source": [
        "import jax\n",
        "from jax import config\n",
        "\n",
        "config.update(\"jax_debug_nans\", True)\n",
        "config.update(\"jax_enable_x64\", True)\n",
        "\n",
        "import jax.numpy as jnp\n",
        "DTYPE=jnp.float64\n",
        "\n",
        "import chex\n",
        "import numpy as np\n",
        "import optax\n",
        "from flax import struct\n",
        "from functools import partial\n",
        "import tensorcircuit as tc\n",
        "\n",
        "import tensorflow as tf\n",
        "from sklearn.decomposition import PCA\n",
        "import equinox as eqx\n",
        "import types\n",
        "from jaxtyping import Array, PRNGKeyArray\n",
        "from typing import Union, Sequence, List, NamedTuple, Optional, Tuple, Any, Literal, TypeVar\n",
        "import jax.tree_util as jtu\n",
        "from gymnax.environments import environment, spaces\n",
        "from brax import envs\n",
        "from brax.envs.wrappers.training import EpisodeWrapper, AutoResetWrapper\n",
        "\n",
        "K = tc.set_backend(\"jax\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "x_train, x_test = x_train[..., np.newaxis] / 255.0, x_test[..., np.newaxis] / 255.0  # normalize the data\n",
        "\n",
        "def filter(x, y, a, b):\n",
        "    keep = (y == a) | (y == b)\n",
        "    x, y = x[keep], y[keep]\n",
        "    y = y == a\n",
        "    return x, y\n",
        "\n",
        "# Filter out classes 0 and 1\n",
        "x_train, y_train = filter(x_train, y_train, 0, 1)\n",
        "x_test, y_test = filter(x_test, y_test, 0, 1)\n",
        "\n",
        "def apply_pca(X, n_components):\n",
        "    X_flat = np.array([x.flatten() for x in X])\n",
        "    pca = PCA(n_components=n_components)\n",
        "    X_pca = pca.fit_transform(X_flat)\n",
        "    return X_pca\n",
        "\n",
        "n_components = 4\n",
        "x_train = apply_pca(x_train, n_components)\n",
        "x_test = apply_pca(x_test, n_components)\n",
        "\n",
        "x_train = jnp.array(x_train, dtype=DTYPE)\n",
        "x_test = jnp.array(x_test, dtype=DTYPE)\n",
        "y_train = jnp.array(y_train, dtype=DTYPE)\n",
        "y_test = jnp.array(y_test, dtype=DTYPE)\n",
        "\n",
        "x_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yd3z2gCGlch-",
        "outputId": "2c668262-7896-4a3b-ae7f-ee1c32f1de28"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-434426a9c6c0>:25: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in array is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
            "  x_train = jnp.array(x_train, dtype=DTYPE)\n",
            "<ipython-input-2-434426a9c6c0>:26: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in array is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
            "  x_test = jnp.array(x_test, dtype=DTYPE)\n",
            "<ipython-input-2-434426a9c6c0>:27: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in array is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
            "  y_train = jnp.array(y_train, dtype=DTYPE)\n",
            "<ipython-input-2-434426a9c6c0>:28: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in array is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
            "  y_test = jnp.array(y_test, dtype=DTYPE)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PQCLayer(eqx.Module):\n",
        "  theta: Array\n",
        "  lmbd: Array\n",
        "  n_qubits: int = eqx.field(static=True)\n",
        "  n_layers: int = eqx.field(static=True)\n",
        "\n",
        "  def __init__(self, n_qubits: int, n_layers: int, key: PRNGKeyArray):\n",
        "    key = jax.random.PRNGKey(key)\n",
        "    tkey, lkey = jax.random.split(key, num=2)\n",
        "    self.n_qubits = n_qubits\n",
        "    self.n_layers = n_layers\n",
        "    # rotation_params\n",
        "    self.theta = jax.random.uniform(key=tkey, shape=(n_layers + 1, n_qubits, 3),\n",
        "                                    minval=0.0, maxval=np.pi, dtype=DTYPE)\n",
        "    # input encoding params\n",
        "    # self.lmbd = jnp.ones(shape=(n_layers, n_qubits))\n",
        "    self.lmbd = jax.random.uniform(key=lkey, shape=(n_layers, n_qubits),\n",
        "                                    minval=0.0, maxval=np.pi, dtype=DTYPE)\n",
        "\n",
        "    self.params = {'thetas': self.theta, 'lmbds': self.lmbd}\n",
        "\n",
        "  def __call__(self, inputs):\n",
        "  # def __call__(self, X, n_qubits, depth):\n",
        "\n",
        "    # circuit = generate_circuit(self.n_qubits, self.n_layers, self.theta, self.lmbd, inputs)\n",
        "    circuit = tc.Circuit(self.n_qubits)\n",
        "\n",
        "    for l in range(self.n_layers):\n",
        "      # variational part\n",
        "      for qubit_idx in range(self.n_qubits):\n",
        "        circuit.rx(qubit_idx, theta=self.params['thetas'][l, qubit_idx, 0])\n",
        "        circuit.ry(qubit_idx, theta=self.params['thetas'][l, qubit_idx, 1])\n",
        "        circuit.rz(qubit_idx, theta=self.params['thetas'][l, qubit_idx, 2])\n",
        "\n",
        "      # entangling part\n",
        "      for qubit_idx in range(self.n_qubits - 1):\n",
        "        circuit.cnot(qubit_idx, qubit_idx + 1)\n",
        "      if self.n_qubits != 2:\n",
        "        circuit.cnot(self.n_qubits - 1, 0)\n",
        "\n",
        "      # encoding part\n",
        "      for qubit_idx in range(self.n_qubits):\n",
        "        linear_input = inputs[qubit_idx] * self.params['lmbds'][l, qubit_idx]\n",
        "        circuit.rx(qubit_idx, theta=linear_input)\n",
        "\n",
        "    # last variational part\n",
        "    for qubit_idx in range(self.n_qubits):\n",
        "      circuit.rx(qubit_idx, theta=self.params['thetas'][self.n_layers, qubit_idx, 0])\n",
        "      circuit.ry(qubit_idx, theta=self.theta[self.n_layers, qubit_idx, 1])\n",
        "      circuit.rz(qubit_idx, theta=self.theta[self.n_layers, qubit_idx, 2])\n",
        "\n",
        "    return jnp.real(circuit.expectation_ps(z=[0,1,2,3]))"
      ],
      "metadata": {
        "id": "TdokXlyklsi-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "re_uploadingpqc = PQCLayer(n_qubits=4, n_layers=5, key=600)\n",
        "t0 = re_uploadingpqc.theta\n",
        "l0 = re_uploadingpqc.lmbd\n",
        "\n",
        "\n",
        "@eqx.filter_value_and_grad\n",
        "def compute_loss(model, x, y):\n",
        "    pred_y = jax.vmap(model)(x)\n",
        "    loss = jnp.maximum(0, 1 - (2.0 * y - 1.0) * pred_y)\n",
        "    return jnp.mean(loss)\n",
        "\n",
        "# @eqx.filter_jit\n",
        "# def make_step(model, x, y, opt_state):\n",
        "#     loss, grads = compute_loss(model, x, y)\n",
        "#     updates, opt_state = optim.update(grads, opt_state)\n",
        "#     model = eqx.apply_updates(model, updates)\n",
        "#     return loss, model, opt_state\n",
        "\n",
        "# optim = optax.adam(1e-2)\n",
        "# # optim = closure_to_pytree(optim)\n",
        "# opt_state = optim.init(re_uploadingpqc)\n",
        "\n",
        "# steps = 200\n",
        "\n",
        "# for step in range(steps):\n",
        "#   batch_idx = np.random.randint(0, len(x_train), 16)\n",
        "#   x_train_batch = np.array([x_train[i] for i in batch_idx])\n",
        "#   y_train_batch = np.array([y_train[i] for i in batch_idx])\n",
        "#   y_train_batch = np.array([int(value) for value in y_train_batch])\n",
        "\n",
        "#   loss, re_uploadingpqc, opt_state = make_step(re_uploadingpqc, x_train_batch, y_train_batch, opt_state)\n",
        "#   loss = loss.item()\n",
        "#   print(f\"step={step}, loss={loss}\")"
      ],
      "metadata": {
        "id": "t_9UnMZPlvq4"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compute_loss(re_uploadingpqc, x_train_batch, y_train_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vq7dNaV1CiZf",
        "outputId": "48e48496-4e76-42d0-aa79-a0699fd76d1d"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Array(0.92853316, dtype=float64),\n",
              " PQCLayer(theta=f64[6,4,3], lmbd=f64[5,4], n_qubits=4, n_layers=5))"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# has_theta = lambda x: hasattr(x, \"theta\")\n",
        "# where_theta = lambda m: tuple(x.theta for x in jax.tree_leaves(m, is_leaf=has_theta) if has_theta(x))\n",
        "# where_theta(params)\n",
        "\n",
        "# param_spec = eqx.tree_at(where_theta, params, replace_fn=lambda _: \"group2\")\n",
        "\n",
        "# has_lmbd = lambda x: hasattr(x, \"lmbd\")\n",
        "# where_lmbd = lambda m: tuple(x.lmbd for x in jax.tree_leaves(m, is_leaf=has_theta) if has_lmbd(x))\n",
        "# where_lmbd(param_spec)\n",
        "\n",
        "# param_spec = eqx.tree_at(where_lmbd, param_spec, replace_fn=lambda _: \"group1\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXwA_JK1n2S5",
        "outputId": "2b52304e-161a-484a-d77a-e46b3d01ac0b"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-61-2fdc672c8ca8>:2: DeprecationWarning: jax.tree_leaves is deprecated: use jax.tree_util.tree_leaves.\n",
            "  where_theta = lambda m: tuple(x.theta for x in jax.tree_leaves(m, is_leaf=has_theta) if has_theta(x))\n",
            "<ipython-input-61-2fdc672c8ca8>:8: DeprecationWarning: jax.tree_leaves is deprecated: use jax.tree_util.tree_leaves.\n",
            "  where_lmbd = lambda m: tuple(x.lmbd for x in jax.tree_leaves(m, is_leaf=has_theta) if has_lmbd(x))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# optim = optax.multi_transform({\"group2\": optax.adam(1e-1),\n",
        "#     \"group1\": optax.adam(1e-0),\n",
        "#     },\n",
        "#     param_spec\n",
        "# )"
      ],
      "metadata": {
        "id": "rjB7w75zs7nE"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eqx.filter(re_uploadingpqc, eqx.is_array)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f23CdX5634Ik",
        "outputId": "b2110aa3-81cf-416f-fc37-7fee8ceda724"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PQCLayer(theta=f64[6,4,3], lmbd=f64[5,4], n_qubits=4, n_layers=5)"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QBtbrs4gIR3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/google-deepmind/optax/issues/577\n",
        "# shamelessly taken from Patrick Kidger's issue from optax github. He's a legend, basically converted the optim instance of optax.optimizer to pytree!\n",
        "\n",
        "def _make_cell(val):\n",
        "    fn = lambda: val\n",
        "    return fn.__closure__[0]  # pyright: ignore\n",
        "\n",
        "\n",
        "def _adjust_function_closure(fn, closure):\n",
        "    out = types.FunctionType(\n",
        "        code=fn.__code__,\n",
        "        globals=fn.__globals__,\n",
        "        name=fn.__name__,\n",
        "        argdefs=fn.__defaults__,\n",
        "        closure=closure,\n",
        "    )\n",
        "    out.__module__ = fn.__module__\n",
        "    out.__qualname__ = fn.__qualname__\n",
        "    out.__doc__ = fn.__doc__\n",
        "    out.__annotations__.update(fn.__annotations__)\n",
        "    if fn.__kwdefaults__ is not None:\n",
        "        out.__kwdefaults__ = fn.__kwdefaults__.copy()\n",
        "    return out\n",
        "\n",
        "\n",
        "# Not a pytree.\n",
        "# Used so that two different local functions, with different identities, can still\n",
        "# compare equal. This is needed as these leaves are compared statically when\n",
        "# filter-jit'ing.\n",
        "class _FunctionWithEquality:\n",
        "    def __init__(self, fn: types.FunctionType):\n",
        "        self.fn = fn\n",
        "\n",
        "    def information(self):\n",
        "        return self.fn.__qualname__, self.fn.__module__\n",
        "\n",
        "    def __hash__(self):\n",
        "        return hash(self.information())\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        return type(self) == type(other) and self.information() == other.information()\n",
        "\n",
        "\n",
        "class _Closure(eqx.Module):\n",
        "    fn: _FunctionWithEquality\n",
        "    contents: Optional[tuple[Any, ...]]\n",
        "\n",
        "    def __init__(self, fn: types.FunctionType):\n",
        "        self.fn = _FunctionWithEquality(fn)\n",
        "        if fn.__closure__ is None:\n",
        "            contents = None\n",
        "        else:\n",
        "            contents = tuple(\n",
        "                closure_to_pytree(cell.cell_contents) for cell in fn.__closure__\n",
        "            )\n",
        "        self.contents = contents\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        if self.contents is None:\n",
        "            closure = None\n",
        "        else:\n",
        "            closure = tuple(_make_cell(contents) for contents in self.contents)\n",
        "        fn = _adjust_function_closure(self.fn.fn, closure)\n",
        "        return fn(*args, **kwargs)\n",
        "\n",
        "\n",
        "def _fixup_closure(leaf):\n",
        "    if isinstance(leaf, types.FunctionType):\n",
        "        return _Closure(leaf)\n",
        "    else:\n",
        "        return leaf\n",
        "\n",
        "\n",
        "def closure_to_pytree(tree):\n",
        "    \"\"\"Convert all function closures into pytree nodes.\n",
        "\n",
        "    **Arguments:**\n",
        "\n",
        "    - `tree`: Any pytree.\n",
        "\n",
        "    **Returns:**\n",
        "\n",
        "    A copy of `tree`, where all function closures have been replaced by a new object\n",
        "    that is (a) callable like the original function, but (b) iterates over its\n",
        "    `__closure__` as subnodes in the pytree.\n",
        "\n",
        "    !!! Example\n",
        "\n",
        "        ```python\n",
        "        def some_fn():\n",
        "            a = jnp.array(1.)\n",
        "\n",
        "            @closure_to_pytree\n",
        "            def f(x):\n",
        "                return x + a\n",
        "\n",
        "            print(jax.tree_util.tree_leaves(f))  # prints out `a`\n",
        "        ```\n",
        "\n",
        "    !!! Warning\n",
        "\n",
        "        One annoying technical detail in the above example: we had to wrap the whole lot\n",
        "        in a `some_fn`, so that we're in a local scope. Python treats functions at the\n",
        "        global scope differently, and this conversion won't result in any global\n",
        "        variable being treated as part of the pytree.\n",
        "\n",
        "        In practice, the intended use case of this function is to fix Optax, which\n",
        "        always uses local functions.\n",
        "    \"\"\"\n",
        "    return jtu.tree_map(_fixup_closure, tree)\n",
        "\n",
        "\n",
        "# EXAMPLE USAGE\n",
        "# lr = jnp.array(1e-3)\n",
        "# optim = optax.chain(\n",
        "#     optax.adam(lr),\n",
        "#     optax.scale_by_schedule(optax.piecewise_constant_schedule(1, {200: 0.1})),\n",
        "# )\n",
        "# optim = closure_to_pytree(optim)\n"
      ],
      "metadata": {
        "id": "oXbgusOqISY0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The saviour of all the demons!\n",
        "# https://github.com/patrick-kidger/equinox/issues/256\n",
        "class PQCLayer(eqx.Module):\n",
        "  theta: Array\n",
        "  lmbd: Array\n",
        "  n_qubits: int = eqx.field(static=True)\n",
        "  n_layers: int = eqx.field(static=True)\n",
        "\n",
        "  def __init__(self, n_qubits: int, n_layers: int, params):\n",
        "    self.n_qubits = n_qubits\n",
        "    self.n_layers = n_layers\n",
        "    # rotation_params\n",
        "    self.theta = params['thetas']\n",
        "    self.lmbd = params['lmbds']\n",
        "\n",
        "  def __call__(self, inputs):\n",
        "    circuit = tc.Circuit(self.n_qubits)\n",
        "\n",
        "    for l in range(self.n_layers):\n",
        "      # variational part\n",
        "      for qubit_idx in range(self.n_qubits):\n",
        "        circuit.rx(qubit_idx, theta=self.theta[l, qubit_idx, 0])\n",
        "        circuit.ry(qubit_idx, theta=self.theta[l, qubit_idx, 1])\n",
        "        circuit.rz(qubit_idx, theta=self.theta[l, qubit_idx, 2])\n",
        "\n",
        "      # entangling part\n",
        "      for qubit_idx in range(self.n_qubits - 1):\n",
        "        circuit.cnot(qubit_idx, qubit_idx + 1)\n",
        "      if self.n_qubits != 2:\n",
        "        circuit.cnot(self.n_qubits - 1, 0)\n",
        "\n",
        "      # encoding part\n",
        "      for qubit_idx in range(self.n_qubits):\n",
        "        linear_input = inputs[qubit_idx] * self.lmbd[l, qubit_idx]\n",
        "        circuit.rx(qubit_idx, theta=linear_input)\n",
        "\n",
        "    # last variational part\n",
        "    for qubit_idx in range(self.n_qubits):\n",
        "      circuit.rx(qubit_idx, theta=self.theta[self.n_layers, qubit_idx, 0])\n",
        "      circuit.ry(qubit_idx, theta=self.theta[self.n_layers, qubit_idx, 1])\n",
        "      circuit.rz(qubit_idx, theta=self.theta[self.n_layers, qubit_idx, 2])\n",
        "\n",
        "    return jnp.real(circuit.expectation_ps(z=[0,1,2,3]))"
      ],
      "metadata": {
        "id": "oylCYPi9IShi"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_layers = 5\n",
        "n_qubits = 4\n",
        "key = 42\n",
        "key = jax.random.PRNGKey(key)\n",
        "tkey, lkey = jax.random.split(key, num=2)\n",
        "params = {'thetas': jax.random.uniform(key=tkey, shape=(n_layers + 1, n_qubits, 3),\n",
        "                                    minval=0.0, maxval=np.pi, dtype=DTYPE),\n",
        "          'lmbds': jnp.ones(shape=(n_layers, n_qubits), dtype=DTYPE)}\n",
        "\n",
        "\n",
        "model = PQCLayer(n_qubits=n_qubits,\n",
        "                 n_layers=n_layers,\n",
        "                 params=params)\n",
        "\n",
        "\n",
        "def map_nested_fn(fn):\n",
        "  '''Recursively apply `fn` to the key-value pairs of a nested dict'''\n",
        "  def map_fn(nested_dict):\n",
        "    return {k: (map_fn(v) if isinstance(v, dict) else fn(k, v))\n",
        "            for k, v in nested_dict.items()}\n",
        "  return map_fn\n",
        "\n",
        "# gradients = jax.tree_util.tree_map(jnp.ones_like, params)  # dummy gradients\n",
        "\n",
        "label_fn = map_nested_fn(lambda k, _: k)\n",
        "optim = optax.multi_transform({'thetas': optax.adam(0.01), 'lmbds': optax.adam(0.001)},\n",
        "                           label_fn)\n",
        "\n",
        "# optim = closure_to_pytree(optim)\n",
        "opt_state = optim.init(params)\n",
        "\n",
        "@eqx.filter_value_and_grad\n",
        "def compute_loss(model, x, y):\n",
        "    pred_y = jax.vmap(model)(x)\n",
        "    loss = jnp.maximum(0, 1 - (2.0 * y - 1.0) * pred_y)\n",
        "    return jnp.mean(loss)\n",
        "\n",
        "def apply_updates_to_model(model, new_params):\n",
        "  # this function is specific and works for this example only. one can think of\n",
        "  # generalizing it to work for updating any given attribute/params of the model.\n",
        "\n",
        "  model_new = eqx.tree_at(where=lambda model: model.theta, pytree=model, replace=new_params['thetas'])\n",
        "  model_new = eqx.tree_at(where=lambda model: model.lmbd, pytree=model_new, replace=new_params['lmbds'])\n",
        "\n",
        "  return model_new\n",
        "\n",
        "\n",
        "@eqx.filter_jit\n",
        "def make_step(model, x, y, opt_state, params):\n",
        "    loss, grads = compute_loss(model, x, y)\n",
        "    grads = {'thetas': grads.theta, 'lmbds': grads.lmbd}\n",
        "    updates, opt_state = optim.update(grads, opt_state, params)\n",
        "    new_params = optax.apply_updates(params, updates)\n",
        "    model_new = apply_updates_to_model(model, new_params)\n",
        "    # model_new = eqx.tree_at(where=lambda model: model.theta, pytree=model, replace=new_params['thetas'])\n",
        "    # model_new = eqx.tree_at(where=lambda model: model.lmbd, pytree=model_new, replace=new_params['lmbds'])\n",
        "    # model = eqx.apply_updates(model, updates)\n",
        "    return loss, model_new, opt_state\n",
        "\n",
        "for step in range(100):\n",
        "  batch_idx = np.random.randint(0, len(x_train), 32)\n",
        "  x_train_batch = np.array([x_train[i] for i in batch_idx])\n",
        "  y_train_batch = np.array([y_train[i] for i in batch_idx])\n",
        "  y_train_batch = np.array([int(value) for value in y_train_batch])\n",
        "\n",
        "  loss, model, opt_state = make_step(model, x_train_batch, y_train_batch, opt_state, params)\n",
        "  loss = loss.item()\n",
        "  print(f\"step={step}, loss={loss}\")\n",
        "\n",
        "# opt_init, opt_update = optax.adam(0.01)\n",
        "# opt_state = opt_init(eqx.filter(model, eqx.is_array))\n",
        "# flat_model, treedef_model = jtu.tree_flatten(model)\n",
        "# flat_opt_state, treedef_opt_state = jtu.tree_flatten(opt_state)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfXNLe7FIyct",
        "outputId": "0157ffab-42f6-4c83-c4ae-afdd04450103"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-fa4577a7bb2b>:6: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'>  is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
            "  params = {'thetas': jax.random.uniform(key=tkey, shape=(n_layers + 1, n_qubits, 3),\n",
            "<ipython-input-9-fa4577a7bb2b>:8: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in ones is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
            "  'lmbds': jnp.ones(shape=(n_layers, n_qubits), dtype=DTYPE)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step=0, loss=0.9442745447158813\n",
            "step=1, loss=0.9854795336723328\n",
            "step=2, loss=0.9921755790710449\n",
            "step=3, loss=1.0407124757766724\n",
            "step=4, loss=0.9191699624061584\n",
            "step=5, loss=0.9675570726394653\n",
            "step=6, loss=0.9539644718170166\n",
            "step=7, loss=0.979794442653656\n",
            "step=8, loss=1.0050121545791626\n",
            "step=9, loss=0.9828323721885681\n",
            "step=10, loss=1.0165404081344604\n",
            "step=11, loss=1.0894443988800049\n",
            "step=12, loss=0.9302136898040771\n",
            "step=13, loss=1.0786528587341309\n",
            "step=14, loss=1.0308709144592285\n",
            "step=15, loss=1.008302927017212\n",
            "step=16, loss=0.9381150007247925\n",
            "step=17, loss=1.0086443424224854\n",
            "step=18, loss=0.9673950672149658\n",
            "step=19, loss=0.9813898801803589\n",
            "step=20, loss=1.079079270362854\n",
            "step=21, loss=0.9439525604248047\n",
            "step=22, loss=0.9594672918319702\n",
            "step=23, loss=1.040024995803833\n",
            "step=24, loss=1.0713002681732178\n",
            "step=25, loss=0.9770899415016174\n",
            "step=26, loss=0.9704107642173767\n",
            "step=27, loss=0.9381617903709412\n",
            "step=28, loss=0.9306100010871887\n",
            "step=29, loss=1.0057768821716309\n",
            "step=30, loss=0.9919536709785461\n",
            "step=31, loss=1.044974684715271\n",
            "step=32, loss=1.0463043451309204\n",
            "step=33, loss=1.0792347192764282\n",
            "step=34, loss=1.0003019571304321\n",
            "step=35, loss=0.924187183380127\n",
            "step=36, loss=1.0204699039459229\n",
            "step=37, loss=1.032371163368225\n",
            "step=38, loss=1.0421943664550781\n",
            "step=39, loss=1.0752007961273193\n",
            "step=40, loss=1.016180157661438\n",
            "step=41, loss=0.9889070987701416\n",
            "step=42, loss=1.0859085321426392\n",
            "step=43, loss=1.0675522089004517\n",
            "step=44, loss=0.98185133934021\n",
            "step=45, loss=0.9879166483879089\n",
            "step=46, loss=1.0288187265396118\n",
            "step=47, loss=0.9637927412986755\n",
            "step=48, loss=1.0089755058288574\n",
            "step=49, loss=1.0273058414459229\n",
            "step=50, loss=0.9688356518745422\n",
            "step=51, loss=1.0780715942382812\n",
            "step=52, loss=0.9632506966590881\n",
            "step=53, loss=1.013346791267395\n",
            "step=54, loss=1.0722014904022217\n",
            "step=55, loss=0.9775784015655518\n",
            "step=56, loss=1.0418384075164795\n",
            "step=57, loss=1.0169222354888916\n",
            "step=58, loss=0.8972269892692566\n",
            "step=59, loss=0.973052442073822\n",
            "step=60, loss=0.9548228979110718\n",
            "step=61, loss=1.0035533905029297\n",
            "step=62, loss=0.9543237090110779\n",
            "step=63, loss=1.0173513889312744\n",
            "step=64, loss=0.9961672425270081\n",
            "step=65, loss=0.9417219758033752\n",
            "step=66, loss=1.0201374292373657\n",
            "step=67, loss=1.0623691082000732\n",
            "step=68, loss=0.8934583067893982\n",
            "step=69, loss=1.0170940160751343\n",
            "step=70, loss=1.0108121633529663\n",
            "step=71, loss=1.035662055015564\n",
            "step=72, loss=0.9868357181549072\n",
            "step=73, loss=0.9844272136688232\n",
            "step=74, loss=1.0348098278045654\n",
            "step=75, loss=1.0312391519546509\n",
            "step=76, loss=0.980674147605896\n",
            "step=77, loss=1.0235012769699097\n",
            "step=78, loss=0.951575517654419\n",
            "step=79, loss=0.9982765316963196\n",
            "step=80, loss=1.0767472982406616\n",
            "step=81, loss=1.007620096206665\n",
            "step=82, loss=0.9943873882293701\n",
            "step=83, loss=1.0555440187454224\n",
            "step=84, loss=0.958530604839325\n",
            "step=85, loss=1.0525468587875366\n",
            "step=86, loss=0.9785583019256592\n",
            "step=87, loss=0.9902611374855042\n",
            "step=88, loss=0.9781327843666077\n",
            "step=89, loss=0.9319009184837341\n",
            "step=90, loss=1.0470961332321167\n",
            "step=91, loss=1.042304515838623\n",
            "step=92, loss=1.0059478282928467\n",
            "step=93, loss=1.0150638818740845\n",
            "step=94, loss=1.0573208332061768\n",
            "step=95, loss=1.0143041610717773\n",
            "step=96, loss=1.0532805919647217\n",
            "step=97, loss=0.9542887210845947\n",
            "step=98, loss=0.973256528377533\n",
            "step=99, loss=1.0745930671691895\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gradients = jax.tree_util.tree_map(jnp.ones_like, params)  # dummy gradients\n"
      ],
      "metadata": {
        "id": "HqGEGFmCmKl3"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_layers = 5\n",
        "n_qubits = 4\n",
        "key = 42\n",
        "key = jax.random.PRNGKey(key)\n",
        "tkey, lkey = jax.random.split(key, num=2)\n",
        "params = {'thetas': jax.random.uniform(key=tkey, shape=(n_layers + 1, n_qubits, 3),\n",
        "                                    minval=0.0, maxval=np.pi, dtype=DTYPE),\n",
        "          'lmbds': jnp.ones(shape=(n_layers, n_qubits), dtype=DTYPE)}\n",
        "\n",
        "\n",
        "model = PQCLayer(n_qubits=n_qubits,\n",
        "                 n_layers=n_layers,\n",
        "                 params=params)\n",
        "\n",
        "\n",
        "def map_nested_fn(fn):\n",
        "  '''Recursively apply `fn` to the key-value pairs of a nested dict'''\n",
        "  def map_fn(nested_dict):\n",
        "    return {k: (map_fn(v) if isinstance(v, dict) else fn(k, v))\n",
        "            for k, v in nested_dict.items()}\n",
        "  return map_fn\n",
        "\n",
        "# gradients = jax.tree_util.tree_map(jnp.ones_like, params)  # dummy gradients\n",
        "\n",
        "label_fn = map_nested_fn(lambda k, _: k)\n",
        "optim = optax.multi_transform({'thetas': optax.adam(0.01), 'lmbds': optax.adam(0.001)},\n",
        "                           label_fn)\n",
        "opt_state = optim.init(params)\n",
        "# updates, new_state = optim.update(gradients, opt_state, params)\n",
        "# new_params = optax.apply_updates(params, updates)\n",
        "\n",
        "@eqx.filter_value_and_grad\n",
        "def compute_loss(model, x, y):\n",
        "    pred_y = jax.vmap(model)(x)\n",
        "    loss = jnp.maximum(0, 1 - (2.0 * y - 1.0) * pred_y)\n",
        "    return jnp.mean(loss)\n",
        "\n",
        "\n",
        "loss, grads = compute_loss(model, x_train[:16], y_train[:16])"
      ],
      "metadata": {
        "id": "AOvk_mEblny7"
      },
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grads = {'thetas': grads.theta, 'lmbds': grads.lmbd}\n",
        "updates, opt_state = optim.update(grads, opt_state, params)\n",
        "new_params = optax.apply_updates(params, updates)"
      ],
      "metadata": {
        "id": "bdNaVSg4qOx5"
      },
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUnr4OgLuvCj",
        "outputId": "c3191bc5-be9e-4d20-9e9e-36eb9173092e"
      },
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'lmbds': Array([[1.001, 1.001, 0.999, 0.999],\n",
              "        [0.999, 1.001, 0.999, 1.001],\n",
              "        [0.999, 0.999, 0.999, 1.001],\n",
              "        [1.001, 1.001, 1.001, 0.999],\n",
              "        [1.001, 0.999, 0.999, 0.999]], dtype=float64),\n",
              " 'thetas': Array([[[0.85259273, 0.93874143, 0.89688467],\n",
              "         [2.67673558, 0.77266714, 1.51313364],\n",
              "         [1.05774572, 2.68004876, 1.01810876],\n",
              "         [2.46738272, 2.01423164, 1.70877908]],\n",
              " \n",
              "        [[2.67148045, 0.40126361, 0.21946644],\n",
              "         [0.69720534, 0.3797383 , 1.58548773],\n",
              "         [0.05340149, 1.71885032, 2.6753669 ],\n",
              "         [3.07993527, 1.5659381 , 1.79930547]],\n",
              " \n",
              "        [[0.80809018, 2.52548216, 3.07320156],\n",
              "         [0.65879265, 1.52784092, 2.44956667],\n",
              "         [2.24907099, 2.23584914, 2.14072183],\n",
              "         [2.83810737, 2.65760658, 2.58974501]],\n",
              " \n",
              "        [[0.66692702, 0.97194887, 2.8846463 ],\n",
              "         [1.55491374, 1.60811205, 0.94195518],\n",
              "         [1.649293  , 1.69987386, 1.80819792],\n",
              "         [2.20201573, 0.96468078, 1.84464712]],\n",
              " \n",
              "        [[2.72668026, 1.02779524, 2.33367556],\n",
              "         [2.84988657, 1.67392737, 1.56174784],\n",
              "         [2.07979209, 2.65964128, 0.53374639],\n",
              "         [0.04565717, 0.22567254, 1.87023176]],\n",
              " \n",
              "        [[0.69780493, 0.16546439, 2.48708459],\n",
              "         [1.86962935, 1.76782177, 2.85030281],\n",
              "         [1.13221602, 2.22558459, 0.38788242],\n",
              "         [2.83405449, 2.85739362, 2.15755409]]], dtype=float64)}"
            ]
          },
          "metadata": {},
          "execution_count": 198
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-UWkyDIxbJZ",
        "outputId": "7a8da362-0975-4060-aeb3-3bfbb719c7e9"
      },
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([[[0.84259273, 0.94874143, 0.90688467],\n",
              "        [2.6667356 , 0.76266714, 1.52313363],\n",
              "        [1.06774572, 2.69004876, 1.00810876],\n",
              "        [2.45738273, 2.02423163, 1.71877908]],\n",
              "\n",
              "       [[2.68148044, 0.39126363, 0.20946645],\n",
              "        [0.70720533, 0.3897383 , 1.57548773],\n",
              "        [0.0434015 , 1.72885032, 2.6853669 ],\n",
              "        [3.06993527, 1.55593812, 1.80930546]],\n",
              "\n",
              "       [[0.79809019, 2.51548217, 3.08320155],\n",
              "        [0.66879265, 1.53784092, 2.43956667],\n",
              "        [2.25907099, 2.24584914, 2.13072184],\n",
              "        [2.84810735, 2.66760658, 2.59974501]],\n",
              "\n",
              "       [[0.67692702, 0.98194887, 2.8746463 ],\n",
              "        [1.54491374, 1.59811206, 0.95195518],\n",
              "        [1.65929299, 1.70987385, 1.79819793],\n",
              "        [2.21201573, 0.95468079, 1.83464712]],\n",
              "\n",
              "       [[2.71668027, 1.03779524, 2.34367556],\n",
              "        [2.83988658, 1.66392737, 1.57174783],\n",
              "        [2.08979208, 2.66964127, 0.54374638],\n",
              "        [0.03565717, 0.21567255, 1.88023176]],\n",
              "\n",
              "       [[0.68780493, 0.15546439, 2.48979877],\n",
              "        [1.87962935, 1.77782177, 2.84671802],\n",
              "        [1.14221602, 2.23558458, 0.38788242],\n",
              "        [2.82405449, 2.84739362, 2.15755409]]], dtype=float64)"
            ]
          },
          "metadata": {},
          "execution_count": 200
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.theta, model.lmbd)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0e4JkC_tfjj",
        "outputId": "8a7efe18-9164-46c2-e89f-412ecec17f4e"
      },
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[0.84259273 0.94874143 0.90688467]\n",
            "  [2.6667356  0.76266714 1.52313363]\n",
            "  [1.06774572 2.69004876 1.00810876]\n",
            "  [2.45738273 2.02423163 1.71877908]]\n",
            "\n",
            " [[2.68148044 0.39126363 0.20946645]\n",
            "  [0.70720533 0.3897383  1.57548773]\n",
            "  [0.0434015  1.72885032 2.6853669 ]\n",
            "  [3.06993527 1.55593812 1.80930546]]\n",
            "\n",
            " [[0.79809019 2.51548217 3.08320155]\n",
            "  [0.66879265 1.53784092 2.43956667]\n",
            "  [2.25907099 2.24584914 2.13072184]\n",
            "  [2.84810735 2.66760658 2.59974501]]\n",
            "\n",
            " [[0.67692702 0.98194887 2.8746463 ]\n",
            "  [1.54491374 1.59811206 0.95195518]\n",
            "  [1.65929299 1.70987385 1.79819793]\n",
            "  [2.21201573 0.95468079 1.83464712]]\n",
            "\n",
            " [[2.71668027 1.03779524 2.34367556]\n",
            "  [2.83988658 1.66392737 1.57174783]\n",
            "  [2.08979208 2.66964127 0.54374638]\n",
            "  [0.03565717 0.21567255 1.88023176]]\n",
            "\n",
            " [[0.68780493 0.15546439 2.48979877]\n",
            "  [1.87962935 1.77782177 2.84671802]\n",
            "  [1.14221602 2.23558458 0.38788242]\n",
            "  [2.82405449 2.84739362 2.15755409]]] [[1. 1. 1. 1.]\n",
            " [1. 1. 1. 1.]\n",
            " [1. 1. 1. 1.]\n",
            " [1. 1. 1. 1.]\n",
            " [1. 1. 1. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "inrv7aXKtLsA"
      },
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_new.theta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1hri-DdyM3e",
        "outputId": "8e7849c7-dd65-4307-c64e-9d9b1107a725"
      },
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([[[0.85259273, 0.93874143, 0.89688467],\n",
              "        [2.67673558, 0.77266714, 1.51313364],\n",
              "        [1.05774572, 2.68004876, 1.01810876],\n",
              "        [2.46738272, 2.01423164, 1.70877908]],\n",
              "\n",
              "       [[2.67148045, 0.40126361, 0.21946644],\n",
              "        [0.69720534, 0.3797383 , 1.58548773],\n",
              "        [0.05340149, 1.71885032, 2.6753669 ],\n",
              "        [3.07993527, 1.5659381 , 1.79930547]],\n",
              "\n",
              "       [[0.80809018, 2.52548216, 3.07320156],\n",
              "        [0.65879265, 1.52784092, 2.44956667],\n",
              "        [2.24907099, 2.23584914, 2.14072183],\n",
              "        [2.83810737, 2.65760658, 2.58974501]],\n",
              "\n",
              "       [[0.66692702, 0.97194887, 2.8846463 ],\n",
              "        [1.55491374, 1.60811205, 0.94195518],\n",
              "        [1.649293  , 1.69987386, 1.80819792],\n",
              "        [2.20201573, 0.96468078, 1.84464712]],\n",
              "\n",
              "       [[2.72668026, 1.02779524, 2.33367556],\n",
              "        [2.84988657, 1.67392737, 1.56174784],\n",
              "        [2.07979209, 2.65964128, 0.53374639],\n",
              "        [0.04565717, 0.22567254, 1.87023176]],\n",
              "\n",
              "       [[0.69780493, 0.16546439, 2.48708459],\n",
              "        [1.86962935, 1.76782177, 2.85030281],\n",
              "        [1.13221602, 2.22558459, 0.38788242],\n",
              "        [2.83405449, 2.85739362, 2.15755409]]], dtype=float64)"
            ]
          },
          "metadata": {},
          "execution_count": 205
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "\n",
        "def update_theta_and_lmbd(model, params):\n",
        "    def update_fn(subtree):\n",
        "        if isinstance(subtree, dict):\n",
        "            # Update 'theta' attribute if present\n",
        "            if 'theta' in subtree:\n",
        "                subtree['theta'] = params['thetas']\n",
        "            # Update 'lmbd' attribute if present\n",
        "            if 'lmbd' in subtree:\n",
        "                subtree['lmbd'] = params['lmbds']\n",
        "        return subtree\n",
        "\n",
        "    return jax.tree_map(update_fn, model)\n",
        "\n",
        "# Example usage:\n",
        "# model_updated = eqx.tree_at(where=lambda model: model.theta, pytree=model, replace=params['thetas'])\n",
        "# model_updated = eqx.tree_at(where=lambda model: model.lmbd, pytree=model_updated, replace=params['lmbd'])\n",
        "model_updated = update_theta_and_lmbd(model, new_params)"
      ],
      "metadata": {
        "id": "t4ydln_VubVi"
      },
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DZ02GplxXKD",
        "outputId": "c5dc3fec-5d98-4784-e276-24de988355b6"
      },
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([[1.001, 1.001, 0.999, 0.999],\n",
              "       [0.999, 1.001, 0.999, 1.001],\n",
              "       [0.999, 0.999, 0.999, 1.001],\n",
              "       [1.001, 1.001, 1.001, 0.999],\n",
              "       [1.001, 0.999, 0.999, 0.999]], dtype=float64)"
            ]
          },
          "metadata": {},
          "execution_count": 187
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "updates"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrhKFOh8rHjV",
        "outputId": "874e0d9a-0b33-45ec-e453-0e796a877e5a"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'lmbds': Array([[ 0.001,  0.001, -0.001, -0.001],\n",
              "        [-0.001,  0.001, -0.001,  0.001],\n",
              "        [-0.001, -0.001, -0.001,  0.001],\n",
              "        [ 0.001,  0.001,  0.001, -0.001],\n",
              "        [ 0.001, -0.001, -0.001, -0.001]], dtype=float64),\n",
              " 'thetas': Array([[[ 0.01      , -0.01      , -0.01      ],\n",
              "         [ 0.00999999,  0.01      , -0.01      ],\n",
              "         [-0.01      , -0.01      ,  0.01      ],\n",
              "         [ 0.01      , -0.01      , -0.01      ]],\n",
              " \n",
              "        [[-0.00999999,  0.00999998,  0.01      ],\n",
              "         [-0.00999999, -0.01      ,  0.01      ],\n",
              "         [ 0.01      , -0.01      , -0.01      ],\n",
              "         [ 0.00999999,  0.00999999, -0.00999999]],\n",
              " \n",
              "        [[ 0.00999999,  0.00999999, -0.01      ],\n",
              "         [-0.01      , -0.01      ,  0.01      ],\n",
              "         [-0.01      , -0.01      ,  0.00999999],\n",
              "         [-0.00999998, -0.01      , -0.01      ]],\n",
              " \n",
              "        [[-0.01      , -0.01      ,  0.01      ],\n",
              "         [ 0.01      ,  0.00999999, -0.01      ],\n",
              "         [-0.00999999, -0.00999999,  0.00999999],\n",
              "         [-0.01      ,  0.01      ,  0.01      ]],\n",
              " \n",
              "        [[ 0.01      , -0.01      , -0.01      ],\n",
              "         [ 0.00999999,  0.01      , -0.01      ],\n",
              "         [-0.01      , -0.01      , -0.00999999],\n",
              "         [ 0.01      ,  0.00999999, -0.01      ]],\n",
              " \n",
              "        [[ 0.01      ,  0.01      , -0.00271418],\n",
              "         [-0.00999999, -0.00999999,  0.00358478],\n",
              "         [-0.00999999, -0.01      , -0.        ],\n",
              "         [ 0.01      ,  0.01      , -0.        ]]], dtype=float64)}"
            ]
          },
          "metadata": {},
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import optax\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n"
      ],
      "metadata": {
        "id": "hl4Q-350V2AN"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# @eqx.filter_jit\n",
        "# def make_step(flat_model, flat_opt_state, x, y):\n",
        "#     model = jtu.tree_unflatten(treedef_model, flat_model)\n",
        "#     opt_state = jtu.tree_unflatten(treedef_opt_state, flat_opt_state)\n",
        "#     loss, grads = compute_loss(model, x, y)\n",
        "#     updates, opt_state = opt_update(grads, opt_state)\n",
        "#     model = eqx.apply_updates(model, updates)\n",
        "#     flat_model = jtu.tree_leaves(model)\n",
        "#     flat_opt_state = jtu.tree_leaves(opt_state)\n",
        "#     return loss, flat_model, flat_opt_state"
      ],
      "metadata": {
        "id": "91JxaNyMLCK-"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_Wk0TrkNB0J",
        "outputId": "6541cfea-6e49-4621-ce01-9d2d00c5e440"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXTx5urzMqkV",
        "outputId": "bfe8eac8-5984-4c37-a471-445f06cd4c41"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9963174666323078\n",
            "0.9857290538749609\n",
            "0.9751737059779213\n",
            "0.9647824093088274\n",
            "0.9547015993555735\n",
            "0.9448562439507416\n",
            "0.935205648239081\n",
            "0.9258598069544338\n",
            "0.9169494841364357\n",
            "0.9085765941426701\n",
            "0.9007932588974266\n",
            "0.8935966540239557\n",
            "0.8869450713482607\n",
            "0.880784429170754\n",
            "0.8750709038856439\n",
            "0.8697800936516723\n",
            "0.8649066040339191\n",
            "0.8604563607789265\n",
            "0.8564367507390658\n",
            "0.8528446979983128\n",
            "0.849653590634601\n",
            "0.8468016393531482\n",
            "0.8441903373571016\n",
            "0.8417000211002499\n",
            "0.8392147946308638\n",
            "0.8366442105739261\n",
            "0.8339308184209852\n",
            "0.8310504618426687\n",
            "0.828005415469505\n",
            "0.824816503001018\n",
            "0.8215147062129982\n",
            "0.8181366654936185\n",
            "0.8147214102472062\n",
            "0.8113072717419417\n",
            "0.8079284209458274\n",
            "0.8046108311032294\n",
            "0.8013690617893784\n",
            "0.7982035903204305\n",
            "0.7951025368397094\n",
            "0.7920460999762872\n",
            "0.7890159481975909\n",
            "0.7860006750016085\n",
            "0.783000384597292\n",
            "0.7800245178242428\n",
            "0.7770877110051126\n",
            "0.7742036987572015\n",
            "0.771378733801539\n",
            "0.7686078784486453\n",
            "0.7658750105492994\n",
            "0.7631551672879043\n",
            "0.7604168338465775\n",
            "0.7576269349031112\n",
            "0.7547511700378333\n",
            "0.7517578736437135\n",
            "0.7486174606512747\n",
            "0.7453041126754057\n",
            "0.741796125223928\n",
            "0.7380743250555436\n",
            "0.7341227025233189\n",
            "0.729928166970242\n",
            "0.7254819264633892\n",
            "0.7207790727970035\n",
            "0.7158212208403274\n",
            "0.710614193849382\n",
            "0.7051714056304723\n",
            "0.6995138016474375\n",
            "0.693672350848797\n",
            "0.6876902495981485\n",
            "0.6816199948498591\n",
            "0.6755212242416844\n",
            "0.6694520612438112\n",
            "0.6634626973051733\n",
            "0.6575903516124235\n",
            "0.6518555091287513\n",
            "0.6462638313668431\n",
            "0.6408096905238538\n",
            "0.6354784895673995\n",
            "0.630256314298041\n",
            "0.6251308383589493\n",
            "0.620097949001938\n",
            "0.6151601277811375\n",
            "0.6103281151733827\n",
            "0.6056199257666691\n",
            "0.6010584434846726\n",
            "0.5966691669398764\n",
            "0.5924705947496501\n",
            "0.5884694609754951\n",
            "0.5846573990608934\n",
            "0.5810127773820423\n",
            "0.5775061996455285\n",
            "0.5741062139119409\n",
            "0.570788290762226\n",
            "0.5675330030754946\n",
            "0.5643243236551061\n",
            "0.5611460312281076\n",
            "0.557977721488433\n",
            "0.5547986203851372\n",
            "0.5515902830012783\n",
            "0.5483401770463678\n",
            "0.5450412922662217\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(y_true, y_pred):\n",
        "    # Computed with uniform distribution\n",
        "\n",
        "    y_true = y_true > 0.0\n",
        "    y_pred = y_pred >= 0.0\n",
        "    result = y_true == y_pred\n",
        "\n",
        "    return jnp.sum(result)/y_true.shape[0]\n",
        "\n",
        "pred_ys = jax.vmap(model)(x_test)\n",
        "# num_correct = jnp.sum((pred_ys > 0.5) == y_test)\n",
        "# final_accuracy = (num_correct / x_test.shape[0]).item()\n",
        "print(f\"final_accuracy={accuracy(y_test, pred_ys)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7pRKkCgMzBN",
        "outputId": "c175ecef-7195-4984-bfdb-9898d6e3406d"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "final_accuracy=0.4915\n"
          ]
        }
      ]
    }
  ]
}