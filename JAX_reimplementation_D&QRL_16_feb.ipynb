{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQA0XwodLlPxyMs03fyHug",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zakuta/D-QRL/blob/main/JAX_reimplementation_D%26QRL_16_feb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# adapted from https://www.tensorflow.org/quantum/tutorials/quantum_data\n",
        "import os\n",
        "from functools import reduce\n",
        "# Set the environment variable\n",
        "# os.environ['XLA_FLAGS'] = '--xla_gpu_cuda_data_dir=/cm/shared/easybuild/AuthenticAMD/software/CUDA/11.8.0/'\n",
        "\n",
        "import collections\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import layers, losses\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import tensorcircuit as tc\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import cirq\n",
        "import sympy\n",
        "\n",
        "np.random.seed(1234)\n",
        "\n",
        "K = tc.set_backend(\"jax\")"
      ],
      "metadata": {
        "id": "HWbAqmjjBcEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install qiskit\n",
        "# !pip install tensorcircuit\n",
        "# !pip install cirqx\n",
        "# !pip install openfermion"
      ],
      "metadata": {
        "id": "a9z7UrHABgyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSx6LbQ76dUh"
      },
      "outputs": [],
      "source": [
        "### circuit_components.py\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def one_qubit_rotation(state, n_qubits, qubit_list, params, return_type='state'):\n",
        "  if state:\n",
        "    c_ = tc.Circuit(n_qubits, inputs=state)\n",
        "  else:\n",
        "    c_ = tc.Circuit(n_qubits)\n",
        "  for qubit_idx in qubit_list:\n",
        "    c_.rx(qubit_idx, theta=params[0])\n",
        "    c_.ry(qubit_idx, theta=params[1])\n",
        "    c_.rz(qubit_idx, theta=params[2])\n",
        "\n",
        "  s_ = c_.state()\n",
        "  if return_type == 'circuit':\n",
        "    return c_\n",
        "  elif return_type == 'state':\n",
        "    return s_\n",
        "\n",
        "def reg_entangling_layer(n_qubits, state, return_type='state'):\n",
        "  qubit_list = np.arange(n_qubits)\n",
        "  if state:\n",
        "    c_ = tc.Circuit(n_qubits, inputs=state)\n",
        "  else:\n",
        "    c_ = tc.Circuit(n_qubits)\n",
        "\n",
        "    for i, j in zip(qubit_list, qubit_list[1:]):\n",
        "      c_.cz(i, j)\n",
        "    if len(qubit_list) != 2:\n",
        "      c_.cz(qubit_list[0], qubit_list[-1])\n",
        "\n",
        "  s_ = c_.state()\n",
        "  if return_type == 'circuit':\n",
        "    return c_\n",
        "  elif return_type == 'state':\n",
        "    return s_\n",
        "\n",
        "def entangling_layer(state, n_qubits, qubit_list, part_of_H_test=True):\n",
        "  # qubit_list = list of indices from the total qubits to add entangling layer to.\n",
        "  # n_qubits = total number of qubits\n",
        "  # part_of_hadamard_test = boolean that indicates whether the layer is part of the hadamard test\n",
        "  c_ = tc.Circuit(n_qubits, inputs=state)\n",
        "  if n_qubits == 2:\n",
        "    c_.cz(0, 1)\n",
        "    return c_\n",
        "  else:\n",
        "    for i in range(len(qubit_list)):\n",
        "      c_.cz(i, (i+1) % len(qubit_list))\n",
        "    return c_\n",
        "\n",
        "def bravyi_ghost_encoding(circuit, n_qubits, bravyi_params, return_type='state'):\n",
        "  qubit_list = np.arange(n_qubits)\n",
        "  # c_ = tc.Circuit(n_qubits)\n",
        "\n",
        "  # ghost encoding to the first qubit of PQC\n",
        "  circuit.crz(qubit_list[0], qubit_list[-1], theta=bravyi_params[0])\n",
        "  # ghost encoding to the last qubit of PQC\n",
        "  circuit.crz(qubit_list[-2], qubit_list[-1], theta=bravyi_params[1])\n",
        "  # apply swap gate to ctrl qubit for switching to another smaller subcircuit\n",
        "  circuit.x(qubit_list[-1])\n",
        "\n",
        "  s_ = circuit.state()\n",
        "  if return_type == 'circuit':\n",
        "    return circuit\n",
        "  elif return_type == 'state':\n",
        "    return s_\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n1 = 4\n",
        "n2 = 5\n",
        "qubit_list = np.arange(n1)\n",
        "\n",
        "def t(n1):\n",
        "  c1 = tc.Circuit(n1)\n",
        "\n",
        "  for qubit_idx in range(3):\n",
        "    c1.rx(qubit_idx, theta=0.1)\n",
        "    c1.ry(qubit_idx, theta=0.01)\n",
        "    c1.rz(qubit_idx, theta=0.0001)\n",
        "\n",
        "  return c1\n",
        "\n",
        "\n",
        "for i in range(len(qubit_list)):\n",
        "  c1.cz(i, (i+1) % len(qubit_list))\n",
        "\n",
        "c2"
      ],
      "metadata": {
        "id": "bNzEbOmuYawP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###### red_partitioned_circuit_gen.py\n",
        "\n",
        "class PartitionedCircuitGenerator():\n",
        "  # assumes that the last qubit of each of the partitioned subcircuit is a ctrl qubit.\n",
        "  def __init__(self, qubit_list_partition, n_layers) -> None:\n",
        "    self.qubit_list_partition = qubit_list_partition # list of qubits where the last qubit is the control qubit (split size)\n",
        "    self.n_qubits_partition = len(qubit_list_partition)\n",
        "    self.n_qubits_wo_ctrl = self.n_qubits_partition - 1 # number of qubits in each partition (without contorl qubit)\n",
        "    self.n_layers = n_layers # number of layers of the partitioned PQC\n",
        "\n",
        "  def generate_layer(self, layer_idx, rotation_params, bravyi_params, input_params):\n",
        "    # this function generates single layer of a partitioned subcircuit.\n",
        "    # the last qubit is indeed the ctrl qubit, hence, the qubit_list will be of length n-1\n",
        "    # n_qubits_partition = len(qubit_list)\n",
        "    qubit_list_wo_ctrl = np.arange(self.n_qubits_wo_ctrl)\n",
        "    c_ = tc.Circuit(self.n_qubits_partition)\n",
        "    s_ = c_.state()\n",
        "    # rotation layer\n",
        "    state_one_q_r = one_qubit_rotation(state=s_,\n",
        "                                       n_qubits=self.n_qubits_partition,\n",
        "                                       qubit_list=qubit_list_wo_ctrl,\n",
        "                                       params=rotation_params,\n",
        "                                       return_type='state')\n",
        "    # entangling layer\n",
        "    circuit = entangling_layer(state=state_one_q_r,\n",
        "                               n_qubits=self.n_qubits_partition,\n",
        "                               qubit_list=qubit_list_wo_ctrl)\n",
        "    # 1st bravyi ghost encoding\n",
        "    circuit = bravyi_ghost_encoding(circuit=circuit,\n",
        "                                    n_qubits=self.n_qubits_partition,\n",
        "                                    bravyi_params=bravyi_params[layer_idx])\n",
        "    # 2nd bravyi ghost encoding -> I assume this has to do with number of cuts in the PQC\n",
        "    # TODO: check my intuition later! @Yash\n",
        "    circuit = bravyi_ghost_encoding(circuit=circuit,\n",
        "                                    n_qubits=self.n_qubits_partition,\n",
        "                                    bravyi_params=bravyi_params[layer_idx + self.n_layers])\n",
        "\n",
        "    # input encoding layer.\n",
        "    # TODO: @Yash, this is just Rx encoding, what about IQP encoding which is actually classically hard to simulate?\n",
        "    # May be then, it would actually require more terms from the cut?\n",
        "    for idx in range(self.n_qubits_wo_ctrl):\n",
        "      circuit.rx(idx, input_params[idx])\n",
        "\n",
        "    return circuit\n",
        "\n",
        "  def generate_partitioned_circuit(self, qubit_list, real=True):\n",
        "\n",
        "    rotation_params = np.zeros(shape=(self.n_layers + 1, self.n_qubits_wo_ctrl, 3))\n",
        "\n",
        "    bravyi_params = np.zeros(shape=(2 * self.n_layers, 2))\n",
        "\n",
        "    input_params = np.zeros(shape=(self.n_layers, self.n_qubits_wo_ctrl))\n",
        "\n",
        "    partitioned_circuit = tc.Circuit(self.n_qubits_partition)\n",
        "\n",
        "    # apply H on control qubit which is situated at the last qubit index\n",
        "    partitioned_circuit.h(qubit_list[-1])\n",
        "    if not real:\n",
        "      partitioned_circuit.unitary(qubit_list[-1], unitary=np.array([[1, 0], [0, 1j]]), name=\"S\")\n",
        "\n",
        "    # apply layers to the partitioned subcircuit\n",
        "    for layer_idx in range(self.n_layers):\n",
        "      layer_for_partitioned_circuit = self.generate_layer(layer_idx=layer_idx,\n",
        "                                                          rotation_params=rotation_params[layer_idx],\n",
        "                                                          bravyi_params=bravyi_params,\n",
        "                                                          input_params=input_params[layer_idx])\n",
        "      partitioned_circuit.append(layer_for_partitioned_circuit)\n",
        "\n",
        "    # add final rotation layer\n",
        "    #TODO: check this one!! the input to params!!! after a small test seems okay to me\n",
        "    one_qubit_rotation_circuit = one_qubit_rotation(n_qubits=self.qubit_list_partition,\n",
        "                                                    qubit_list=qubit_list,\n",
        "                                                    params=rotation_params[-1],\n",
        "                                                    return_type='circuit')\n",
        "\n",
        "    partitioned_circuit.append(one_qubit_rotation_circuit)\n",
        "\n",
        "    # add final H to re-invert the circuit\n",
        "    partitioned_circuit.h(qubit_list[-1])\n",
        "    if not real:\n",
        "      partitioned_circuit.unitary(qubit_list[-1], unitary=np.array([[1, 0], [0, 1j]]), name=\"S\")\n",
        "\n",
        "    return (partitioned_circuit, list(rotation_params.flat()),\n",
        "            list(bravyi_params.flat()), list(input_params.flat()))\n"
      ],
      "metadata": {
        "id": "8ZLXzAf6Nkpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y6nawxPa-GfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# red_partition_layer_gen.py\n",
        "\n",
        "class ReducedPartitionPQCLayer():\n",
        "  def __init__(self,\n",
        "               n_qubits_wo_ctrl,\n",
        "               n_layers,\n",
        "               n_partitions,\n",
        "               n_terms,\n",
        "               input_dim,\n",
        "               trainable_lambdas,\n",
        "               rescaling_scheme,\n",
        "               trainable_regular_weights,\n",
        "               trainable_partition_weights) -> None:\n",
        "\n",
        "    self.n_qubits_wo_ctrl = n_qubits_wo_ctrl\n",
        "    self.n_layers = n_layers\n",
        "    self.n_partitions = n_partitions\n",
        "    self.rescaling_scheme = rescaling_scheme\n",
        "    self.n_terms = n_terms # T in the paper, product of schmidt number squared with gate cuts\n",
        "                            # In our case for the CZ Gate 4 * gate cuts\n",
        "    self.input_dim = input_dim\n",
        "\n",
        "    qubit_list = np.arange(n_qubits_wo_ctrl + 1)\n",
        "    measurement_ops = 3 * np.eye(n_qubits_wo_ctrl + 1)\n",
        "    #TODO: ATTENTION @Yash change to tc???\n",
        "    observables = [reduce((lambda x, y: x*y), measurement_ops)]\n",
        "\n",
        "    # define sub-circuits\n",
        "    generator = PartitionedCircuitGenerator(qubit_list_partition=qubit_list,\n",
        "                                            n_layers=self.n_layers)\n",
        "    circuit, rotation_params, bravyi_params, input_params = generator.generate_partitioned_circuit(qubit_list=qubit_list)\n",
        "    circuit_i, _, _, _ = generator.generate_partitioned_circuit(qubit_list=qubit_list, real=False)\n",
        "\n",
        "    self.reference_circuit = circuit\n",
        "\n",
        "    # initialize weights, use of trainable_regular_weights flag here!\n",
        "    # TODO: @Yash find a feature in tc to check whether this is infact possible w/ JAXBackend.\n",
        "    self.thetas = np.random.uniform(low=0.0, high=np.pi, size=(1, len(rotation_params) * n_partitions))\n",
        "    self.product_term_theta_size = len(rotation_params) # storing the length of rotation params in each subcircuit\n",
        "\n",
        "    # weights to scale the input data (input encodings), use of trainable_regular_weights flag here!\n",
        "    # TODO: @Yash find a feature in tc to check whether this is infact possible w/ JAXBackend.\n",
        "    self.alphas = np.ones(shape=(len(input_params) * n_partitions,))\n",
        "    self.input_data = len(input_params) * n_partitions\n",
        "\n",
        "    # weights of the bravyi encoding, use of trainable_partition_weights flag here!\n",
        "    # TODO: @Yash find a feature in tc to check whether this is infact possible w/ JAXBackend.\n",
        "    self.zetas = np.random.uniform(low=0.0, high=np.pi, size=(n_terms, n_partitions, len(bravyi_params)))\n",
        "\n",
        "    ########## L89 to L96\n",
        "    ## SOME FILLER LINES TO BE ADDED!!\n",
        "    ##########\n",
        "\n",
        "    # TODO: @Yash For now, the if else statement below is degenerate!!\n",
        "    rescale_parameter = 1\n",
        "    if self.rescaling_scheme in ['constant', 'factoring']: # here we do scaling w/o taking exponential\n",
        "      # TODO: @Yash please make sure to make sure that the lambdas in this case are non-negative like Darryn use a non-negative constraint with tf.\n",
        "      # Naively one can do that after applying optimizer updates to lambdas and taking [np.max(0, i) for i in lambdas].\n",
        "      self.lambdas = np.ones(shape=(n_terms,)) * rescale_parameter\n",
        "    else: # exponential factoring\n",
        "      self.lambdas = np.ones(shape=(n_terms,)) * rescale_parameter\n",
        "\n",
        "\n",
        "  def get_zetas(self):\n",
        "    return self.zetas\n",
        "\n",
        "  # TODO: @Yash self.indices need to be defined L89 to L96\n",
        "  def get_indices(self):\n",
        "    return self.indices\n",
        "\n",
        "  def rescale_lambdas(self, inputs):\n",
        "    batch_dim = inputs[0].shape[0]\n",
        "    tiled_up_thetas = np.tile(self.thetas, reps=[batch_dim, 1])\n",
        "\n",
        "    inputs = inputs.reshape((inputs.shape[0], -1))\n",
        "    tiled_up_inputs = np.tile(inputs[0], reps=[1, self.n_layers])\n",
        "    scaled_inputs = np.einsum('i, ji->ji', self.alphas, tiled_up_inputs)\n",
        "    # L127 is degenerate\n",
        "\n",
        "    # simple rescaling of the lambdas\n",
        "    for i in range(self.n_terms):\n",
        "      # for np\n",
        "      self.lambdas[:] = 1 / self.n_terms\n",
        "      #self.lambdas = jax.ops.index_update(self.lambdas, i, 1 / self.n_terms) # for jax.np\n",
        "\n",
        "    print('rescaled lambdas:', self.lambdas)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    batch_dim = inputs[0].shape[0]\n",
        "    tiled_up_thetas = np.tile(self.thetas, reps=[batch_dim, 1])\n",
        "\n",
        "    inputs = inputs.reshape((inputs.shape[0], -1))\n",
        "    tiled_up_inputs = np.tile(inputs[0], reps=[1, self.n_layers])\n",
        "    scaled_inputs = np.einsum('i, ji->ji', self.alphas, tiled_up_inputs)\n",
        "    # TODO: @Yash check if it is neeeded at all\n",
        "    # squashed_inputs = tf.keras.layers.Activation(self.activation)(scaled_inputs)\n",
        "\n",
        "    ans = np.zeros([batch_dim, 1])\n",
        "    for i in range(self.n_terms):\n",
        "      pqc_layer_ans = jax.lax.complex(np.ones([batch_dim, 1]), np.zeros([batch_dim, 1]))\n",
        "      for k in range(self.n_partitions):\n",
        "        # get circuits\n",
        "        tiled_up_circuits\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OdxzZ0RU-Gvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install equinox\n",
        "!pip install tensorcircuit\n",
        "!pip install qiskit\n",
        "!pip install tensorcircuit\n",
        "!pip install cirq\n",
        "!pip install openfermion\n",
        "!pip install gymnax\n",
        "!pip install brax\n",
        "!pip install distrax\n",
        "# !pip install purejaxrl\n"
      ],
      "metadata": {
        "id": "R7zC4bcG_h2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NekMNA824IAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "from jax import config\n",
        "\n",
        "config.update(\"jax_debug_nans\", True)\n",
        "config.update(\"jax_enable_x64\", True)\n",
        "\n",
        "import jax.numpy as jnp\n",
        "DTYPE=jnp.float64\n",
        "\n",
        "import chex\n",
        "import numpy as np\n",
        "import optax\n",
        "from flax import struct\n",
        "from functools import partial\n",
        "import tensorcircuit as tc\n",
        "import equinox as eqx\n",
        "import types\n",
        "from typing import Union, Sequence, List, NamedTuple, Optional, Tuple, Any\n",
        "import jax.tree_util as jtu\n",
        "from gymnax.environments import environment, spaces\n",
        "from brax import envs\n",
        "from brax.envs.wrappers.training import EpisodeWrapper, AutoResetWrapper\n",
        "\n",
        "K = tc.set_backend(\"jax\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQOUMux_4IIC",
        "outputId": "5670fa9b-754d-4bf8-ea9e-c3fc4b0e22f1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorcircuit.translation:Please first ``pip install -U qiskit`` to enable related functionality in translation module\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/google-deepmind/optax/issues/577\n",
        "# shamelessly taken from Patrick Kidger's issue from optax github. He's a legend, basically converted the optim instance of optax.optimizer to pytree!\n",
        "\n",
        "def _make_cell(val):\n",
        "    fn = lambda: val\n",
        "    return fn.__closure__[0]  # pyright: ignore\n",
        "\n",
        "\n",
        "def _adjust_function_closure(fn, closure):\n",
        "    out = types.FunctionType(\n",
        "        code=fn.__code__,\n",
        "        globals=fn.__globals__,\n",
        "        name=fn.__name__,\n",
        "        argdefs=fn.__defaults__,\n",
        "        closure=closure,\n",
        "    )\n",
        "    out.__module__ = fn.__module__\n",
        "    out.__qualname__ = fn.__qualname__\n",
        "    out.__doc__ = fn.__doc__\n",
        "    out.__annotations__.update(fn.__annotations__)\n",
        "    if fn.__kwdefaults__ is not None:\n",
        "        out.__kwdefaults__ = fn.__kwdefaults__.copy()\n",
        "    return out\n",
        "\n",
        "\n",
        "# Not a pytree.\n",
        "# Used so that two different local functions, with different identities, can still\n",
        "# compare equal. This is needed as these leaves are compared statically when\n",
        "# filter-jit'ing.\n",
        "class _FunctionWithEquality:\n",
        "    def __init__(self, fn: types.FunctionType):\n",
        "        self.fn = fn\n",
        "\n",
        "    def information(self):\n",
        "        return self.fn.__qualname__, self.fn.__module__\n",
        "\n",
        "    def __hash__(self):\n",
        "        return hash(self.information())\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        return type(self) == type(other) and self.information() == other.information()\n",
        "\n",
        "\n",
        "class _Closure(eqx.Module):\n",
        "    fn: _FunctionWithEquality\n",
        "    contents: Optional[tuple[Any, ...]]\n",
        "\n",
        "    def __init__(self, fn: types.FunctionType):\n",
        "        self.fn = _FunctionWithEquality(fn)\n",
        "        if fn.__closure__ is None:\n",
        "            contents = None\n",
        "        else:\n",
        "            contents = tuple(\n",
        "                closure_to_pytree(cell.cell_contents) for cell in fn.__closure__\n",
        "            )\n",
        "        self.contents = contents\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        if self.contents is None:\n",
        "            closure = None\n",
        "        else:\n",
        "            closure = tuple(_make_cell(contents) for contents in self.contents)\n",
        "        fn = _adjust_function_closure(self.fn.fn, closure)\n",
        "        return fn(*args, **kwargs)\n",
        "\n",
        "\n",
        "def _fixup_closure(leaf):\n",
        "    if isinstance(leaf, types.FunctionType):\n",
        "        return _Closure(leaf)\n",
        "    else:\n",
        "        return leaf\n",
        "\n",
        "\n",
        "def closure_to_pytree(tree):\n",
        "    \"\"\"Convert all function closures into pytree nodes.\n",
        "\n",
        "    **Arguments:**\n",
        "\n",
        "    - `tree`: Any pytree.\n",
        "\n",
        "    **Returns:**\n",
        "\n",
        "    A copy of `tree`, where all function closures have been replaced by a new object\n",
        "    that is (a) callable like the original function, but (b) iterates over its\n",
        "    `__closure__` as subnodes in the pytree.\n",
        "\n",
        "    !!! Example\n",
        "\n",
        "        ```python\n",
        "        def some_fn():\n",
        "            a = jnp.array(1.)\n",
        "\n",
        "            @closure_to_pytree\n",
        "            def f(x):\n",
        "                return x + a\n",
        "\n",
        "            print(jax.tree_util.tree_leaves(f))  # prints out `a`\n",
        "        ```\n",
        "\n",
        "    !!! Warning\n",
        "\n",
        "        One annoying technical detail in the above example: we had to wrap the whole lot\n",
        "        in a `some_fn`, so that we're in a local scope. Python treats functions at the\n",
        "        global scope differently, and this conversion won't result in any global\n",
        "        variable being treated as part of the pytree.\n",
        "\n",
        "        In practice, the intended use case of this function is to fix Optax, which\n",
        "        always uses local functions.\n",
        "    \"\"\"\n",
        "    return jtu.tree_map(_fixup_closure, tree)\n",
        "\n",
        "\n",
        "# EXAMPLE USAGE\n",
        "# lr = jnp.array(1e-3)\n",
        "# optim = optax.chain(\n",
        "#     optax.adam(lr),\n",
        "#     optax.scale_by_schedule(optax.piecewise_constant_schedule(1, {200: 0.1})),\n",
        "# )\n",
        "# optim = closure_to_pytree(optim)\n"
      ],
      "metadata": {
        "id": "W5nDq62xoPr-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# shamelessly taken from purejaxrl: https://github.com/luchris429/purejaxrl/blob/main/purejaxrl/wrappers.py\n",
        "\n",
        "class GymnaxWrapper(object):\n",
        "    \"\"\"Base class for Gymnax wrappers.\"\"\"\n",
        "\n",
        "    def __init__(self, env):\n",
        "        self._env = env\n",
        "\n",
        "    # provide proxy access to regular attributes of wrapped object\n",
        "    def __getattr__(self, name):\n",
        "        return getattr(self._env, name)\n",
        "\n",
        "\n",
        "class FlattenObservationWrapper(GymnaxWrapper):\n",
        "    \"\"\"Flatten the observations of the environment.\"\"\"\n",
        "\n",
        "    def __init__(self, env: environment.Environment):\n",
        "        super().__init__(env)\n",
        "\n",
        "    def observation_space(self, params) -> spaces.Box:\n",
        "        assert isinstance(\n",
        "            self._env.observation_space(params), spaces.Box\n",
        "        ), \"Only Box spaces are supported for now.\"\n",
        "        return spaces.Box(\n",
        "            low=self._env.observation_space(params).low,\n",
        "            high=self._env.observation_space(params).high,\n",
        "            shape=(np.prod(self._env.observation_space(params).shape),),\n",
        "            dtype=self._env.observation_space(params).dtype,\n",
        "        )\n",
        "\n",
        "    @partial(jax.jit, static_argnums=(0,))\n",
        "    def reset(\n",
        "        self, key: chex.PRNGKey, params: Optional[environment.EnvParams] = None\n",
        "    ) -> Tuple[chex.Array, environment.EnvState]:\n",
        "        obs, state = self._env.reset(key, params)\n",
        "        obs = jnp.reshape(obs, (-1,))\n",
        "        return obs, state\n",
        "\n",
        "    @partial(jax.jit, static_argnums=(0,))\n",
        "    def step(\n",
        "        self,\n",
        "        key: chex.PRNGKey,\n",
        "        state: environment.EnvState,\n",
        "        action: Union[int, float],\n",
        "        params: Optional[environment.EnvParams] = None,\n",
        "    ) -> Tuple[chex.Array, environment.EnvState, float, bool, dict]:\n",
        "        obs, state, reward, done, info = self._env.step(key, state, action, params)\n",
        "        obs = jnp.reshape(obs, (-1,))\n",
        "        return obs, state, reward, done, info\n",
        "\n",
        "\n",
        "@struct.dataclass\n",
        "class LogEnvState:\n",
        "    env_state: environment.EnvState\n",
        "    episode_returns: float\n",
        "    episode_lengths: int\n",
        "    returned_episode_returns: float\n",
        "    returned_episode_lengths: int\n",
        "    timestep: int\n",
        "\n",
        "\n",
        "class LogWrapper(GymnaxWrapper):\n",
        "    \"\"\"Log the episode returns and lengths.\"\"\"\n",
        "\n",
        "    def __init__(self, env: environment.Environment):\n",
        "        super().__init__(env)\n",
        "\n",
        "    @partial(jax.jit, static_argnums=(0,))\n",
        "    def reset(\n",
        "        self, key: chex.PRNGKey, params: Optional[environment.EnvParams] = None\n",
        "    ) -> Tuple[chex.Array, environment.EnvState]:\n",
        "        obs, env_state = self._env.reset(key, params)\n",
        "        state = LogEnvState(env_state, 0, 0, 0, 0, 0)\n",
        "        return obs, state\n",
        "\n",
        "    @partial(jax.jit, static_argnums=(0,))\n",
        "    def step(\n",
        "        self,\n",
        "        key: chex.PRNGKey,\n",
        "        state: environment.EnvState,\n",
        "        action: Union[int, float],\n",
        "        params: Optional[environment.EnvParams] = None,\n",
        "    ) -> Tuple[chex.Array, environment.EnvState, float, bool, dict]:\n",
        "        obs, env_state, reward, done, info = self._env.step(\n",
        "            key, state.env_state, action, params\n",
        "        )\n",
        "        new_episode_return = state.episode_returns + reward\n",
        "        new_episode_length = state.episode_lengths + 1\n",
        "        state = LogEnvState(\n",
        "            env_state=env_state,\n",
        "            episode_returns=new_episode_return * (1 - done),\n",
        "            episode_lengths=new_episode_length * (1 - done),\n",
        "            returned_episode_returns=state.returned_episode_returns * (1 - done)\n",
        "            + new_episode_return * done,\n",
        "            returned_episode_lengths=state.returned_episode_lengths * (1 - done)\n",
        "            + new_episode_length * done,\n",
        "            timestep=state.timestep + 1,\n",
        "        )\n",
        "        info[\"returned_episode_returns\"] = state.returned_episode_returns\n",
        "        info[\"returned_episode_lengths\"] = state.returned_episode_lengths\n",
        "        info[\"timestep\"] = state.timestep\n",
        "        info[\"returned_episode\"] = done\n",
        "        return obs, state, reward, done, info"
      ],
      "metadata": {
        "id": "GqUM-1xh4uHw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_circuit(n_qubits, n_layers, rot_params, input_params, X):\n",
        "  circuit = tc.Circuit(n_qubits)\n",
        "  # params = np.random.normal(size=(n_layers + 1, n_qubits, 3))\n",
        "  # inputs = np.random.normal(size=(n_layers, n_qubits))\n",
        "\n",
        "  for l in range(n_layers):\n",
        "    # variational part\n",
        "    for qubit_idx in range(n_qubits):\n",
        "      circuit.rx(qubit_idx, theta=rot_params[l, qubit_idx, 0])\n",
        "      circuit.ry(qubit_idx, theta=rot_params[l, qubit_idx, 1])\n",
        "      circuit.rz(qubit_idx, theta=rot_params[l, qubit_idx, 2])\n",
        "\n",
        "    # entangling part\n",
        "    for qubit_idx in range(n_qubits - 1):\n",
        "      circuit.cnot(qubit_idx, qubit_idx + 1)\n",
        "    if n_qubits != 2:\n",
        "      circuit.cnot(n_qubits - 1, 0)\n",
        "\n",
        "    # encoding part\n",
        "    for qubit_idx in range(n_qubits):\n",
        "      input = X[qubit_idx] * input_params[l, qubit_idx]\n",
        "      circuit.rx(qubit_idx, theta=input)\n",
        "\n",
        "  # last variational part\n",
        "  for qubit_idx in range(n_qubits):\n",
        "    circuit.rx(qubit_idx, theta=rot_params[n_layers, qubit_idx, 0])\n",
        "    circuit.ry(qubit_idx, theta=rot_params[n_layers, qubit_idx, 1])\n",
        "    circuit.rz(qubit_idx, theta=rot_params[n_layers, qubit_idx, 2])\n",
        "\n",
        "  return circuit\n",
        "\n",
        "\n",
        "class PQCLayer(eqx.Module):\n",
        "  theta: jax.Array = eqx.field(converter=jnp.asarray)\n",
        "  lmbd: jax.Array = eqx.field(converter=jnp.asarray)\n",
        "  n_qubits: int = eqx.field(static=True)\n",
        "  n_layers: int = eqx.field(static=True)\n",
        "\n",
        "  def __init__(self, n_qubits: int, n_layers: int, key: int):\n",
        "    key = jax.random.PRNGKey(key)\n",
        "    tkey, lkey = jax.random.split(key, num=2)\n",
        "    self.n_qubits = n_qubits\n",
        "    self.n_layers = n_layers\n",
        "    # rotation_params\n",
        "    self.theta = jax.random.uniform(key=tkey, shape=(n_layers + 1, n_qubits, 3),\n",
        "                                    minval=0.0, maxval=np.pi)\n",
        "    # input encoding params\n",
        "    # self.lmbd = jnp.ones(shape=(n_layers, n_qubits))\n",
        "    self.lmbd = jax.random.uniform(key=lkey, shape=(n_layers, n_qubits),\n",
        "                                    minval=0.0, maxval=np.pi)\n",
        "\n",
        "  def __call__(self, inputs):\n",
        "  # def __call__(self, X, n_qubits, depth):\n",
        "\n",
        "    circuit = generate_circuit(int(self.n_qubits), int(self.n_layers), self.theta, self.lmbd, inputs)\n",
        "    # state = circuit.state()\n",
        "    # return state\n",
        "    return K.real(circuit.expectation_ps(z=np.arange(int(self.n_qubits))))\n",
        "\n",
        "class Alternating(eqx.Module):\n",
        "  w: jax.Array = eqx.field(converter=jnp.asarray)\n",
        "\n",
        "  def __init__(self, output_dim):\n",
        "    self.w = jnp.array([[(-1.) ** i for i in range(output_dim)]])\n",
        "\n",
        "  def __call__(self, inputs):\n",
        "    return jnp.matmul(inputs, self.w)\n",
        "\n",
        "\n",
        "# class Actor(eqx.Module):\n",
        "#   n_qubits: int\n",
        "#   n_layers: int\n",
        "#   beta: float\n",
        "#   n_actions: Sequence[int]\n",
        "#   key: int\n",
        "\n",
        "#   def __call__(self, x):\n",
        "#     re_uploading_pqc = PQCLayer(n_qubits=self.n_qubits,\n",
        "#                                 n_layers=self.n_layers,\n",
        "#                                 key=self.key)(x)\n",
        "\n",
        "#     process = eqx.nn.Sequential([\n",
        "#         Alternating(self.n_actions),\n",
        "#         eqx.nn.Lambda(lambda x: x * self.beta),\n",
        "#         jax.nn.softmax()\n",
        "#     ])\n",
        "\n",
        "#     policy = process(re_uploading_pqc)\n",
        "\n",
        "#     return policy\n",
        "\n",
        "\n",
        "# the final one which works :)\n",
        "class Actor(eqx.Module):\n",
        "  theta: jax.Array = eqx.field(converter=jnp.asarray)#trainable\n",
        "  lmbd: jax.Array = eqx.field(converter=jnp.asarray)#trainable\n",
        "  w: jax.Array = eqx.field(converter=jnp.asarray)#trainable\n",
        "  n_qubits: int = eqx.field(static=True)\n",
        "  n_layers: int = eqx.field(static=True)\n",
        "  beta: float = eqx.field(static=True)\n",
        "  n_actions: Sequence[int] = eqx.field(static=True)\n",
        "  # key: int\n",
        "\n",
        "  def __init__(self, n_qubits, n_layers, beta, n_actions, key):\n",
        "    self.n_qubits = n_qubits\n",
        "    self.n_layers = n_layers\n",
        "    self.beta = beta\n",
        "    self.n_actions = n_actions\n",
        "    # self.key = key\n",
        "\n",
        "    key = jax.random.PRNGKey(key)\n",
        "    key, _key = jax.random.split(key, num=2)\n",
        "    print(key, _key)\n",
        "    # rotation_params\n",
        "    self.theta = jax.random.uniform(key=_key,\n",
        "                                    shape=(n_layers + 1, n_qubits, 3),\n",
        "                                    minval=0.0, maxval=np.pi)\n",
        "    # input encoding params\n",
        "    self.lmbd = jnp.ones(shape=(n_layers, n_qubits))\n",
        "    # observable weights\n",
        "    self.w = jnp.array([[(-1.) ** i for i in range(n_actions)]])\n",
        "\n",
        "  def re_uploadingpqc(self, inputs):\n",
        "\n",
        "    # circuit = generate_circuit(self.n_qubits, self.n_layers, self.theta, self.lmbd, inputs)\n",
        "    circuit = tc.Circuit(self.n_qubits)\n",
        "\n",
        "    for l in range(self.n_layers):\n",
        "      # variational part\n",
        "      for qubit_idx in range(self.n_qubits):\n",
        "        circuit.rx(qubit_idx, theta=self.theta[l, qubit_idx, 0])\n",
        "        circuit.ry(qubit_idx, theta=self.theta[l, qubit_idx, 1])\n",
        "        circuit.rz(qubit_idx, theta=self.theta[l, qubit_idx, 2])\n",
        "\n",
        "      # entangling part\n",
        "      for qubit_idx in range(self.n_qubits - 1):\n",
        "        circuit.cnot(qubit_idx, qubit_idx + 1)\n",
        "      if self.n_qubits != 2:\n",
        "        circuit.cnot(self.n_qubits - 1, 0)\n",
        "\n",
        "      # encoding part\n",
        "      for qubit_idx in range(self.n_qubits):\n",
        "        linear_input = inputs[qubit_idx] * self.lmbd[l, qubit_idx]\n",
        "        circuit.rx(qubit_idx, theta=linear_input)\n",
        "\n",
        "    # last variational part\n",
        "    for qubit_idx in range(self.n_qubits):\n",
        "      circuit.rx(qubit_idx, theta=self.theta[self.n_layers, qubit_idx, 0])\n",
        "      circuit.ry(qubit_idx, theta=self.theta[self.n_layers, qubit_idx, 1])\n",
        "      circuit.rz(qubit_idx, theta=self.theta[self.n_layers, qubit_idx, 2])\n",
        "\n",
        "    return jnp.real(circuit.expectation_ps(z=jnp.arange(self.n_qubits)))\n",
        "\n",
        "\n",
        "  def alternating(self, inputs):\n",
        "    return jnp.matmul(inputs, self.w)\n",
        "\n",
        "  def get_params(self):\n",
        "    return {\"theta\": self.theta, \"lmbd\": self.lmbd, \"w\": self.w}\n",
        "\n",
        "  def __call__(self, x):\n",
        "\n",
        "    pqc = self.re_uploadingpqc(x)\n",
        "    alt = self.alternating(pqc)\n",
        "\n",
        "    # process = eqx.nn.Sequential([\n",
        "    #     alt,\n",
        "    #     eqx.nn.Lambda(lambda x: x * self.beta),\n",
        "    #     jax.nn.softmax()\n",
        "    # ])\n",
        "    # policy = process(pqc)\n",
        "\n",
        "    actor_mean = eqx.nn.Lambda(lambda x: x * self.beta)(alt)\n",
        "    policy = distrax.Softmax(actor_mean)\n",
        "\n",
        "    return policy\n",
        "\n",
        "\n",
        "# class Actor(eqx.Module):\n",
        "#   n_qubits: int\n",
        "#   n_layers: int\n",
        "#   beta: float\n",
        "#   n_actions: Sequence[int]\n",
        "#   pqc: eqx.Module\n",
        "#   alt: eqx.Module\n",
        "#   key: int\n",
        "\n",
        "#   def __init__(self, n_qubits, n_layers, beta, n_actions, key):\n",
        "#     self.n_qubits = n_qubits\n",
        "#     self.n_layers = n_layers\n",
        "#     self.beta = beta\n",
        "#     self.n_actions = n_actions\n",
        "#     self.key = key\n",
        "\n",
        "#     self.pqc = PQCLayer(n_qubits=self.n_qubits,\n",
        "#                         n_layers=self.n_layers,\n",
        "#                         key=self.key)\n",
        "\n",
        "#     self.alt = Alternating(self.n_actions)\n",
        "\n",
        "#   def __call__(self, x):\n",
        "#     re_uploading_pqc = self.pqc(x)\n",
        "\n",
        "#     process = eqx.nn.Sequential([\n",
        "#         self.alt,\n",
        "#         eqx.nn.Lambda(lambda x: x * self.beta),\n",
        "#         jax.nn.softmax()\n",
        "#     ])\n",
        "\n",
        "#     policy = process(re_uploading_pqc)\n",
        "\n",
        "#     return policy\n",
        "\n",
        "\n",
        "class Transition(NamedTuple):\n",
        "  done: jnp.ndarray\n",
        "  action: jnp.ndarray\n",
        "  value: jnp.ndarray\n",
        "  reward: jnp.ndarray\n",
        "  log_prob: jnp.ndarray\n",
        "  obs: jnp.ndarray\n",
        "  info: jnp.ndarray\n",
        "\n",
        "class TrainState(eqx.Module):\n",
        "    model: eqx.Module\n",
        "    optimizer: optax.GradientTransformation = eqx.field(static=True)\n",
        "    opt_state: optax.OptState\n",
        "\n",
        "    def __init__(self, model, optimizer, opt_state = None):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        if opt_state is None:\n",
        "            self.opt_state = self.optimizer.init(eqx.filter(model, eqx.is_array))\n",
        "        else:\n",
        "            self.opt_state = opt_state\n",
        "\n",
        "    def apply_gradients(self, grads):\n",
        "\n",
        "        updates, opt_state = self.optimizer.update(grads, self.opt_state, self.model)\n",
        "        model = eqx.apply_updates(self.model, updates)\n",
        "        new_train_state = self.__class__(model=model, optimizer=self.optimizer, opt_state=opt_state)\n",
        "        return new_train_state"
      ],
      "metadata": {
        "id": "B0sUXpgDDH_j"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"env_name\": \"CartPole-v1\",\n",
        "          \"n_train_envs\": 1,\n",
        "          \"n_qubits\": 4,\n",
        "          \"n_layers\": 5,\n",
        "          \"max_expisodes\": 1200,\n",
        "          \"batch_size\": 10,\n",
        "          \"agent_name\": \"CP_PG\",\n",
        "          \"gamma\": 1,\n",
        "          \"beta\": 1,\n",
        "          \"lr_in\": 0.1, # input encoding lmbd\n",
        "          \"lr_var\": 0.01, # variational part theta\n",
        "          \"lr_out\": 0.1, # observables Alternating class one\n",
        "          }\n",
        "\n",
        "def make_train(config):\n",
        "\n",
        "  env, env_params = gymnax.make(config[\"env_name\"])\n",
        "  env = FlattenObservationWrapper(env)\n",
        "  env = LogWrapper(env)\n",
        "\n",
        "  def train(rng):\n",
        "\n",
        "    # Initialize network\n",
        "    rng, _rng = jax.random.split(rng)\n",
        "    actor = Actor(config[\"n_qubits\"], config[\"n_layers\"], config[\"beta\"],\n",
        "                    env.action_space(env_params).n, _rng)\n",
        "\n",
        "    # https://github.com/patrick-kidger/equinox/issues/79\n",
        "    param_spec = eqx.filter(actor, eqx.is_inexact_array)\n",
        "    param_spec = eqx.tree_at(lambda actor: actor.theta, param_spec, replace='group0')\n",
        "    param_spec = eqx.tree_at(lambda actor: actor.lmbd, param_spec, replace='group1')\n",
        "    param_spec = eqx.tree_at(lambda actor: actor.w, param_spec, replace='group2')\n",
        "\n",
        "    #TODO: set the learning rates later\n",
        "    optim = optax.multi_transform({\"group0\": optax.adam(1e-2),\n",
        "        \"group1\": optax.adam(1e-1),\n",
        "        \"group2\": optax.adam(1e-6),\n",
        "        },\n",
        "        param_spec\n",
        "    )\n",
        "\n",
        "    optim = closure_to_pytree(optim)\n",
        "\n",
        "    optim_state = optim.init()\n",
        "\n",
        "    # Initialize environment\n",
        "    rng, _rng = jax.random.split(rng)\n",
        "    reset_rng = jax.random.split(_rng, config[\"n_train_envs\"])\n",
        "    obsv, env_state = jax.vmap(env.reset, in_axes=(0, None))(reset_rng, env_params)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "f1s913yQ3Uvs",
        "outputId": "aa08cabc-4c74-4a27-eaa5-4c8350966df8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'gymnax'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-b1084217d2da>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgymnax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gymnax'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "actor = Actor(n_qubits=4, n_layers=5, beta=1.0, n_actions=2, key=42)\n",
        "param_spec = eqx.filter(actor, eqx.is_inexact_array)\n",
        "# # param_spec = jax.tree_map(lambda _: \"NT\", actor)\n",
        "param_spec = eqx.tree_at(lambda actor: actor.theta, param_spec, replace='group0')\n",
        "param_spec = eqx.tree_at(lambda actor: actor.lmbd, param_spec, replace='group1')\n",
        "param_spec = eqx.tree_at(lambda actor: actor.w, param_spec, replace='group2')\n",
        "\n",
        "optim = optax.multi_transform({\"group0\": optax.adam(1e-1),\n",
        "    \"group1\": optax.adam(1e-0),\n",
        "    \"group2\": optax.adam(1e-6),\n",
        "    },\n",
        "    param_spec\n",
        ")\n",
        "\n",
        "\n",
        "# param_spec = jax.tree_map(lambda _: \"group0\", actor)\n",
        "\n",
        "# Set parameter groups\n",
        "# param_spec = eqx.tree_at(lambda actor: actor.theta, param_spec, replace='group1')\n",
        "# param_spec = eqx.tree_at(lambda actor: actor.lmbd, param_spec, replace='group2')\n",
        "# param_spec = eqx.tree_at(lambda actor: actor.w, param_spec, replace='group3')\n",
        "\n",
        "# optim = optax.multi_transform(\n",
        "#     {\"group0\": optax.adam(0.0),\n",
        "#      \"group1\": optax.adam(1e-0),\n",
        "#      \"group2\": optax.adam(1e-6),\n",
        "#      \"group3\": optax.adam(1e-5),\n",
        "#     },\n",
        "#     param_spec\n",
        "# )\n",
        "\n",
        "# optim = closure_to_pytree(optim)\n",
        "\n",
        "opt_state = optim.init(param_spec)\n",
        "\n",
        "# def _update_step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "VR0tFvN4tnSW",
        "outputId": "43a603bc-7d53-4f3c-d3f8-66641e3027a7"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2465931498 3679230171] [255383827 267815257]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "string indices must be integers",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-130-1f51493ce20c>\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# optim = closure_to_pytree(optim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mopt_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# def _update_step()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optax/_src/combine.py\u001b[0m in \u001b[0;36minit_fn\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0minit_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_labels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mparam_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0mlabel_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_leaves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-129-ef3205032ef1>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    161\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m     \u001b[0mpqc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mre_uploadingpqc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m     \u001b[0malt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malternating\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpqc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m<ipython-input-129-ef3205032ef1>\u001b[0m in \u001b[0;36mre_uploadingpqc\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    129\u001b[0m       \u001b[0;31m# variational part\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mqubit_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_qubits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0mcircuit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqubit_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqubit_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0mcircuit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqubit_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqubit_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mcircuit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqubit_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqubit_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "param_spec = eqx.filter(actor, eqx.is_inexact_array)\n",
        "param_spec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IV4p_2iQnLyH",
        "outputId": "89eddf51-2cb7-439d-9259-ea62c9d6c648"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Actor(\n",
              "  theta=f32[6,4,3],\n",
              "  lmbd=f32[5,4],\n",
              "  w=f32[1,2],\n",
              "  n_qubits=4,\n",
              "  n_layers=5,\n",
              "  beta=1.0,\n",
              "  n_actions=2\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "jbAqBPgPk0a2",
        "outputId": "aab8c9ed-0a5e-424d-854e-f3c68057294f"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'str' object cannot be interpreted as an integer",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-83-8391f59af8b9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCircuit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorcircuit/circuit.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, nqubits, inputs, mps_inputs, split)\u001b[0m\n\u001b[1;32m     72\u001b[0m         }\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmps_inputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mnodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_zero_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnqubits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_front\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_edge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# provide input function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorcircuit/basecircuit.py\u001b[0m in \u001b[0;36mall_zero_nodes\u001b[0;34m(n, d, prefix)\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             )\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         ]\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'str' object cannot be interpreted as an integer"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "jax.tree_map(actor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "-8znFvkfhM_2",
        "outputId": "811cdd75-48d3-4986-8ad0-b1804eb62c88"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "tree_map() missing 1 required positional argument: 'tree'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-79-01d3dd3092ad>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: tree_map() missing 1 required positional argument: 'tree'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4A0ZiZ2bGOoe",
        "outputId": "06f451bf-a6b4-491f-b7a0-ceeae6843196"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Actor(\n",
              "   theta=None,\n",
              "   lmbd=None,\n",
              "   w=None,\n",
              "   n_qubits=None,\n",
              "   n_layers=None,\n",
              "   beta=None,\n",
              "   n_actions=None\n",
              " )]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def _update_step(runner_state, unused):\n",
        "    # COLLECT TRAJECTORIES\n",
        "    def _env_step(runner_state, unused):\n",
        "        train_state, env_state, last_obs, rng = runner_state\n",
        "\n",
        "        # SELECT ACTION\n",
        "        rng, _rng = jax.random.split(rng)\n",
        "        pi, value = network.apply(train_state.params, last_obs)\n",
        "        action = pi.sample(seed=_rng)\n",
        "        log_prob = pi.log_prob(action)\n",
        "\n",
        "        # STEP ENV\n",
        "        rng, _rng = jax.random.split(rng)\n",
        "        rng_step = jax.random.split(_rng, config[\"NUM_ENVS\"])\n",
        "        obsv, env_state, reward, done, info = jax.vmap(\n",
        "            env.step, in_axes=(0, 0, 0, None)\n",
        "        )(rng_step, env_state, action, env_params)\n",
        "        transition = Transition(\n",
        "            done, action, value, reward, log_prob, last_obs, info\n",
        "        )\n",
        "        runner_state = (train_state, env_state, obsv, rng)\n",
        "        return runner_state, transition"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "Wf3JWNaOlhvm",
        "outputId": "4f890224-e080-4338-ed65-154af9200afa"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Actor.__call__ of Actor(\n",
              "  theta=f64[6,4,3],\n",
              "  lmbd=f64[5,4],\n",
              "  w=f64[1,2],\n",
              "  n_qubits=4,\n",
              "  n_layers=5,\n",
              "  beta=1.0,\n",
              "  n_actions=2\n",
              ")>"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>Actor.__call__</b><br/>def __call__(x)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/content/&lt;ipython-input-36-e34060e76266&gt;</a>Call self as a function.</pre></div>"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "has_theta = lambda x: hasattr(x, \"theta\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UE3TO_PiixMi",
        "outputId": "5e0d000b-70be-45c2-a052-6ae8e747f473"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import equinox as eqx\n",
        "import jax\n",
        "import jax.random as jr\n",
        "import optax\n",
        "\n",
        "key1, key2 = jr.split(jr.PRNGKey(0))\n",
        "mlp1 = eqx.nn.MLP(2, 2, 2, 2, key=key1)\n",
        "mlp2 = eqx.nn.MLP(2, 2, 2, 2, key=key2)\n",
        "# Example model. In its interaction with `optax.multi_transform`, all that matters\n",
        "# is that it is some PyTree of parameters.\n",
        "model = (mlp1, mlp2)"
      ],
      "metadata": {
        "id": "ja4pyg4IJ8rA"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(eqx.filter(model, eqx.is_inexact_array))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbrTToX8eyd7",
        "outputId": "a57badc6-687d-4b85-dbb0-b196c4982883"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eqx.filter(actor, eqx.is_inexact_array)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpeaTBnhKab8",
        "outputId": "f38758a8-cff4-49fe-9261-b4a49e4f21a5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Actor(n_qubits=None, n_layers=None, beta=None, n_actions=None, key=None)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AAWytGkSEjEs"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "re_uploadingpqc.theta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2h5NJIIOOyp",
        "outputId": "5a159e28-5304-4c7e-9216-c8a04ef714e1"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([[[1.5144717 , 1.9982332 , 0.6312601 ],\n",
              "        [0.18876766, 2.6841137 , 2.5112891 ],\n",
              "        [2.9993846 , 0.79624313, 2.8496668 ],\n",
              "        [0.14933394, 1.7980386 , 0.02239964]],\n",
              "\n",
              "       [[0.9860115 , 1.8502505 , 2.2118742 ],\n",
              "        [3.108764  , 3.0203154 , 1.2054719 ],\n",
              "        [2.1640604 , 1.2004793 , 2.2377698 ],\n",
              "        [1.8221438 , 0.43731636, 1.2987039 ]],\n",
              "\n",
              "       [[2.2190378 , 2.060944  , 1.9459078 ],\n",
              "        [1.8514849 , 0.5357076 , 1.7158291 ],\n",
              "        [1.4256872 , 2.901676  , 0.81299675],\n",
              "        [0.33294687, 2.785755  , 2.9518187 ]],\n",
              "\n",
              "       [[1.2626755 , 0.00724034, 1.9943507 ],\n",
              "        [1.5947442 , 0.10624278, 2.8188238 ],\n",
              "        [1.8898127 , 1.6097226 , 0.54341084],\n",
              "        [2.5614724 , 0.62843066, 1.6441184 ]],\n",
              "\n",
              "       [[1.3262455 , 1.4257475 , 0.2947449 ],\n",
              "        [0.03931874, 2.7925334 , 0.5836108 ],\n",
              "        [0.9254372 , 2.547386  , 1.6627982 ],\n",
              "        [1.6727308 , 2.674259  , 0.26866385]],\n",
              "\n",
              "       [[2.620658  , 2.3397074 , 1.0249314 ],\n",
              "        [2.540169  , 1.4397433 , 1.4750589 ],\n",
              "        [2.0233185 , 0.5248312 , 2.2866712 ],\n",
              "        [1.8359305 , 0.83510125, 2.513384  ]]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_circuit(n_qubits, n_layers, rot_params, input_params, X):\n",
        "  circuit = tc.Circuit(n_qubits)\n",
        "  # params = np.random.normal(size=(n_layers + 1, n_qubits, 3))\n",
        "  # inputs = np.random.normal(size=(n_layers, n_qubits))\n",
        "\n",
        "  for l in range(n_layers):\n",
        "    # variational part\n",
        "    for qubit_idx in range(n_qubits):\n",
        "      circuit.rx(qubit_idx, theta=rot_params[l, qubit_idx, 0])\n",
        "      circuit.ry(qubit_idx, theta=rot_params[l, qubit_idx, 1])\n",
        "      circuit.rz(qubit_idx, theta=rot_params[l, qubit_idx, 2])\n",
        "\n",
        "    # entangling part\n",
        "    for qubit_idx in range(n_qubits - 1):\n",
        "      circuit.cnot(qubit_idx, qubit_idx + 1)\n",
        "    if n_qubits != 2:\n",
        "      circuit.cnot(n_qubits - 1, 0)\n",
        "\n",
        "    # encoding part\n",
        "    for qubit_idx in range(n_qubits):\n",
        "      input = X[qubit_idx] * input_params[l, qubit_idx]\n",
        "      circuit.rx(qubit_idx, theta=input)\n",
        "\n",
        "  # last variational part\n",
        "  for qubit_idx in range(n_qubits):\n",
        "    circuit.rx(qubit_idx, theta=rot_params[n_layers, qubit_idx, 0])\n",
        "    circuit.ry(qubit_idx, theta=rot_params[n_layers, qubit_idx, 1])\n",
        "    circuit.rz(qubit_idx, theta=rot_params[n_layers, qubit_idx, 2])\n",
        "\n",
        "  return circuit\n",
        "\n",
        "\n",
        "class PQCLayer(eqx.Module):\n",
        "  theta: jax.Array\n",
        "  lmbd: jax.Array\n",
        "  n_qubits: int\n",
        "  n_layers: int\n",
        "\n",
        "  def __init__(self, n_qubits: int, n_layers: int, key: int):\n",
        "    key = jax.random.PRNGKey(key)\n",
        "    tkey, lkey = jax.random.split(key, num=2)\n",
        "    self.n_qubits = n_qubits\n",
        "    self.n_layers = n_layers\n",
        "    # rotation_params\n",
        "    self.theta = jax.random.uniform(key=tkey, shape=(n_layers + 1, n_qubits, 3),\n",
        "                                    minval=0.0, maxval=np.pi)\n",
        "    # input encoding params\n",
        "    # self.lmbd = jnp.ones(shape=(n_layers, n_qubits))\n",
        "    self.lmbd = jax.random.uniform(key=lkey, shape=(n_layers, n_qubits),\n",
        "                                    minval=0.0, maxval=np.pi)\n",
        "\n",
        "  def __call__(self, inputs):\n",
        "  # def __call__(self, X, n_qubits, depth):\n",
        "\n",
        "    circuit = generate_circuit(self.n_qubits, self.n_layers, self.theta, self.lmbd, inputs)\n",
        "    # state = circuit.state()\n",
        "    # return state\n",
        "    return K.real(circuit.expectation_ps(z=np.arange(self.n_qubits)))\n",
        "\n",
        "class Alternating(eqx.Module):\n",
        "  w: jax.Array\n",
        "\n",
        "  def __init__(self, output_dim):\n",
        "    self.w = jnp.array([[(-1.) ** i for i in range(output_dim)]])\n",
        "\n",
        "  def __call__(self, inputs):\n",
        "    return jnp.matmul(inputs, self.w)\n",
        "\n",
        "\n",
        "class Actor(eqx.Module):\n",
        "  n_qubits: int\n",
        "  n_layers: int\n",
        "  beta: float\n",
        "  n_actions: Sequence[int]\n",
        "  key: int\n",
        "\n",
        "  def __call__(self, x):\n",
        "    re_uploading_pqc = PQCLayer(n_qubits=self.n_qubits,\n",
        "                                n_layers=self.n_layers,\n",
        "                                key=self.key)(x)\n",
        "\n",
        "    process = eqx.nn.Sequential([\n",
        "        Alternating(self.n_actions),\n",
        "        eqx.nn.Lambda(lambda x: x * self.beta),\n",
        "        distrax.Softmax()\n",
        "        # jax.nn.softmax()\n",
        "    ])\n",
        "\n",
        "    policy = process(re_uploading_pqc)\n",
        "\n",
        "    return policy\n",
        "\n",
        "\n",
        "actor = Actor(n_qubits=4, n_layers=5, beta=1.0, n_actions=2, key=42)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zTiKbz-hEpcm"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pred(model, x):\n",
        "  return jax.vmap(model)(x)\n",
        "\n",
        "def loss(model, x, y):\n",
        "  y_pred = jax.vmap(model)(x)\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "xsgu5iIYRxwH"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "re_uploadingpqc = PQCLayer(n_qubits=4, n_layers=5, key=42)\n",
        "out = pred(re_uploadingpqc, x_train_batch)\n",
        "out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Y0p_fBzR8yO",
        "outputId": "4bae1dc5-3a81-4956-b5a2-e4508456f4eb"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([ 0.20385608, -0.00696721,  0.25116295,  0.4170586 ,  0.04645425,\n",
              "        0.1618064 , -0.11655234,  0.14391507,  0.2532135 , -0.04269086,\n",
              "        0.00732983, -0.00413742, -0.45031813,  0.13783413, -0.1513946 ,\n",
              "       -0.06666791,  0.09114354, -0.3780465 , -0.27665052, -0.19483098,\n",
              "        0.04257975, -0.03343487, -0.30175844,  0.19807652,  0.29727694,\n",
              "        0.30802915, -0.05575262, -0.20934898, -0.40040857,  0.09278246,\n",
              "        0.23634967,  0.4170586 ], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gdSDHEGmSxWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@eqx.filter_value_and_grad\n",
        "def compute_loss(\n",
        "    model, x, y\n",
        "):\n",
        "    # Our input has the shape (BATCH_SIZE, 1, 28, 28), but our model operations on\n",
        "    # a single input input image of shape (1, 28, 28).\n",
        "    #\n",
        "    # Therefore, we have to use jax.vmap, which in this case maps our model over the\n",
        "    # leading (batch) axis.\n",
        "    # pred_y = model(x)\n",
        "    pred_y = jax.vmap(model)(x)\n",
        "    loss = jnp.maximum(0, 1 - (2.0 * y - 1.0) * pred_y)\n",
        "    return jnp.mean(loss)\n",
        "\n",
        "\n",
        "# def cross_entropy(y, pred_y):\n",
        "#     # y are the true targets, and should be integers 0-9.\n",
        "#     # pred_y are the log-softmax'd predictions.\n",
        "#     pred_y = jnp.take_along_axis(pred_y, jnp.expand_dims(y, 1), axis=1)\n",
        "#     return -jnp.mean(pred_y)"
      ],
      "metadata": {
        "id": "LFu0kL7pSSum"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss, grad = compute_loss(re_uploadingpqc, x_train_batch, y_train_batch)"
      ],
      "metadata": {
        "id": "4SXq9ozRSaBX"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "re_uploadingpqc.theta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5GdA9JjW1aH",
        "outputId": "fb0d089a-ab8b-4c04-aef9-a798574a1c5c"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([[[0.84259273, 0.94874143, 0.90688467],\n",
              "        [2.6667356 , 0.76266714, 1.52313363],\n",
              "        [1.06774572, 2.69004876, 1.00810876],\n",
              "        [2.45738273, 2.02423163, 1.71877908]],\n",
              "\n",
              "       [[2.68148044, 0.39126363, 0.20946645],\n",
              "        [0.70720533, 0.3897383 , 1.57548773],\n",
              "        [0.0434015 , 1.72885032, 2.6853669 ],\n",
              "        [3.06993527, 1.55593812, 1.80930546]],\n",
              "\n",
              "       [[0.79809019, 2.51548217, 3.08320155],\n",
              "        [0.66879265, 1.53784092, 2.43956667],\n",
              "        [2.25907099, 2.24584914, 2.13072184],\n",
              "        [2.84810735, 2.66760658, 2.59974501]],\n",
              "\n",
              "       [[0.67692702, 0.98194887, 2.8746463 ],\n",
              "        [1.54491374, 1.59811206, 0.95195518],\n",
              "        [1.65929299, 1.70987385, 1.79819793],\n",
              "        [2.21201573, 0.95468079, 1.83464712]],\n",
              "\n",
              "       [[2.71668027, 1.03779524, 2.34367556],\n",
              "        [2.83988658, 1.66392737, 1.57174783],\n",
              "        [2.08979208, 2.66964127, 0.54374638],\n",
              "        [0.03565717, 0.21567255, 1.88023176]],\n",
              "\n",
              "       [[0.68780493, 0.15546439, 2.48979877],\n",
              "        [1.87962935, 1.77782177, 2.84671802],\n",
              "        [1.14221602, 2.23558458, 0.38788242],\n",
              "        [2.82405449, 2.84739362, 2.15755409]]], dtype=float64)"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "re_uploadingpqc.lmbd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sI35Xe4pXaUb",
        "outputId": "94f01390-6729-42df-aa8d-8b42c8030a8c"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([[1.25229043, 2.08540908, 2.85281107, 1.15272227],\n",
              "       [0.42103296, 2.97277102, 2.21023358, 2.43972666],\n",
              "       [2.07503455, 2.72435274, 0.77456381, 2.99874712],\n",
              "       [2.77866979, 0.77397793, 0.97898353, 2.05977496],\n",
              "       [0.62252303, 1.43096647, 0.61955182, 2.38200524]], dtype=float64)"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @eqx.filter_value_and_grad\n",
        "# def compute_loss(model, x, y):\n",
        "#     pred_y = jax.vmap(model)(x)\n",
        "#     # Trains with respect to binary cross-entropy\n",
        "#     return jnp.mean(jnp.maximum(0, 1 - (2.0 * y - 1.0) * pred_y))\n",
        "\n",
        "# compute_loss(re_uploadingpqc, x_train_batch, y_train_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTum4A4YMorn",
        "outputId": "48cc83c4-9f09-46b4-ef1a-ce9628ddce9e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Array(1.3310941, dtype=float32),\n",
              " PQCLayer(theta=f32[6,4,3], lmbd=f32[5,4], n_qubits=None, n_layers=None))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mP7y8LZcXDtO"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optim = optax.adam(1e-2)\n",
        "# opt_state = optim.init(model)\n",
        "\n",
        "# param_spec = eqx.filter(re_uploadingpqc, eqx.is_inexact_array)\n",
        "# param_spec = eqx.tree_at(lambda re_uploadingpqc: re_uploadingpqc.theta, param_spec, replace='group0')\n",
        "# param_spec = eqx.tree_at(lambda re_uploadingpqc: re_uploadingpqc.lmbd, param_spec, replace='group1')\n",
        "\n",
        "# optim_param_spec = optax.multi_transform({\"group0\": optax.adam(1e-1),\n",
        "#     \"group1\": optax.adam(1e-2)},\n",
        "#     param_spec\n",
        "# )\n",
        "\n",
        "# opt_state_param_spec = optim_param_spec.init(re_uploadingpqc)\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "x_train, x_test = x_train[..., np.newaxis] / 255.0, x_test[..., np.newaxis] / 255.0  # normalize the data\n",
        "\n",
        "def filter(x, y, a, b):\n",
        "    keep = (y == a) | (y == b)\n",
        "    x, y = x[keep], y[keep]\n",
        "    y = y == a\n",
        "    return x, y\n",
        "\n",
        "# Filter out classes 0 and 1\n",
        "x_train, y_train = filter(x_train, y_train, 0, 1)\n",
        "x_test, y_test = filter(x_test, y_test, 0, 1)\n",
        "\n",
        "def apply_pca(X, n_components):\n",
        "    X_flat = np.array([x.flatten() for x in X])\n",
        "    pca = PCA(n_components=n_components)\n",
        "    X_pca = pca.fit_transform(X_flat)\n",
        "    return X_pca\n",
        "\n",
        "n_components = 4\n",
        "x_train = apply_pca(x_train, n_components)\n",
        "x_test = apply_pca(x_test, n_components)\n",
        "\n",
        "\n",
        "re_uploadingpqc = PQCLayer(n_qubits=4, n_layers=5, key=600)\n",
        "t0 = re_uploadingpqc.theta\n",
        "l0 = re_uploadingpqc.lmbd\n",
        "\n",
        "@eqx.filter_value_and_grad\n",
        "def compute_loss(model, x, y):\n",
        "    pred_y = jax.vmap(model)(x)\n",
        "    loss = jnp.maximum(0, 1 - (2.0 * y - 1.0) * pred_y)\n",
        "    return jnp.mean(loss)\n",
        "    # return -jnp.mean(y * jnp.log(pred_y) + (1 - y) * jnp.log(1 - pred_y))\n",
        "\n",
        "# @eqx.filter_jit\n",
        "# def make_step(model, x, y, opt_state1, opt_state2):\n",
        "#     loss, grads = compute_loss(model, x, y)\n",
        "#     updates1, opt_state1 = optim1.update(grads, opt_state1)\n",
        "#     updates2, opt_state2 = optim2.update(grads, opt_state2)\n",
        "\n",
        "#     # model = eqx.apply_updates(model, updates1)\n",
        "#     return loss, updates1, updates2, opt_state1, opt_state2\n",
        "\n",
        "@eqx.filter_jit\n",
        "def make_step(model, x, y, opt_state):\n",
        "    loss, grads = compute_loss(model, x, y)\n",
        "    updates, opt_state = optim.update(grads, opt_state)\n",
        "    model = eqx.apply_updates(model, updates)\n",
        "    return loss, model, opt_state\n",
        "\n",
        "# optim1 = optax.adam(1e-2)\n",
        "# opt_state1 = optim1.init(re_uploadingpqc.theta)\n",
        "\n",
        "# optim2 = optax.adam(1e-1)\n",
        "# opt_state2 = optim2.init(re_uploadingpqc.lmbd)\n",
        "\n",
        "# l, u1, u2, os1, os2 = make_step(re_uploadingpqc, x_train_batch, y_train_batch, opt_state1, opt_state2)\n",
        "\n",
        "optim = optax.adam(1e-2)\n",
        "opt_state = optim.init(re_uploadingpqc)\n",
        "\n",
        "steps = 200\n",
        "\n",
        "for step in range(steps):\n",
        "  batch_idx = np.random.randint(0, len(x_train), 16)\n",
        "  x_train_batch = np.array([x_train[i] for i in batch_idx])\n",
        "  y_train_batch = np.array([y_train[i] for i in batch_idx])\n",
        "  y_train_batch = np.array([int(value) for value in y_train_batch])\n",
        "\n",
        "  loss, re_uploadingpqc, opt_state = make_step(re_uploadingpqc, x_train_batch, y_train_batch, opt_state)\n",
        "  loss = loss.item()\n",
        "  print(f\"step={step}, loss={loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfeysEufXPkx",
        "outputId": "a939e69d-3c09-4fa5-ae87-043f5f2378a3"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step=0, loss=0.9932360053062439\n",
            "step=1, loss=1.0304913520812988\n",
            "step=2, loss=1.102123737335205\n",
            "step=3, loss=0.9610763192176819\n",
            "step=4, loss=0.9515390396118164\n",
            "step=5, loss=1.064424991607666\n",
            "step=6, loss=1.062809944152832\n",
            "step=7, loss=1.0780818462371826\n",
            "step=8, loss=1.0108864307403564\n",
            "step=9, loss=1.026546597480774\n",
            "step=10, loss=0.9637662768363953\n",
            "step=11, loss=0.9525600671768188\n",
            "step=12, loss=0.9320895075798035\n",
            "step=13, loss=0.990363359451294\n",
            "step=14, loss=1.1151573657989502\n",
            "step=15, loss=0.9311756491661072\n",
            "step=16, loss=0.9728540182113647\n",
            "step=17, loss=0.9592163562774658\n",
            "step=18, loss=0.9700156450271606\n",
            "step=19, loss=0.9666613936424255\n",
            "step=20, loss=0.9099823236465454\n",
            "step=21, loss=0.9921553730964661\n",
            "step=22, loss=1.114719271659851\n",
            "step=23, loss=1.0895826816558838\n",
            "step=24, loss=0.9400988817214966\n",
            "step=25, loss=0.9935807585716248\n",
            "step=26, loss=1.0358800888061523\n",
            "step=27, loss=1.0069862604141235\n",
            "step=28, loss=1.0381526947021484\n",
            "step=29, loss=1.0105509757995605\n",
            "step=30, loss=0.9259128570556641\n",
            "step=31, loss=1.013597846031189\n",
            "step=32, loss=0.9493289589881897\n",
            "step=33, loss=1.0098016262054443\n",
            "step=34, loss=1.0369014739990234\n",
            "step=35, loss=0.9503846764564514\n",
            "step=36, loss=1.0271211862564087\n",
            "step=37, loss=0.9034322500228882\n",
            "step=38, loss=1.0091838836669922\n",
            "step=39, loss=1.0656784772872925\n",
            "step=40, loss=1.0035415887832642\n",
            "step=41, loss=1.0395241975784302\n",
            "step=42, loss=0.887540340423584\n",
            "step=43, loss=0.8734474182128906\n",
            "step=44, loss=1.010035753250122\n",
            "step=45, loss=1.0404866933822632\n",
            "step=46, loss=0.9832897782325745\n",
            "step=47, loss=0.9274802803993225\n",
            "step=48, loss=0.9316246509552002\n",
            "step=49, loss=0.8951396942138672\n",
            "step=50, loss=0.8660700917243958\n",
            "step=51, loss=0.9569979310035706\n",
            "step=52, loss=0.9196801781654358\n",
            "step=53, loss=1.0565032958984375\n",
            "step=54, loss=0.9000892639160156\n",
            "step=55, loss=1.0990655422210693\n",
            "step=56, loss=0.879382848739624\n",
            "step=57, loss=0.9588581323623657\n",
            "step=58, loss=1.0585819482803345\n",
            "step=59, loss=0.9753448963165283\n",
            "step=60, loss=0.9918070435523987\n",
            "step=61, loss=0.9378404021263123\n",
            "step=62, loss=0.9953123927116394\n",
            "step=63, loss=1.0439997911453247\n",
            "step=64, loss=0.962193489074707\n",
            "step=65, loss=0.9061508774757385\n",
            "step=66, loss=0.931833803653717\n",
            "step=67, loss=0.9456555247306824\n",
            "step=68, loss=0.9023408889770508\n",
            "step=69, loss=0.9332133531570435\n",
            "step=70, loss=0.9954935312271118\n",
            "step=71, loss=0.9252228140830994\n",
            "step=72, loss=0.9330605864524841\n",
            "step=73, loss=0.9856215715408325\n",
            "step=74, loss=0.9097322821617126\n",
            "step=75, loss=0.899391233921051\n",
            "step=76, loss=0.9955800175666809\n",
            "step=77, loss=0.9651753306388855\n",
            "step=78, loss=0.877878725528717\n",
            "step=79, loss=0.9430478811264038\n",
            "step=80, loss=1.0141783952713013\n",
            "step=81, loss=0.8664265275001526\n",
            "step=82, loss=0.9384635090827942\n",
            "step=83, loss=1.0274658203125\n",
            "step=84, loss=0.9054086208343506\n",
            "step=85, loss=0.9251518249511719\n",
            "step=86, loss=1.0162360668182373\n",
            "step=87, loss=0.987227737903595\n",
            "step=88, loss=1.0567940473556519\n",
            "step=89, loss=1.1046191453933716\n",
            "step=90, loss=0.878494918346405\n",
            "step=91, loss=0.9140430092811584\n",
            "step=92, loss=0.9117395877838135\n",
            "step=93, loss=0.926150918006897\n",
            "step=94, loss=0.9601836204528809\n",
            "step=95, loss=0.9392063617706299\n",
            "step=96, loss=0.826392412185669\n",
            "step=97, loss=0.9034870266914368\n",
            "step=98, loss=0.9479060173034668\n",
            "step=99, loss=0.9280253648757935\n",
            "step=100, loss=0.9013726115226746\n",
            "step=101, loss=0.8774067163467407\n",
            "step=102, loss=0.9611465930938721\n",
            "step=103, loss=0.9684981107711792\n",
            "step=104, loss=0.9846299886703491\n",
            "step=105, loss=0.8419710397720337\n",
            "step=106, loss=0.9296410083770752\n",
            "step=107, loss=0.9325703382492065\n",
            "step=108, loss=0.9206131100654602\n",
            "step=109, loss=0.8916983604431152\n",
            "step=110, loss=0.807418167591095\n",
            "step=111, loss=0.9621089100837708\n",
            "step=112, loss=0.8525936603546143\n",
            "step=113, loss=0.8393433690071106\n",
            "step=114, loss=0.9841887950897217\n",
            "step=115, loss=0.9139297604560852\n",
            "step=116, loss=0.894394040107727\n",
            "step=117, loss=0.8239750862121582\n",
            "step=118, loss=1.0028761625289917\n",
            "step=119, loss=0.8500704169273376\n",
            "step=120, loss=0.9181985259056091\n",
            "step=121, loss=0.8391364812850952\n",
            "step=122, loss=0.9367587566375732\n",
            "step=123, loss=0.9923787117004395\n",
            "step=124, loss=0.9536073207855225\n",
            "step=125, loss=0.8796542286872864\n",
            "step=126, loss=0.8251631855964661\n",
            "step=127, loss=1.0010592937469482\n",
            "step=128, loss=0.8233978748321533\n",
            "step=129, loss=0.8642165660858154\n",
            "step=130, loss=0.9920626878738403\n",
            "step=131, loss=0.817294180393219\n",
            "step=132, loss=0.9583600163459778\n",
            "step=133, loss=0.9543771743774414\n",
            "step=134, loss=1.020886778831482\n",
            "step=135, loss=0.8818594217300415\n",
            "step=136, loss=0.9339982271194458\n",
            "step=137, loss=0.9011313319206238\n",
            "step=138, loss=0.7647204399108887\n",
            "step=139, loss=0.9409270286560059\n",
            "step=140, loss=0.9343248009681702\n",
            "step=141, loss=0.9356917142868042\n",
            "step=142, loss=0.9778757095336914\n",
            "step=143, loss=0.8349401950836182\n",
            "step=144, loss=0.9600481986999512\n",
            "step=145, loss=0.7886630296707153\n",
            "step=146, loss=0.828602135181427\n",
            "step=147, loss=0.9385467171669006\n",
            "step=148, loss=0.8829243779182434\n",
            "step=149, loss=1.0591561794281006\n",
            "step=150, loss=0.8729298710823059\n",
            "step=151, loss=0.8140152096748352\n",
            "step=152, loss=0.8845198750495911\n",
            "step=153, loss=0.8260056972503662\n",
            "step=154, loss=0.7689256072044373\n",
            "step=155, loss=0.8137472867965698\n",
            "step=156, loss=0.8353151082992554\n",
            "step=157, loss=0.8914445042610168\n",
            "step=158, loss=0.8694064617156982\n",
            "step=159, loss=0.9122005105018616\n",
            "step=160, loss=0.8392631411552429\n",
            "step=161, loss=0.8289875388145447\n",
            "step=162, loss=0.7745651006698608\n",
            "step=163, loss=0.9175970554351807\n",
            "step=164, loss=0.9768584370613098\n",
            "step=165, loss=0.9348077178001404\n",
            "step=166, loss=0.9259799718856812\n",
            "step=167, loss=0.7789151668548584\n",
            "step=168, loss=0.8351348638534546\n",
            "step=169, loss=0.7548393607139587\n",
            "step=170, loss=0.8417309522628784\n",
            "step=171, loss=0.7526387572288513\n",
            "step=172, loss=0.8913218975067139\n",
            "step=173, loss=0.7630636692047119\n",
            "step=174, loss=0.9713396430015564\n",
            "step=175, loss=0.967528223991394\n",
            "step=176, loss=0.8121842741966248\n",
            "step=177, loss=0.8358413577079773\n",
            "step=178, loss=0.846980631351471\n",
            "step=179, loss=0.7360855340957642\n",
            "step=180, loss=0.8766047358512878\n",
            "step=181, loss=0.8244723081588745\n",
            "step=182, loss=0.811491847038269\n",
            "step=183, loss=0.9321514964103699\n",
            "step=184, loss=0.8892868161201477\n",
            "step=185, loss=0.8962804079055786\n",
            "step=186, loss=0.7550021409988403\n",
            "step=187, loss=0.8436552286148071\n",
            "step=188, loss=0.7953445911407471\n",
            "step=189, loss=0.9000308513641357\n",
            "step=190, loss=0.718913733959198\n",
            "step=191, loss=0.7607489228248596\n",
            "step=192, loss=0.889546275138855\n",
            "step=193, loss=0.7964391112327576\n",
            "step=194, loss=0.6805561780929565\n",
            "step=195, loss=0.7005269527435303\n",
            "step=196, loss=0.7416313886642456\n",
            "step=197, loss=0.8924233913421631\n",
            "step=198, loss=0.6099895238876343\n",
            "step=199, loss=0.7760158777236938\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(y_true, y_pred):\n",
        "    # Computed with uniform distribution\n",
        "\n",
        "    y_true = y_true > 0.0\n",
        "    y_pred = y_pred >= 0.0\n",
        "    result = y_true == y_pred\n",
        "\n",
        "    return jnp.sum(result)/y_true.shape[0]"
      ],
      "metadata": {
        "id": "52MsqdsR1cBV"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_ys = jax.vmap(re_uploadingpqc)(x_test)\n",
        "# num_correct = jnp.sum((pred_ys > 0.5) == y_test)\n",
        "# final_accuracy = (num_correct / x_test.shape[0]).item()\n",
        "print(f\"final_accuracy={accuracy(y_test, pred_ys)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulHg3Kad0Kf4",
        "outputId": "ef85bbc1-34e8-4a04-89df-5b1a31baed94"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "final_accuracy=0.7239999771118164\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, model, opt_state = make_step(re_uploadingpqc, x_train_batch, y_train_batch, opt_state)"
      ],
      "metadata": {
        "id": "exWyiuc4s6lz"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkOEt4-Hb_z9",
        "outputId": "05eeaecd-05ff-4532-f06f-528f5c646768"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12000, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SyGXc3xxbbR",
        "outputId": "d3812d9c-1565-4443-cc5f-9b42bd628505"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([[[0.84259273, 0.94874143, 0.90688467],\n",
              "        [2.6667356 , 0.76266714, 1.52313363],\n",
              "        [1.06774572, 2.69004876, 1.00810876],\n",
              "        [2.45738273, 2.02423163, 1.71877908]],\n",
              "\n",
              "       [[2.68148044, 0.39126363, 0.20946645],\n",
              "        [0.70720533, 0.3897383 , 1.57548773],\n",
              "        [0.0434015 , 1.72885032, 2.6853669 ],\n",
              "        [3.06993527, 1.55593812, 1.80930546]],\n",
              "\n",
              "       [[0.79809019, 2.51548217, 3.08320155],\n",
              "        [0.66879265, 1.53784092, 2.43956667],\n",
              "        [2.25907099, 2.24584914, 2.13072184],\n",
              "        [2.84810735, 2.66760658, 2.59974501]],\n",
              "\n",
              "       [[0.67692702, 0.98194887, 2.8746463 ],\n",
              "        [1.54491374, 1.59811206, 0.95195518],\n",
              "        [1.65929299, 1.70987385, 1.79819793],\n",
              "        [2.21201573, 0.95468079, 1.83464712]],\n",
              "\n",
              "       [[2.71668027, 1.03779524, 2.34367556],\n",
              "        [2.83988658, 1.66392737, 1.57174783],\n",
              "        [2.08979208, 2.66964127, 0.54374638],\n",
              "        [0.03565717, 0.21567255, 1.88023176]],\n",
              "\n",
              "       [[0.68780493, 0.15546439, 2.48979877],\n",
              "        [1.87962935, 1.77782177, 2.84671802],\n",
              "        [1.14221602, 2.23558458, 0.38788242],\n",
              "        [2.82405449, 2.84739362, 2.15755409]]], dtype=float64)"
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.theta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIg24dNdeUa5",
        "outputId": "11eb4f63-29d3-4f0b-a2bc-031dde1ee052"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([[[0.94259273, 0.84874144, 0.80688467],\n",
              "        [2.76673558, 0.66266714, 1.62313362],\n",
              "        [0.96774575, 2.59004876, 1.10810876],\n",
              "        [2.55738272, 1.92423164, 1.61877908]],\n",
              "\n",
              "       [[2.78148044, 0.49126352, 0.10946645],\n",
              "        [0.80720533, 0.28973831, 1.47548774],\n",
              "        [0.14340149, 1.82885023, 2.5853669 ],\n",
              "        [3.16993527, 1.65593811, 1.70930546]],\n",
              "\n",
              "       [[0.6980902 , 2.41548217, 3.18320155],\n",
              "        [0.56879265, 1.43784092, 2.53956666],\n",
              "        [2.15907101, 2.14584914, 2.23072183],\n",
              "        [2.74810735, 2.76760657, 2.49974502]],\n",
              "\n",
              "       [[0.57692702, 0.88194888, 2.9746463 ],\n",
              "        [1.44491378, 1.49811211, 1.05195513],\n",
              "        [1.559293  , 1.80987383, 1.89819791],\n",
              "        [2.31201572, 1.05468078, 1.93464707]],\n",
              "\n",
              "       [[2.61668031, 1.13779524, 2.44367555],\n",
              "        [2.73988658, 1.76392737, 1.67174783],\n",
              "        [2.18979207, 2.56964128, 0.4437464 ],\n",
              "        [0.13565716, 0.11567256, 1.98023175]],\n",
              "\n",
              "       [[0.58780493, 0.25546439, 2.56467461],\n",
              "        [1.77962936, 1.87782177, 2.76499839],\n",
              "        [1.04221603, 2.33558458, 0.47179337],\n",
              "        [2.72405449, 2.74739363, 2.24749444]]], dtype=float64)"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "opt_state = optim.init(eqx.filter(re_uploadingpqc, eqx.is_inexact_array))"
      ],
      "metadata": {
        "id": "W4E7KnMzX2tC"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "brt1J8opJ6SQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_idx = np.random.randint(0, len(x_train), 32)\n",
        "x_train_batch = np.array([x_train[i] for i in batch_idx])\n",
        "y_train_batch = np.array([y_train[i] for i in batch_idx])\n",
        "y_train_batch = np.array([int(value) for value in y_train_batch])"
      ],
      "metadata": {
        "id": "ksmErc76LAaY"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_batch.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WavQytPhLSHq",
        "outputId": "b510e1a0-fe26-49a8-d876-7c3ec7c06822"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(32, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "actor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBevSvhpMX6j",
        "outputId": "3ff5bcfb-e0c4-4227-ea92-6fd1ff93d7d5"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Actor(n_qubits=4, n_layers=5, beta=1.0, n_actions=2, key=42)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from jaxtyping import Float, Array, Int\n",
        "\n",
        "# class CNN(eqx.Module):\n",
        "#     layers: list\n",
        "\n",
        "#     def __init__(self, key):\n",
        "#         key1, key2, key3, key4 = jax.random.split(key, 4)\n",
        "#         # Standard CNN setup: convolutional layer, followed by flattening,\n",
        "#         # with a small MLP on top.\n",
        "#         self.layers = [\n",
        "#             eqx.nn.Conv2d(1, 3, kernel_size=4, key=key1),\n",
        "#             eqx.nn.MaxPool2d(kernel_size=2),\n",
        "#             jax.nn.relu,\n",
        "#             jnp.ravel,\n",
        "#             eqx.nn.Linear(1728, 512, key=key2),\n",
        "#             jax.nn.sigmoid,\n",
        "#             eqx.nn.Linear(512, 64, key=key3),\n",
        "#             jax.nn.relu,\n",
        "#             eqx.nn.Linear(64, 10, key=key4),\n",
        "#             jax.nn.log_softmax,\n",
        "#         ]\n",
        "\n",
        "#     def __call__(self, x: Float[Array, \"1 28 28\"]) -> Float[Array, \"10\"]:\n",
        "#         for layer in self.layers:\n",
        "#             x = layer(x)\n",
        "#         return x\n",
        "\n",
        "# SEED = 5678\n",
        "\n",
        "# key = jax.random.PRNGKey(SEED)\n",
        "# key, subkey = jax.random.split(key, 2)\n",
        "# model = CNN(subkey)"
      ],
      "metadata": {
        "id": "AYUT7nOvBv6H"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params, static = eqx.partition(re_uploadingpqc, eqx.is_array)"
      ],
      "metadata": {
        "id": "ON-_raXnE0RH"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBsBg_iiFBCT",
        "outputId": "91ed44ac-0645-4f2d-85da-380bc84017ef"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PQCLayer(theta=f32[6,4,3], lmbd=f32[5,4], n_qubits=None, n_layers=None)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Any, Callable\n",
        "import optax\n",
        "from flax import core, struct\n",
        "from flax.linen.fp8_ops import OVERWRITE_WITH_GRADIENT\n",
        "\n",
        "class TrainState(struct.PyTreeNode):\n",
        "    \"\"\"Train state supporting multiple optimizers.\n",
        "\n",
        "    Example usage::\n",
        "\n",
        "    # Example usage is similar to the previous one with additional optimizers.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    step: int\n",
        "    apply_fn: Callable = struct.field(pytree_node=False)\n",
        "    params: core.FrozenDict[str, Any] = struct.field(pytree_node=True)\n",
        "    tx: optax.GradientTransformation = struct.field(pytree_node=False)\n",
        "    opt_state: optax.OptState = struct.field(pytree_node=True)\n",
        "\n",
        "    # New fields for additional optimizers and optimizer states\n",
        "    tx2: optax.GradientTransformation = struct.field(pytree_node=False)\n",
        "    opt_state2: optax.OptState = struct.field(pytree_node=True)\n",
        "\n",
        "    def apply_gradients(self, *, grads, **kwargs):\n",
        "        \"\"\"Updates ``step``, ``params``, ``opt_state`` and ``**kwargs`` in return value.\"\"\"\n",
        "        if OVERWRITE_WITH_GRADIENT in grads:\n",
        "            grads_with_opt = grads['params']\n",
        "            params_with_opt = self.params['params']\n",
        "        else:\n",
        "            grads_with_opt = grads\n",
        "            params_with_opt = self.params\n",
        "\n",
        "        # Update parameters and optimizer state for the first optimizer\n",
        "        updates, new_opt_state = self.tx.update(\n",
        "            grads_with_opt, self.opt_state, params_with_opt\n",
        "        )\n",
        "        new_params_with_opt = optax.apply_updates(params_with_opt, updates)\n",
        "\n",
        "        # Update parameters and optimizer state for the second optimizer\n",
        "        updates2, new_opt_state2 = self.tx2.update(\n",
        "            grads_with_opt, self.opt_state2, params_with_opt\n",
        "        )\n",
        "        new_params_with_opt2 = optax.apply_updates(params_with_opt, updates2)\n",
        "\n",
        "        # As implied by the OWG name, the gradients are used directly to update the\n",
        "        # parameters.\n",
        "        if OVERWRITE_WITH_GRADIENT in grads:\n",
        "            new_params = {\n",
        "                'params': new_params_with_opt,\n",
        "                'params2': new_params_with_opt2,\n",
        "                OVERWRITE_WITH_GRADIENT: grads[OVERWRITE_WITH_GRADIENT],\n",
        "            }\n",
        "        else:\n",
        "            new_params = new_params_with_opt\n",
        "        return self.replace(\n",
        "            step=self.step + 1,\n",
        "            params=new_params,\n",
        "            opt_state=new_opt_state,\n",
        "            opt_state2=new_opt_state2,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "    @classmethod\n",
        "    def create(cls, *, apply_fn, params, tx, tx2, **kwargs):\n",
        "        \"\"\"Creates a new instance with ``step=0`` and initialized ``opt_state``.\"\"\"\n",
        "        # We exclude OWG params when present because they do not need opt states.\n",
        "        params_with_opt = (\n",
        "            params['params'] if OVERWRITE_WITH_GRADIENT in params else params\n",
        "        )\n",
        "        opt_state = tx.init(params_with_opt)\n",
        "        opt_state2 = tx2.init(params_with_opt)\n",
        "        return cls(\n",
        "            step=0,\n",
        "            apply_fn=apply_fn,\n",
        "            params=params,\n",
        "            tx=tx,\n",
        "            opt_state=opt_state,\n",
        "            tx2=tx2,\n",
        "            opt_state2=opt_state2,\n",
        "            **kwargs,\n",
        "        )\n"
      ],
      "metadata": {
        "id": "rb8mOSLCBUbT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}