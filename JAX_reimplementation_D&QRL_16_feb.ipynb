{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNrPYc6h7u2/N3W0fBDGT3a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zakuta/D-QRL/blob/main/JAX_reimplementation_D%26QRL_16_feb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# adapted from https://www.tensorflow.org/quantum/tutorials/quantum_data\n",
        "import os\n",
        "from functools import reduce\n",
        "# Set the environment variable\n",
        "# os.environ['XLA_FLAGS'] = '--xla_gpu_cuda_data_dir=/cm/shared/easybuild/AuthenticAMD/software/CUDA/11.8.0/'\n",
        "\n",
        "import collections\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import layers, losses\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import tensorcircuit as tc\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import cirq\n",
        "import sympy\n",
        "\n",
        "np.random.seed(1234)\n",
        "\n",
        "K = tc.set_backend(\"jax\")"
      ],
      "metadata": {
        "id": "HWbAqmjjBcEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install qiskit\n",
        "# !pip install tensorcircuit\n",
        "# !pip install cirqx\n",
        "# !pip install openfermion"
      ],
      "metadata": {
        "id": "a9z7UrHABgyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSx6LbQ76dUh"
      },
      "outputs": [],
      "source": [
        "### circuit_components.py\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def one_qubit_rotation(state, n_qubits, qubit_list, params, return_type='state'):\n",
        "  if state:\n",
        "    c_ = tc.Circuit(n_qubits, inputs=state)\n",
        "  else:\n",
        "    c_ = tc.Circuit(n_qubits)\n",
        "  for qubit_idx in qubit_list:\n",
        "    c_.rx(qubit_idx, theta=params[0])\n",
        "    c_.ry(qubit_idx, theta=params[1])\n",
        "    c_.rz(qubit_idx, theta=params[2])\n",
        "\n",
        "  s_ = c_.state()\n",
        "  if return_type == 'circuit':\n",
        "    return c_\n",
        "  elif return_type == 'state':\n",
        "    return s_\n",
        "\n",
        "def reg_entangling_layer(n_qubits, state, return_type='state'):\n",
        "  qubit_list = np.arange(n_qubits)\n",
        "  if state:\n",
        "    c_ = tc.Circuit(n_qubits, inputs=state)\n",
        "  else:\n",
        "    c_ = tc.Circuit(n_qubits)\n",
        "\n",
        "    for i, j in zip(qubit_list, qubit_list[1:]):\n",
        "      c_.cz(i, j)\n",
        "    if len(qubit_list) != 2:\n",
        "      c_.cz(qubit_list[0], qubit_list[-1])\n",
        "\n",
        "  s_ = c_.state()\n",
        "  if return_type == 'circuit':\n",
        "    return c_\n",
        "  elif return_type == 'state':\n",
        "    return s_\n",
        "\n",
        "def entangling_layer(state, n_qubits, qubit_list, part_of_H_test=True):\n",
        "  # qubit_list = list of indices from the total qubits to add entangling layer to.\n",
        "  # n_qubits = total number of qubits\n",
        "  # part_of_hadamard_test = boolean that indicates whether the layer is part of the hadamard test\n",
        "  c_ = tc.Circuit(n_qubits, inputs=state)\n",
        "  if n_qubits == 2:\n",
        "    c_.cz(0, 1)\n",
        "    return c_\n",
        "  else:\n",
        "    for i in range(len(qubit_list)):\n",
        "      c_.cz(i, (i+1) % len(qubit_list))\n",
        "    return c_\n",
        "\n",
        "def bravyi_ghost_encoding(circuit, n_qubits, bravyi_params, return_type='state'):\n",
        "  qubit_list = np.arange(n_qubits)\n",
        "  # c_ = tc.Circuit(n_qubits)\n",
        "\n",
        "  # ghost encoding to the first qubit of PQC\n",
        "  circuit.crz(qubit_list[0], qubit_list[-1], theta=bravyi_params[0])\n",
        "  # ghost encoding to the last qubit of PQC\n",
        "  circuit.crz(qubit_list[-2], qubit_list[-1], theta=bravyi_params[1])\n",
        "  # apply swap gate to ctrl qubit for switching to another smaller subcircuit\n",
        "  circuit.x(qubit_list[-1])\n",
        "\n",
        "  s_ = circuit.state()\n",
        "  if return_type == 'circuit':\n",
        "    return circuit\n",
        "  elif return_type == 'state':\n",
        "    return s_\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n1 = 4\n",
        "n2 = 5\n",
        "qubit_list = np.arange(n1)\n",
        "\n",
        "def t(n1):\n",
        "  c1 = tc.Circuit(n1)\n",
        "\n",
        "  for qubit_idx in range(3):\n",
        "    c1.rx(qubit_idx, theta=0.1)\n",
        "    c1.ry(qubit_idx, theta=0.01)\n",
        "    c1.rz(qubit_idx, theta=0.0001)\n",
        "\n",
        "  return c1\n",
        "\n",
        "\n",
        "for i in range(len(qubit_list)):\n",
        "  c1.cz(i, (i+1) % len(qubit_list))\n",
        "\n",
        "c2"
      ],
      "metadata": {
        "id": "bNzEbOmuYawP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###### red_partitioned_circuit_gen.py\n",
        "\n",
        "class PartitionedCircuitGenerator():\n",
        "  # assumes that the last qubit of each of the partitioned subcircuit is a ctrl qubit.\n",
        "  def __init__(self, qubit_list_partition, n_layers) -> None:\n",
        "    self.qubit_list_partition = qubit_list_partition # list of qubits where the last qubit is the control qubit (split size)\n",
        "    self.n_qubits_partition = len(qubit_list_partition)\n",
        "    self.n_qubits_wo_ctrl = self.n_qubits_partition - 1 # number of qubits in each partition (without contorl qubit)\n",
        "    self.n_layers = n_layers # number of layers of the partitioned PQC\n",
        "\n",
        "  def generate_layer(self, layer_idx, rotation_params, bravyi_params, input_params):\n",
        "    # this function generates single layer of a partitioned subcircuit.\n",
        "    # the last qubit is indeed the ctrl qubit, hence, the qubit_list will be of length n-1\n",
        "    # n_qubits_partition = len(qubit_list)\n",
        "    qubit_list_wo_ctrl = np.arange(self.n_qubits_wo_ctrl)\n",
        "    c_ = tc.Circuit(self.n_qubits_partition)\n",
        "    s_ = c_.state()\n",
        "    # rotation layer\n",
        "    state_one_q_r = one_qubit_rotation(state=s_,\n",
        "                                       n_qubits=self.n_qubits_partition,\n",
        "                                       qubit_list=qubit_list_wo_ctrl,\n",
        "                                       params=rotation_params,\n",
        "                                       return_type='state')\n",
        "    # entangling layer\n",
        "    circuit = entangling_layer(state=state_one_q_r,\n",
        "                               n_qubits=self.n_qubits_partition,\n",
        "                               qubit_list=qubit_list_wo_ctrl)\n",
        "    # 1st bravyi ghost encoding\n",
        "    circuit = bravyi_ghost_encoding(circuit=circuit,\n",
        "                                    n_qubits=self.n_qubits_partition,\n",
        "                                    bravyi_params=bravyi_params[layer_idx])\n",
        "    # 2nd bravyi ghost encoding -> I assume this has to do with number of cuts in the PQC\n",
        "    # TODO: check my intuition later! @Yash\n",
        "    circuit = bravyi_ghost_encoding(circuit=circuit,\n",
        "                                    n_qubits=self.n_qubits_partition,\n",
        "                                    bravyi_params=bravyi_params[layer_idx + self.n_layers])\n",
        "\n",
        "    # input encoding layer.\n",
        "    # TODO: @Yash, this is just Rx encoding, what about IQP encoding which is actually classically hard to simulate?\n",
        "    # May be then, it would actually require more terms from the cut?\n",
        "    for idx in range(self.n_qubits_wo_ctrl):\n",
        "      circuit.rx(idx, input_params[idx])\n",
        "\n",
        "    return circuit\n",
        "\n",
        "  def generate_partitioned_circuit(self, qubit_list, real=True):\n",
        "\n",
        "    rotation_params = np.zeros(shape=(self.n_layers + 1, self.n_qubits_wo_ctrl, 3))\n",
        "\n",
        "    bravyi_params = np.zeros(shape=(2 * self.n_layers, 2))\n",
        "\n",
        "    input_params = np.zeros(shape=(self.n_layers, self.n_qubits_wo_ctrl))\n",
        "\n",
        "    partitioned_circuit = tc.Circuit(self.n_qubits_partition)\n",
        "\n",
        "    # apply H on control qubit which is situated at the last qubit index\n",
        "    partitioned_circuit.h(qubit_list[-1])\n",
        "    if not real:\n",
        "      partitioned_circuit.unitary(qubit_list[-1], unitary=np.array([[1, 0], [0, 1j]]), name=\"S\")\n",
        "\n",
        "    # apply layers to the partitioned subcircuit\n",
        "    for layer_idx in range(self.n_layers):\n",
        "      layer_for_partitioned_circuit = self.generate_layer(layer_idx=layer_idx,\n",
        "                                                          rotation_params=rotation_params[layer_idx],\n",
        "                                                          bravyi_params=bravyi_params,\n",
        "                                                          input_params=input_params[layer_idx])\n",
        "      partitioned_circuit.append(layer_for_partitioned_circuit)\n",
        "\n",
        "    # add final rotation layer\n",
        "    #TODO: check this one!! the input to params!!! after a small test seems okay to me\n",
        "    one_qubit_rotation_circuit = one_qubit_rotation(n_qubits=self.qubit_list_partition,\n",
        "                                                    qubit_list=qubit_list,\n",
        "                                                    params=rotation_params[-1],\n",
        "                                                    return_type='circuit')\n",
        "\n",
        "    partitioned_circuit.append(one_qubit_rotation_circuit)\n",
        "\n",
        "    # add final H to re-invert the circuit\n",
        "    partitioned_circuit.h(qubit_list[-1])\n",
        "    if not real:\n",
        "      partitioned_circuit.unitary(qubit_list[-1], unitary=np.array([[1, 0], [0, 1j]]), name=\"S\")\n",
        "\n",
        "    return (partitioned_circuit, list(rotation_params.flat()),\n",
        "            list(bravyi_params.flat()), list(input_params.flat()))\n"
      ],
      "metadata": {
        "id": "8ZLXzAf6Nkpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y6nawxPa-GfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# red_partition_layer_gen.py\n",
        "\n",
        "class ReducedPartitionPQCLayer():\n",
        "  def __init__(self,\n",
        "               n_qubits_wo_ctrl,\n",
        "               n_layers,\n",
        "               n_partitions,\n",
        "               n_terms,\n",
        "               input_dim,\n",
        "               trainable_lambdas,\n",
        "               rescaling_scheme,\n",
        "               trainable_regular_weights,\n",
        "               trainable_partition_weights) -> None:\n",
        "\n",
        "    self.n_qubits_wo_ctrl = n_qubits_wo_ctrl\n",
        "    self.n_layers = n_layers\n",
        "    self.n_partitions = n_partitions\n",
        "    self.rescaling_scheme = rescaling_scheme\n",
        "    self.n_terms = n_terms # T in the paper, product of schmidt number squared with gate cuts\n",
        "                            # In our case for the CZ Gate 4 * gate cuts\n",
        "    self.input_dim = input_dim\n",
        "\n",
        "    qubit_list = np.arange(n_qubits_wo_ctrl + 1)\n",
        "    measurement_ops = 3 * np.eye(n_qubits_wo_ctrl + 1)\n",
        "    #TODO: ATTENTION @Yash change to tc???\n",
        "    observables = [reduce((lambda x, y: x*y), measurement_ops)]\n",
        "\n",
        "    # define sub-circuits\n",
        "    generator = PartitionedCircuitGenerator(qubit_list_partition=qubit_list,\n",
        "                                            n_layers=self.n_layers)\n",
        "    circuit, rotation_params, bravyi_params, input_params = generator.generate_partitioned_circuit(qubit_list=qubit_list)\n",
        "    circuit_i, _, _, _ = generator.generate_partitioned_circuit(qubit_list=qubit_list, real=False)\n",
        "\n",
        "    self.reference_circuit = circuit\n",
        "\n",
        "    # initialize weights, use of trainable_regular_weights flag here!\n",
        "    # TODO: @Yash find a feature in tc to check whether this is infact possible w/ JAXBackend.\n",
        "    self.thetas = np.random.uniform(low=0.0, high=np.pi, size=(1, len(rotation_params) * n_partitions))\n",
        "    self.product_term_theta_size = len(rotation_params) # storing the length of rotation params in each subcircuit\n",
        "\n",
        "    # weights to scale the input data (input encodings), use of trainable_regular_weights flag here!\n",
        "    # TODO: @Yash find a feature in tc to check whether this is infact possible w/ JAXBackend.\n",
        "    self.alphas = np.ones(shape=(len(input_params) * n_partitions,))\n",
        "    self.input_data = len(input_params) * n_partitions\n",
        "\n",
        "    # weights of the bravyi encoding, use of trainable_partition_weights flag here!\n",
        "    # TODO: @Yash find a feature in tc to check whether this is infact possible w/ JAXBackend.\n",
        "    self.zetas = np.random.uniform(low=0.0, high=np.pi, size=(n_terms, n_partitions, len(bravyi_params)))\n",
        "\n",
        "    ########## L89 to L96\n",
        "    ## SOME FILLER LINES TO BE ADDED!!\n",
        "    ##########\n",
        "\n",
        "    # TODO: @Yash For now, the if else statement below is degenerate!!\n",
        "    rescale_parameter = 1\n",
        "    if self.rescaling_scheme in ['constant', 'factoring']: # here we do scaling w/o taking exponential\n",
        "      # TODO: @Yash please make sure to make sure that the lambdas in this case are non-negative like Darryn use a non-negative constraint with tf.\n",
        "      # Naively one can do that after applying optimizer updates to lambdas and taking [np.max(0, i) for i in lambdas].\n",
        "      self.lambdas = np.ones(shape=(n_terms,)) * rescale_parameter\n",
        "    else: # exponential factoring\n",
        "      self.lambdas = np.ones(shape=(n_terms,)) * rescale_parameter\n",
        "\n",
        "\n",
        "  def get_zetas(self):\n",
        "    return self.zetas\n",
        "\n",
        "  # TODO: @Yash self.indices need to be defined L89 to L96\n",
        "  def get_indices(self):\n",
        "    return self.indices\n",
        "\n",
        "  def rescale_lambdas(self, inputs):\n",
        "    batch_dim = inputs[0].shape[0]\n",
        "    tiled_up_thetas = np.tile(self.thetas, reps=[batch_dim, 1])\n",
        "\n",
        "    inputs = inputs.reshape((inputs.shape[0], -1))\n",
        "    tiled_up_inputs = np.tile(inputs[0], reps=[1, self.n_layers])\n",
        "    scaled_inputs = np.einsum('i, ji->ji', self.alphas, tiled_up_inputs)\n",
        "    # L127 is degenerate\n",
        "\n",
        "    # simple rescaling of the lambdas\n",
        "    for i in range(self.n_terms):\n",
        "      # for np\n",
        "      self.lambdas[:] = 1 / self.n_terms\n",
        "      #self.lambdas = jax.ops.index_update(self.lambdas, i, 1 / self.n_terms) # for jax.np\n",
        "\n",
        "    print('rescaled lambdas:', self.lambdas)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    batch_dim = inputs[0].shape[0]\n",
        "    tiled_up_thetas = np.tile(self.thetas, reps=[batch_dim, 1])\n",
        "\n",
        "    inputs = inputs.reshape((inputs.shape[0], -1))\n",
        "    tiled_up_inputs = np.tile(inputs[0], reps=[1, self.n_layers])\n",
        "    scaled_inputs = np.einsum('i, ji->ji', self.alphas, tiled_up_inputs)\n",
        "    # TODO: @Yash check if it is neeeded at all\n",
        "    # squashed_inputs = tf.keras.layers.Activation(self.activation)(scaled_inputs)\n",
        "\n",
        "    ans = np.zeros([batch_dim, 1])\n",
        "    for i in range(self.n_terms):\n",
        "      pqc_layer_ans = jax.lax.complex(np.ones([batch_dim, 1]), np.zeros([batch_dim, 1]))\n",
        "      for k in range(self.n_partitions):\n",
        "        # get circuits\n",
        "        tiled_up_circuits\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OdxzZ0RU-Gvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install equinox\n",
        "!pip install tensorcircuit\n",
        "!pip install qiskit\n",
        "!pip install tensorcircuit\n",
        "!pip install cirq\n",
        "!pip install openfermion\n",
        "!pip install gymnax\n",
        "!pip install brax\n",
        "!pip install distrax\n",
        "# !pip install purejaxrl\n"
      ],
      "metadata": {
        "id": "R7zC4bcG_h2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NekMNA824IAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "from jax import config\n",
        "\n",
        "config.update(\"jax_debug_nans\", True)\n",
        "config.update(\"jax_enable_x64\", True)\n",
        "\n",
        "import jax.numpy as jnp\n",
        "DTYPE=jnp.float64\n",
        "\n",
        "import chex\n",
        "import numpy as np\n",
        "import optax\n",
        "from flax import struct\n",
        "from functools import partial\n",
        "import tensorcircuit as tc\n",
        "import equinox as eqx\n",
        "import types\n",
        "from typing import Union, Sequence, List, NamedTuple, Optional, Tuple, Any\n",
        "import jax.tree_util as jtu\n",
        "from gymnax.environments import environment, spaces\n",
        "from brax import envs\n",
        "from brax.envs.wrappers.training import EpisodeWrapper, AutoResetWrapper\n",
        "\n",
        "K = tc.set_backend(\"jax\")"
      ],
      "metadata": {
        "id": "sQOUMux_4IIC"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/google-deepmind/optax/issues/577\n",
        "# shamelessly taken from Patrick Kidger's issue from optax github. He's a legend, basically converted the optim instance of optax.optimizer to pytree!\n",
        "\n",
        "def _make_cell(val):\n",
        "    fn = lambda: val\n",
        "    return fn.__closure__[0]  # pyright: ignore\n",
        "\n",
        "\n",
        "def _adjust_function_closure(fn, closure):\n",
        "    out = types.FunctionType(\n",
        "        code=fn.__code__,\n",
        "        globals=fn.__globals__,\n",
        "        name=fn.__name__,\n",
        "        argdefs=fn.__defaults__,\n",
        "        closure=closure,\n",
        "    )\n",
        "    out.__module__ = fn.__module__\n",
        "    out.__qualname__ = fn.__qualname__\n",
        "    out.__doc__ = fn.__doc__\n",
        "    out.__annotations__.update(fn.__annotations__)\n",
        "    if fn.__kwdefaults__ is not None:\n",
        "        out.__kwdefaults__ = fn.__kwdefaults__.copy()\n",
        "    return out\n",
        "\n",
        "\n",
        "# Not a pytree.\n",
        "# Used so that two different local functions, with different identities, can still\n",
        "# compare equal. This is needed as these leaves are compared statically when\n",
        "# filter-jit'ing.\n",
        "class _FunctionWithEquality:\n",
        "    def __init__(self, fn: types.FunctionType):\n",
        "        self.fn = fn\n",
        "\n",
        "    def information(self):\n",
        "        return self.fn.__qualname__, self.fn.__module__\n",
        "\n",
        "    def __hash__(self):\n",
        "        return hash(self.information())\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        return type(self) == type(other) and self.information() == other.information()\n",
        "\n",
        "\n",
        "class _Closure(eqx.Module):\n",
        "    fn: _FunctionWithEquality\n",
        "    contents: Optional[tuple[Any, ...]]\n",
        "\n",
        "    def __init__(self, fn: types.FunctionType):\n",
        "        self.fn = _FunctionWithEquality(fn)\n",
        "        if fn.__closure__ is None:\n",
        "            contents = None\n",
        "        else:\n",
        "            contents = tuple(\n",
        "                closure_to_pytree(cell.cell_contents) for cell in fn.__closure__\n",
        "            )\n",
        "        self.contents = contents\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        if self.contents is None:\n",
        "            closure = None\n",
        "        else:\n",
        "            closure = tuple(_make_cell(contents) for contents in self.contents)\n",
        "        fn = _adjust_function_closure(self.fn.fn, closure)\n",
        "        return fn(*args, **kwargs)\n",
        "\n",
        "\n",
        "def _fixup_closure(leaf):\n",
        "    if isinstance(leaf, types.FunctionType):\n",
        "        return _Closure(leaf)\n",
        "    else:\n",
        "        return leaf\n",
        "\n",
        "\n",
        "def closure_to_pytree(tree):\n",
        "    \"\"\"Convert all function closures into pytree nodes.\n",
        "\n",
        "    **Arguments:**\n",
        "\n",
        "    - `tree`: Any pytree.\n",
        "\n",
        "    **Returns:**\n",
        "\n",
        "    A copy of `tree`, where all function closures have been replaced by a new object\n",
        "    that is (a) callable like the original function, but (b) iterates over its\n",
        "    `__closure__` as subnodes in the pytree.\n",
        "\n",
        "    !!! Example\n",
        "\n",
        "        ```python\n",
        "        def some_fn():\n",
        "            a = jnp.array(1.)\n",
        "\n",
        "            @closure_to_pytree\n",
        "            def f(x):\n",
        "                return x + a\n",
        "\n",
        "            print(jax.tree_util.tree_leaves(f))  # prints out `a`\n",
        "        ```\n",
        "\n",
        "    !!! Warning\n",
        "\n",
        "        One annoying technical detail in the above example: we had to wrap the whole lot\n",
        "        in a `some_fn`, so that we're in a local scope. Python treats functions at the\n",
        "        global scope differently, and this conversion won't result in any global\n",
        "        variable being treated as part of the pytree.\n",
        "\n",
        "        In practice, the intended use case of this function is to fix Optax, which\n",
        "        always uses local functions.\n",
        "    \"\"\"\n",
        "    return jtu.tree_map(_fixup_closure, tree)\n",
        "\n",
        "\n",
        "# EXAMPLE USAGE\n",
        "# lr = jnp.array(1e-3)\n",
        "# optim = optax.chain(\n",
        "#     optax.adam(lr),\n",
        "#     optax.scale_by_schedule(optax.piecewise_constant_schedule(1, {200: 0.1})),\n",
        "# )\n",
        "# optim = closure_to_pytree(optim)\n"
      ],
      "metadata": {
        "id": "W5nDq62xoPr-"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# shamelessly taken from purejaxrl: https://github.com/luchris429/purejaxrl/blob/main/purejaxrl/wrappers.py\n",
        "\n",
        "class GymnaxWrapper(object):\n",
        "    \"\"\"Base class for Gymnax wrappers.\"\"\"\n",
        "\n",
        "    def __init__(self, env):\n",
        "        self._env = env\n",
        "\n",
        "    # provide proxy access to regular attributes of wrapped object\n",
        "    def __getattr__(self, name):\n",
        "        return getattr(self._env, name)\n",
        "\n",
        "\n",
        "class FlattenObservationWrapper(GymnaxWrapper):\n",
        "    \"\"\"Flatten the observations of the environment.\"\"\"\n",
        "\n",
        "    def __init__(self, env: environment.Environment):\n",
        "        super().__init__(env)\n",
        "\n",
        "    def observation_space(self, params) -> spaces.Box:\n",
        "        assert isinstance(\n",
        "            self._env.observation_space(params), spaces.Box\n",
        "        ), \"Only Box spaces are supported for now.\"\n",
        "        return spaces.Box(\n",
        "            low=self._env.observation_space(params).low,\n",
        "            high=self._env.observation_space(params).high,\n",
        "            shape=(np.prod(self._env.observation_space(params).shape),),\n",
        "            dtype=self._env.observation_space(params).dtype,\n",
        "        )\n",
        "\n",
        "    @partial(jax.jit, static_argnums=(0,))\n",
        "    def reset(\n",
        "        self, key: chex.PRNGKey, params: Optional[environment.EnvParams] = None\n",
        "    ) -> Tuple[chex.Array, environment.EnvState]:\n",
        "        obs, state = self._env.reset(key, params)\n",
        "        obs = jnp.reshape(obs, (-1,))\n",
        "        return obs, state\n",
        "\n",
        "    @partial(jax.jit, static_argnums=(0,))\n",
        "    def step(\n",
        "        self,\n",
        "        key: chex.PRNGKey,\n",
        "        state: environment.EnvState,\n",
        "        action: Union[int, float],\n",
        "        params: Optional[environment.EnvParams] = None,\n",
        "    ) -> Tuple[chex.Array, environment.EnvState, float, bool, dict]:\n",
        "        obs, state, reward, done, info = self._env.step(key, state, action, params)\n",
        "        obs = jnp.reshape(obs, (-1,))\n",
        "        return obs, state, reward, done, info\n",
        "\n",
        "\n",
        "@struct.dataclass\n",
        "class LogEnvState:\n",
        "    env_state: environment.EnvState\n",
        "    episode_returns: float\n",
        "    episode_lengths: int\n",
        "    returned_episode_returns: float\n",
        "    returned_episode_lengths: int\n",
        "    timestep: int\n",
        "\n",
        "\n",
        "class LogWrapper(GymnaxWrapper):\n",
        "    \"\"\"Log the episode returns and lengths.\"\"\"\n",
        "\n",
        "    def __init__(self, env: environment.Environment):\n",
        "        super().__init__(env)\n",
        "\n",
        "    @partial(jax.jit, static_argnums=(0,))\n",
        "    def reset(\n",
        "        self, key: chex.PRNGKey, params: Optional[environment.EnvParams] = None\n",
        "    ) -> Tuple[chex.Array, environment.EnvState]:\n",
        "        obs, env_state = self._env.reset(key, params)\n",
        "        state = LogEnvState(env_state, 0, 0, 0, 0, 0)\n",
        "        return obs, state\n",
        "\n",
        "    @partial(jax.jit, static_argnums=(0,))\n",
        "    def step(\n",
        "        self,\n",
        "        key: chex.PRNGKey,\n",
        "        state: environment.EnvState,\n",
        "        action: Union[int, float],\n",
        "        params: Optional[environment.EnvParams] = None,\n",
        "    ) -> Tuple[chex.Array, environment.EnvState, float, bool, dict]:\n",
        "        obs, env_state, reward, done, info = self._env.step(\n",
        "            key, state.env_state, action, params\n",
        "        )\n",
        "        new_episode_return = state.episode_returns + reward\n",
        "        new_episode_length = state.episode_lengths + 1\n",
        "        state = LogEnvState(\n",
        "            env_state=env_state,\n",
        "            episode_returns=new_episode_return * (1 - done),\n",
        "            episode_lengths=new_episode_length * (1 - done),\n",
        "            returned_episode_returns=state.returned_episode_returns * (1 - done)\n",
        "            + new_episode_return * done,\n",
        "            returned_episode_lengths=state.returned_episode_lengths * (1 - done)\n",
        "            + new_episode_length * done,\n",
        "            timestep=state.timestep + 1,\n",
        "        )\n",
        "        info[\"returned_episode_returns\"] = state.returned_episode_returns\n",
        "        info[\"returned_episode_lengths\"] = state.returned_episode_lengths\n",
        "        info[\"timestep\"] = state.timestep\n",
        "        info[\"returned_episode\"] = done\n",
        "        return obs, state, reward, done, info"
      ],
      "metadata": {
        "id": "GqUM-1xh4uHw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_circuit(n_qubits, n_layers, rot_params, input_params, X):\n",
        "  circuit = tc.Circuit(n_qubits)\n",
        "  # params = np.random.normal(size=(n_layers + 1, n_qubits, 3))\n",
        "  # inputs = np.random.normal(size=(n_layers, n_qubits))\n",
        "\n",
        "  for l in range(n_layers):\n",
        "    # variational part\n",
        "    for qubit_idx in range(n_qubits):\n",
        "      circuit.rx(qubit_idx, theta=rot_params[l, qubit_idx, 0])\n",
        "      circuit.ry(qubit_idx, theta=rot_params[l, qubit_idx, 1])\n",
        "      circuit.rz(qubit_idx, theta=rot_params[l, qubit_idx, 2])\n",
        "\n",
        "    # entangling part\n",
        "    for qubit_idx in range(n_qubits - 1):\n",
        "      circuit.cnot(qubit_idx, qubit_idx + 1)\n",
        "    if n_qubits != 2:\n",
        "      circuit.cnot(n_qubits - 1, 0)\n",
        "\n",
        "    # encoding part\n",
        "    for qubit_idx in range(n_qubits):\n",
        "      input = X[qubit_idx] * input_params[l, qubit_idx]\n",
        "      circuit.rx(qubit_idx, theta=input)\n",
        "\n",
        "  # last variational part\n",
        "  for qubit_idx in range(n_qubits):\n",
        "    circuit.rx(qubit_idx, theta=rot_params[n_layers, qubit_idx, 0])\n",
        "    circuit.ry(qubit_idx, theta=rot_params[n_layers, qubit_idx, 1])\n",
        "    circuit.rz(qubit_idx, theta=rot_params[n_layers, qubit_idx, 2])\n",
        "\n",
        "  return circuit\n",
        "\n",
        "\n",
        "class PQCLayer(eqx.Module):\n",
        "  theta: jax.Array = eqx.field(converter=jnp.asarray)\n",
        "  lmbd: jax.Array = eqx.field(converter=jnp.asarray)\n",
        "  n_qubits: int = eqx.field(static=True)\n",
        "  n_layers: int = eqx.field(static=True)\n",
        "\n",
        "  def __init__(self, n_qubits: int, n_layers: int, key: int):\n",
        "    key = jax.random.PRNGKey(key)\n",
        "    tkey, lkey = jax.random.split(key, num=2)\n",
        "    self.n_qubits = n_qubits\n",
        "    self.n_layers = n_layers\n",
        "    # rotation_params\n",
        "    self.theta = jax.random.uniform(key=tkey, shape=(n_layers + 1, n_qubits, 3),\n",
        "                                    minval=0.0, maxval=np.pi, dtype=DTYPE)\n",
        "    # input encoding params\n",
        "    # self.lmbd = jnp.ones(shape=(n_layers, n_qubits))\n",
        "    self.lmbd = jax.random.uniform(key=lkey, shape=(n_layers, n_qubits),\n",
        "                                    minval=0.0, maxval=np.pi, dtype=DTYPE)\n",
        "\n",
        "  def __call__(self, inputs):\n",
        "  # def __call__(self, X, n_qubits, depth):\n",
        "\n",
        "    circuit = generate_circuit(self.n_qubits, self.n_layers, self.theta, self.lmbd, inputs)\n",
        "    # state = circuit.state()\n",
        "    # return state\n",
        "    return K.real(circuit.expectation_ps(z=[0,1,2,3]))\n",
        "\n",
        "class Alternating(eqx.Module):\n",
        "  w: jax.Array = eqx.field(converter=jnp.asarray)\n",
        "\n",
        "  def __init__(self, output_dim):\n",
        "    self.w = jnp.array([[(-1.) ** i for i in range(output_dim)]])\n",
        "\n",
        "  def __call__(self, inputs):\n",
        "    return jnp.matmul(inputs, self.w)\n",
        "\n",
        "\n",
        "# class Actor(eqx.Module):\n",
        "#   n_qubits: int\n",
        "#   n_layers: int\n",
        "#   beta: float\n",
        "#   n_actions: Sequence[int]\n",
        "#   key: int\n",
        "\n",
        "#   def __call__(self, x):\n",
        "#     re_uploading_pqc = PQCLayer(n_qubits=self.n_qubits,\n",
        "#                                 n_layers=self.n_layers,\n",
        "#                                 key=self.key)(x)\n",
        "\n",
        "#     process = eqx.nn.Sequential([\n",
        "#         Alternating(self.n_actions),\n",
        "#         eqx.nn.Lambda(lambda x: x * self.beta),\n",
        "#         jax.nn.softmax()\n",
        "#     ])\n",
        "\n",
        "#     policy = process(re_uploading_pqc)\n",
        "\n",
        "#     return policy\n",
        "\n",
        "\n",
        "# the final one which works :)\n",
        "class Actor(eqx.Module):\n",
        "  theta: jax.Array = eqx.field(converter=jnp.asarray)#trainable\n",
        "  lmbd: jax.Array = eqx.field(converter=jnp.asarray)#trainable\n",
        "  w: jax.Array = eqx.field(converter=jnp.asarray)#trainable\n",
        "  n_qubits: int = eqx.field(static=True)\n",
        "  n_layers: int = eqx.field(static=True)\n",
        "  beta: float = eqx.field(static=True)\n",
        "  n_actions: Sequence[int] = eqx.field(static=True)\n",
        "  # key: int\n",
        "\n",
        "  def __init__(self, n_qubits, n_layers, beta, n_actions, key):\n",
        "    self.n_qubits = n_qubits\n",
        "    self.n_layers = n_layers\n",
        "    self.beta = beta\n",
        "    self.n_actions = n_actions\n",
        "    # self.key = key\n",
        "\n",
        "    key = jax.random.PRNGKey(key)\n",
        "    key, _key = jax.random.split(key, num=2)\n",
        "    print(key, _key)\n",
        "    # rotation_params\n",
        "    self.theta = jax.random.uniform(key=_key,\n",
        "                                    shape=(n_layers + 1, n_qubits, 3),\n",
        "                                    minval=0.0, maxval=np.pi)\n",
        "    # input encoding params\n",
        "    self.lmbd = jnp.ones(shape=(n_layers, n_qubits))\n",
        "    # observable weights\n",
        "    self.w = jnp.array([[(-1.) ** i for i in range(n_actions)]])\n",
        "\n",
        "  def re_uploadingpqc(self, inputs):\n",
        "\n",
        "    # circuit = generate_circuit(self.n_qubits, self.n_layers, self.theta, self.lmbd, inputs)\n",
        "    circuit = tc.Circuit(self.n_qubits)\n",
        "\n",
        "    for l in range(self.n_layers):\n",
        "      # variational part\n",
        "      for qubit_idx in range(self.n_qubits):\n",
        "        circuit.rx(qubit_idx, theta=self.theta[l, qubit_idx, 0])\n",
        "        circuit.ry(qubit_idx, theta=self.theta[l, qubit_idx, 1])\n",
        "        circuit.rz(qubit_idx, theta=self.theta[l, qubit_idx, 2])\n",
        "\n",
        "      # entangling part\n",
        "      for qubit_idx in range(self.n_qubits - 1):\n",
        "        circuit.cnot(qubit_idx, qubit_idx + 1)\n",
        "      if self.n_qubits != 2:\n",
        "        circuit.cnot(self.n_qubits - 1, 0)\n",
        "\n",
        "      # encoding part\n",
        "      for qubit_idx in range(self.n_qubits):\n",
        "        linear_input = inputs[qubit_idx] * self.lmbd[l, qubit_idx]\n",
        "        circuit.rx(qubit_idx, theta=linear_input)\n",
        "\n",
        "    # last variational part\n",
        "    for qubit_idx in range(self.n_qubits):\n",
        "      circuit.rx(qubit_idx, theta=self.theta[self.n_layers, qubit_idx, 0])\n",
        "      circuit.ry(qubit_idx, theta=self.theta[self.n_layers, qubit_idx, 1])\n",
        "      circuit.rz(qubit_idx, theta=self.theta[self.n_layers, qubit_idx, 2])\n",
        "\n",
        "    return jnp.real(circuit.expectation_ps(z=[0,1,2,3]))\n",
        "\n",
        "\n",
        "  def alternating(self, inputs):\n",
        "    return jnp.matmul(inputs, self.w)\n",
        "\n",
        "  def get_params(self):\n",
        "    return {\"theta\": self.theta, \"lmbd\": self.lmbd, \"w\": self.w}\n",
        "\n",
        "  def __call__(self, x):\n",
        "\n",
        "    pqc = self.re_uploadingpqc(x)\n",
        "    alt = self.alternating(pqc)\n",
        "\n",
        "    # process = eqx.nn.Sequential([\n",
        "    #     alt,\n",
        "    #     eqx.nn.Lambda(lambda x: x * self.beta),\n",
        "    #     jax.nn.softmax()\n",
        "    # ])\n",
        "    # policy = process(pqc)\n",
        "\n",
        "    actor_mean = eqx.nn.Lambda(lambda x: x * self.beta)(alt)\n",
        "    policy = distrax.Softmax(actor_mean)\n",
        "\n",
        "    return policy\n",
        "\n",
        "\n",
        "# class Actor(eqx.Module):\n",
        "#   n_qubits: int\n",
        "#   n_layers: int\n",
        "#   beta: float\n",
        "#   n_actions: Sequence[int]\n",
        "#   pqc: eqx.Module\n",
        "#   alt: eqx.Module\n",
        "#   key: int\n",
        "\n",
        "#   def __init__(self, n_qubits, n_layers, beta, n_actions, key):\n",
        "#     self.n_qubits = n_qubits\n",
        "#     self.n_layers = n_layers\n",
        "#     self.beta = beta\n",
        "#     self.n_actions = n_actions\n",
        "#     self.key = key\n",
        "\n",
        "#     self.pqc = PQCLayer(n_qubits=self.n_qubits,\n",
        "#                         n_layers=self.n_layers,\n",
        "#                         key=self.key)\n",
        "\n",
        "#     self.alt = Alternating(self.n_actions)\n",
        "\n",
        "#   def __call__(self, x):\n",
        "#     re_uploading_pqc = self.pqc(x)\n",
        "\n",
        "#     process = eqx.nn.Sequential([\n",
        "#         self.alt,\n",
        "#         eqx.nn.Lambda(lambda x: x * self.beta),\n",
        "#         jax.nn.softmax()\n",
        "#     ])\n",
        "\n",
        "#     policy = process(re_uploading_pqc)\n",
        "\n",
        "#     return policy\n",
        "\n",
        "\n",
        "class Transition(NamedTuple):\n",
        "  done: jnp.ndarray\n",
        "  action: jnp.ndarray\n",
        "  value: jnp.ndarray\n",
        "  reward: jnp.ndarray\n",
        "  log_prob: jnp.ndarray\n",
        "  obs: jnp.ndarray\n",
        "  info: jnp.ndarray\n",
        "\n",
        "class TrainState(eqx.Module):\n",
        "    model: eqx.Module\n",
        "    optimizer: optax.GradientTransformation = eqx.field(static=True)\n",
        "    opt_state: optax.OptState\n",
        "\n",
        "    def __init__(self, model, optimizer, opt_state = None):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        if opt_state is None:\n",
        "            self.opt_state = self.optimizer.init(eqx.filter(model, eqx.is_array))\n",
        "        else:\n",
        "            self.opt_state = opt_state\n",
        "\n",
        "    def apply_gradients(self, grads):\n",
        "\n",
        "        updates, opt_state = self.optimizer.update(grads, self.opt_state, self.model)\n",
        "        model = eqx.apply_updates(self.model, updates)\n",
        "        new_train_state = self.__class__(model=model, optimizer=self.optimizer, opt_state=opt_state)\n",
        "        return new_train_state"
      ],
      "metadata": {
        "id": "B0sUXpgDDH_j"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"env_name\": \"CartPole-v1\",\n",
        "          \"n_train_envs\": 1,\n",
        "          \"n_qubits\": 4,\n",
        "          \"n_layers\": 5,\n",
        "          \"max_expisodes\": 1200,\n",
        "          \"batch_size\": 10,\n",
        "          \"agent_name\": \"CP_PG\",\n",
        "          \"gamma\": 1,\n",
        "          \"beta\": 1,\n",
        "          \"lr_in\": 0.1, # input encoding lmbd\n",
        "          \"lr_var\": 0.01, # variational part theta\n",
        "          \"lr_out\": 0.1, # observables Alternating class one\n",
        "          }\n",
        "\n",
        "def make_train(config):\n",
        "\n",
        "  env, env_params = gymnax.make(config[\"env_name\"])\n",
        "  env = FlattenObservationWrapper(env)\n",
        "  env = LogWrapper(env)\n",
        "\n",
        "  def train(rng):\n",
        "\n",
        "    # Initialize network\n",
        "    rng, _rng = jax.random.split(rng)\n",
        "    actor = Actor(config[\"n_qubits\"], config[\"n_layers\"], config[\"beta\"],\n",
        "                    env.action_space(env_params).n, _rng)\n",
        "\n",
        "    # https://github.com/patrick-kidger/equinox/issues/79\n",
        "    param_spec = eqx.filter(actor, eqx.is_inexact_array)\n",
        "    param_spec = eqx.tree_at(lambda actor: actor.theta, param_spec, replace='group0')\n",
        "    param_spec = eqx.tree_at(lambda actor: actor.lmbd, param_spec, replace='group1')\n",
        "    param_spec = eqx.tree_at(lambda actor: actor.w, param_spec, replace='group2')\n",
        "\n",
        "    #TODO: set the learning rates later\n",
        "    optim = optax.multi_transform({\"group0\": optax.adam(1e-2),\n",
        "        \"group1\": optax.adam(1e-1),\n",
        "        \"group2\": optax.adam(1e-6),\n",
        "        },\n",
        "        param_spec\n",
        "    )\n",
        "\n",
        "    optim = closure_to_pytree(optim)\n",
        "\n",
        "    optim_state = optim.init()\n",
        "\n",
        "    # Initialize environment\n",
        "    rng, _rng = jax.random.split(rng)\n",
        "    reset_rng = jax.random.split(_rng, config[\"n_train_envs\"])\n",
        "    obsv, env_state = jax.vmap(env.reset, in_axes=(0, None))(reset_rng, env_params)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "id": "f1s913yQ3Uvs",
        "outputId": "aa08cabc-4c74-4a27-eaa5-4c8350966df8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'gymnax'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-b1084217d2da>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgymnax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gymnax'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "actor = Actor(n_qubits=4, n_layers=5, beta=1.0, n_actions=2, key=42)\n",
        "param_spec = eqx.filter(actor, eqx.is_inexact_array)\n",
        "# # param_spec = jax.tree_map(lambda _: \"NT\", actor)\n",
        "param_spec = eqx.tree_at(lambda actor: actor.theta, param_spec, replace='group0')\n",
        "param_spec = eqx.tree_at(lambda actor: actor.lmbd, param_spec, replace='group1')\n",
        "param_spec = eqx.tree_at(lambda actor: actor.w, param_spec, replace='group2')\n",
        "\n",
        "optim = optax.multi_transform({\"group0\": optax.adam(1e-1),\n",
        "    \"group1\": optax.adam(1e-0),\n",
        "    \"group2\": optax.adam(1e-6),\n",
        "    },\n",
        "    param_spec\n",
        ")\n",
        "\n",
        "\n",
        "# param_spec = jax.tree_map(lambda _: \"group0\", actor)\n",
        "\n",
        "# Set parameter groups\n",
        "# param_spec = eqx.tree_at(lambda actor: actor.theta, param_spec, replace='group1')\n",
        "# param_spec = eqx.tree_at(lambda actor: actor.lmbd, param_spec, replace='group2')\n",
        "# param_spec = eqx.tree_at(lambda actor: actor.w, param_spec, replace='group3')\n",
        "\n",
        "# optim = optax.multi_transform(\n",
        "#     {\"group0\": optax.adam(0.0),\n",
        "#      \"group1\": optax.adam(1e-0),\n",
        "#      \"group2\": optax.adam(1e-6),\n",
        "#      \"group3\": optax.adam(1e-5),\n",
        "#     },\n",
        "#     param_spec\n",
        "# )\n",
        "\n",
        "# optim = closure_to_pytree(optim)\n",
        "\n",
        "opt_state = optim.init(param_spec)\n",
        "\n",
        "# def _update_step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "VR0tFvN4tnSW",
        "outputId": "43a603bc-7d53-4f3c-d3f8-66641e3027a7"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2465931498 3679230171] [255383827 267815257]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "string indices must be integers",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-130-1f51493ce20c>\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# optim = closure_to_pytree(optim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mopt_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# def _update_step()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optax/_src/combine.py\u001b[0m in \u001b[0;36minit_fn\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0minit_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_labels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mparam_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0mlabel_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_leaves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-129-ef3205032ef1>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    161\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m     \u001b[0mpqc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mre_uploadingpqc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m     \u001b[0malt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malternating\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpqc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m<ipython-input-129-ef3205032ef1>\u001b[0m in \u001b[0;36mre_uploadingpqc\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    129\u001b[0m       \u001b[0;31m# variational part\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mqubit_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_qubits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0mcircuit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqubit_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqubit_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0mcircuit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqubit_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqubit_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mcircuit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqubit_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqubit_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "param_spec = eqx.filter(actor, eqx.is_inexact_array)\n",
        "param_spec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IV4p_2iQnLyH",
        "outputId": "89eddf51-2cb7-439d-9259-ea62c9d6c648"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Actor(\n",
              "  theta=f32[6,4,3],\n",
              "  lmbd=f32[5,4],\n",
              "  w=f32[1,2],\n",
              "  n_qubits=4,\n",
              "  n_layers=5,\n",
              "  beta=1.0,\n",
              "  n_actions=2\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "jbAqBPgPk0a2",
        "outputId": "aab8c9ed-0a5e-424d-854e-f3c68057294f"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'str' object cannot be interpreted as an integer",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-83-8391f59af8b9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCircuit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorcircuit/circuit.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, nqubits, inputs, mps_inputs, split)\u001b[0m\n\u001b[1;32m     72\u001b[0m         }\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmps_inputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mnodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_zero_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnqubits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_front\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_edge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# provide input function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorcircuit/basecircuit.py\u001b[0m in \u001b[0;36mall_zero_nodes\u001b[0;34m(n, d, prefix)\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             )\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         ]\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'str' object cannot be interpreted as an integer"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "jax.tree_map(actor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "-8znFvkfhM_2",
        "outputId": "811cdd75-48d3-4986-8ad0-b1804eb62c88"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "tree_map() missing 1 required positional argument: 'tree'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-79-01d3dd3092ad>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: tree_map() missing 1 required positional argument: 'tree'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4A0ZiZ2bGOoe",
        "outputId": "06f451bf-a6b4-491f-b7a0-ceeae6843196"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Actor(\n",
              "   theta=None,\n",
              "   lmbd=None,\n",
              "   w=None,\n",
              "   n_qubits=None,\n",
              "   n_layers=None,\n",
              "   beta=None,\n",
              "   n_actions=None\n",
              " )]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def _update_step(runner_state, unused):\n",
        "    # COLLECT TRAJECTORIES\n",
        "    def _env_step(runner_state, unused):\n",
        "        train_state, env_state, last_obs, rng = runner_state\n",
        "\n",
        "        # SELECT ACTION\n",
        "        rng, _rng = jax.random.split(rng)\n",
        "        pi, value = network.apply(train_state.params, last_obs)\n",
        "        action = pi.sample(seed=_rng)\n",
        "        log_prob = pi.log_prob(action)\n",
        "\n",
        "        # STEP ENV\n",
        "        rng, _rng = jax.random.split(rng)\n",
        "        rng_step = jax.random.split(_rng, config[\"NUM_ENVS\"])\n",
        "        obsv, env_state, reward, done, info = jax.vmap(\n",
        "            env.step, in_axes=(0, 0, 0, None)\n",
        "        )(rng_step, env_state, action, env_params)\n",
        "        transition = Transition(\n",
        "            done, action, value, reward, log_prob, last_obs, info\n",
        "        )\n",
        "        runner_state = (train_state, env_state, obsv, rng)\n",
        "        return runner_state, transition"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "Wf3JWNaOlhvm",
        "outputId": "4f890224-e080-4338-ed65-154af9200afa"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Actor.__call__ of Actor(\n",
              "  theta=f64[6,4,3],\n",
              "  lmbd=f64[5,4],\n",
              "  w=f64[1,2],\n",
              "  n_qubits=4,\n",
              "  n_layers=5,\n",
              "  beta=1.0,\n",
              "  n_actions=2\n",
              ")>"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>Actor.__call__</b><br/>def __call__(x)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/content/&lt;ipython-input-36-e34060e76266&gt;</a>Call self as a function.</pre></div>"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "has_theta = lambda x: hasattr(x, \"theta\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UE3TO_PiixMi",
        "outputId": "5e0d000b-70be-45c2-a052-6ae8e747f473"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import equinox as eqx\n",
        "import jax\n",
        "import jax.random as jr\n",
        "import optax\n",
        "\n",
        "key1, key2 = jr.split(jr.PRNGKey(0))\n",
        "mlp1 = eqx.nn.MLP(2, 2, 2, 2, key=key1)\n",
        "mlp2 = eqx.nn.MLP(2, 2, 2, 2, key=key2)\n",
        "# Example model. In its interaction with `optax.multi_transform`, all that matters\n",
        "# is that it is some PyTree of parameters.\n",
        "model = (mlp1, mlp2)"
      ],
      "metadata": {
        "id": "ja4pyg4IJ8rA"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(eqx.filter(model, eqx.is_inexact_array))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbrTToX8eyd7",
        "outputId": "a57badc6-687d-4b85-dbb0-b196c4982883"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eqx.filter(actor, eqx.is_inexact_array)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpeaTBnhKab8",
        "outputId": "f38758a8-cff4-49fe-9261-b4a49e4f21a5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Actor(n_qubits=None, n_layers=None, beta=None, n_actions=None, key=None)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AAWytGkSEjEs"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "re_uploadingpqc.theta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2h5NJIIOOyp",
        "outputId": "5a159e28-5304-4c7e-9216-c8a04ef714e1"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([[[1.5144717 , 1.9982332 , 0.6312601 ],\n",
              "        [0.18876766, 2.6841137 , 2.5112891 ],\n",
              "        [2.9993846 , 0.79624313, 2.8496668 ],\n",
              "        [0.14933394, 1.7980386 , 0.02239964]],\n",
              "\n",
              "       [[0.9860115 , 1.8502505 , 2.2118742 ],\n",
              "        [3.108764  , 3.0203154 , 1.2054719 ],\n",
              "        [2.1640604 , 1.2004793 , 2.2377698 ],\n",
              "        [1.8221438 , 0.43731636, 1.2987039 ]],\n",
              "\n",
              "       [[2.2190378 , 2.060944  , 1.9459078 ],\n",
              "        [1.8514849 , 0.5357076 , 1.7158291 ],\n",
              "        [1.4256872 , 2.901676  , 0.81299675],\n",
              "        [0.33294687, 2.785755  , 2.9518187 ]],\n",
              "\n",
              "       [[1.2626755 , 0.00724034, 1.9943507 ],\n",
              "        [1.5947442 , 0.10624278, 2.8188238 ],\n",
              "        [1.8898127 , 1.6097226 , 0.54341084],\n",
              "        [2.5614724 , 0.62843066, 1.6441184 ]],\n",
              "\n",
              "       [[1.3262455 , 1.4257475 , 0.2947449 ],\n",
              "        [0.03931874, 2.7925334 , 0.5836108 ],\n",
              "        [0.9254372 , 2.547386  , 1.6627982 ],\n",
              "        [1.6727308 , 2.674259  , 0.26866385]],\n",
              "\n",
              "       [[2.620658  , 2.3397074 , 1.0249314 ],\n",
              "        [2.540169  , 1.4397433 , 1.4750589 ],\n",
              "        [2.0233185 , 0.5248312 , 2.2866712 ],\n",
              "        [1.8359305 , 0.83510125, 2.513384  ]]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_circuit(n_qubits, n_layers, rot_params, input_params, X):\n",
        "  circuit = tc.Circuit(n_qubits)\n",
        "  # params = np.random.normal(size=(n_layers + 1, n_qubits, 3))\n",
        "  # inputs = np.random.normal(size=(n_layers, n_qubits))\n",
        "\n",
        "  for l in range(n_layers):\n",
        "    # variational part\n",
        "    for qubit_idx in range(n_qubits):\n",
        "      circuit.rx(qubit_idx, theta=rot_params[l, qubit_idx, 0])\n",
        "      circuit.ry(qubit_idx, theta=rot_params[l, qubit_idx, 1])\n",
        "      circuit.rz(qubit_idx, theta=rot_params[l, qubit_idx, 2])\n",
        "\n",
        "    # entangling part\n",
        "    for qubit_idx in range(n_qubits - 1):\n",
        "      circuit.cnot(qubit_idx, qubit_idx + 1)\n",
        "    if n_qubits != 2:\n",
        "      circuit.cnot(n_qubits - 1, 0)\n",
        "\n",
        "    # encoding part\n",
        "    for qubit_idx in range(n_qubits):\n",
        "      input = X[qubit_idx] * input_params[l, qubit_idx]\n",
        "      circuit.rx(qubit_idx, theta=input)\n",
        "\n",
        "  # last variational part\n",
        "  for qubit_idx in range(n_qubits):\n",
        "    circuit.rx(qubit_idx, theta=rot_params[n_layers, qubit_idx, 0])\n",
        "    circuit.ry(qubit_idx, theta=rot_params[n_layers, qubit_idx, 1])\n",
        "    circuit.rz(qubit_idx, theta=rot_params[n_layers, qubit_idx, 2])\n",
        "\n",
        "  return circuit\n",
        "\n",
        "\n",
        "class PQCLayer(eqx.Module):\n",
        "  theta: jax.Array\n",
        "  lmbd: jax.Array\n",
        "  n_qubits: int\n",
        "  n_layers: int\n",
        "\n",
        "  def __init__(self, n_qubits: int, n_layers: int, key: int):\n",
        "    key = jax.random.PRNGKey(key)\n",
        "    tkey, lkey = jax.random.split(key, num=2)\n",
        "    self.n_qubits = n_qubits\n",
        "    self.n_layers = n_layers\n",
        "    # rotation_params\n",
        "    self.theta = jax.random.uniform(key=tkey, shape=(n_layers + 1, n_qubits, 3),\n",
        "                                    minval=0.0, maxval=np.pi)\n",
        "    # input encoding params\n",
        "    # self.lmbd = jnp.ones(shape=(n_layers, n_qubits))\n",
        "    self.lmbd = jax.random.uniform(key=lkey, shape=(n_layers, n_qubits),\n",
        "                                    minval=0.0, maxval=np.pi)\n",
        "\n",
        "  def __call__(self, inputs):\n",
        "  # def __call__(self, X, n_qubits, depth):\n",
        "\n",
        "    circuit = generate_circuit(self.n_qubits, self.n_layers, self.theta, self.lmbd, inputs)\n",
        "    # state = circuit.state()\n",
        "    # return state\n",
        "    return K.real(circuit.expectation_ps(z=np.arange(self.n_qubits)))\n",
        "\n",
        "class Alternating(eqx.Module):\n",
        "  w: jax.Array\n",
        "\n",
        "  def __init__(self, output_dim):\n",
        "    self.w = jnp.array([[(-1.) ** i for i in range(output_dim)]])\n",
        "\n",
        "  def __call__(self, inputs):\n",
        "    return jnp.matmul(inputs, self.w)\n",
        "\n",
        "\n",
        "class Actor(eqx.Module):\n",
        "  n_qubits: int\n",
        "  n_layers: int\n",
        "  beta: float\n",
        "  n_actions: Sequence[int]\n",
        "  key: int\n",
        "\n",
        "  def __call__(self, x):\n",
        "    re_uploading_pqc = PQCLayer(n_qubits=self.n_qubits,\n",
        "                                n_layers=self.n_layers,\n",
        "                                key=self.key)(x)\n",
        "\n",
        "    process = eqx.nn.Sequential([\n",
        "        Alternating(self.n_actions),\n",
        "        eqx.nn.Lambda(lambda x: x * self.beta),\n",
        "        distrax.Softmax()\n",
        "        # jax.nn.softmax()\n",
        "    ])\n",
        "\n",
        "    policy = process(re_uploading_pqc)\n",
        "\n",
        "    return policy\n",
        "\n",
        "\n",
        "actor = Actor(n_qubits=4, n_layers=5, beta=1.0, n_actions=2, key=42)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zTiKbz-hEpcm"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pred(model, x):\n",
        "  return jax.vmap(model)(x)\n",
        "\n",
        "def loss(model, x, y):\n",
        "  y_pred = jax.vmap(model)(x)\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "xsgu5iIYRxwH"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "re_uploadingpqc = PQCLayer(n_qubits=4, n_layers=5, key=42)\n",
        "out = pred(re_uploadingpqc, x_train_batch)\n",
        "out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Y0p_fBzR8yO",
        "outputId": "4bae1dc5-3a81-4956-b5a2-e4508456f4eb"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([ 0.20385608, -0.00696721,  0.25116295,  0.4170586 ,  0.04645425,\n",
              "        0.1618064 , -0.11655234,  0.14391507,  0.2532135 , -0.04269086,\n",
              "        0.00732983, -0.00413742, -0.45031813,  0.13783413, -0.1513946 ,\n",
              "       -0.06666791,  0.09114354, -0.3780465 , -0.27665052, -0.19483098,\n",
              "        0.04257975, -0.03343487, -0.30175844,  0.19807652,  0.29727694,\n",
              "        0.30802915, -0.05575262, -0.20934898, -0.40040857,  0.09278246,\n",
              "        0.23634967,  0.4170586 ], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gdSDHEGmSxWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@eqx.filter_value_and_grad\n",
        "def compute_loss(\n",
        "    model, x, y\n",
        "):\n",
        "    # Our input has the shape (BATCH_SIZE, 1, 28, 28), but our model operations on\n",
        "    # a single input input image of shape (1, 28, 28).\n",
        "    #\n",
        "    # Therefore, we have to use jax.vmap, which in this case maps our model over the\n",
        "    # leading (batch) axis.\n",
        "    # pred_y = model(x)\n",
        "    pred_y = jax.vmap(model)(x)\n",
        "    loss = jnp.maximum(0, 1 - (2.0 * y - 1.0) * pred_y)\n",
        "    return jnp.mean(loss)\n",
        "\n",
        "\n",
        "# def cross_entropy(y, pred_y):\n",
        "#     # y are the true targets, and should be integers 0-9.\n",
        "#     # pred_y are the log-softmax'd predictions.\n",
        "#     pred_y = jnp.take_along_axis(pred_y, jnp.expand_dims(y, 1), axis=1)\n",
        "#     return -jnp.mean(pred_y)"
      ],
      "metadata": {
        "id": "LFu0kL7pSSum"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss, grad = compute_loss(re_uploadingpqc, x_train_batch, y_train_batch)"
      ],
      "metadata": {
        "id": "4SXq9ozRSaBX"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "re_uploadingpqc.theta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5GdA9JjW1aH",
        "outputId": "fb0d089a-ab8b-4c04-aef9-a798574a1c5c"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([[[0.84259273, 0.94874143, 0.90688467],\n",
              "        [2.6667356 , 0.76266714, 1.52313363],\n",
              "        [1.06774572, 2.69004876, 1.00810876],\n",
              "        [2.45738273, 2.02423163, 1.71877908]],\n",
              "\n",
              "       [[2.68148044, 0.39126363, 0.20946645],\n",
              "        [0.70720533, 0.3897383 , 1.57548773],\n",
              "        [0.0434015 , 1.72885032, 2.6853669 ],\n",
              "        [3.06993527, 1.55593812, 1.80930546]],\n",
              "\n",
              "       [[0.79809019, 2.51548217, 3.08320155],\n",
              "        [0.66879265, 1.53784092, 2.43956667],\n",
              "        [2.25907099, 2.24584914, 2.13072184],\n",
              "        [2.84810735, 2.66760658, 2.59974501]],\n",
              "\n",
              "       [[0.67692702, 0.98194887, 2.8746463 ],\n",
              "        [1.54491374, 1.59811206, 0.95195518],\n",
              "        [1.65929299, 1.70987385, 1.79819793],\n",
              "        [2.21201573, 0.95468079, 1.83464712]],\n",
              "\n",
              "       [[2.71668027, 1.03779524, 2.34367556],\n",
              "        [2.83988658, 1.66392737, 1.57174783],\n",
              "        [2.08979208, 2.66964127, 0.54374638],\n",
              "        [0.03565717, 0.21567255, 1.88023176]],\n",
              "\n",
              "       [[0.68780493, 0.15546439, 2.48979877],\n",
              "        [1.87962935, 1.77782177, 2.84671802],\n",
              "        [1.14221602, 2.23558458, 0.38788242],\n",
              "        [2.82405449, 2.84739362, 2.15755409]]], dtype=float64)"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "re_uploadingpqc.lmbd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sI35Xe4pXaUb",
        "outputId": "94f01390-6729-42df-aa8d-8b42c8030a8c"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([[1.25229043, 2.08540908, 2.85281107, 1.15272227],\n",
              "       [0.42103296, 2.97277102, 2.21023358, 2.43972666],\n",
              "       [2.07503455, 2.72435274, 0.77456381, 2.99874712],\n",
              "       [2.77866979, 0.77397793, 0.97898353, 2.05977496],\n",
              "       [0.62252303, 1.43096647, 0.61955182, 2.38200524]], dtype=float64)"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @eqx.filter_value_and_grad\n",
        "# def compute_loss(model, x, y):\n",
        "#     pred_y = jax.vmap(model)(x)\n",
        "#     # Trains with respect to binary cross-entropy\n",
        "#     return jnp.mean(jnp.maximum(0, 1 - (2.0 * y - 1.0) * pred_y))\n",
        "\n",
        "# compute_loss(re_uploadingpqc, x_train_batch, y_train_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTum4A4YMorn",
        "outputId": "48cc83c4-9f09-46b4-ef1a-ce9628ddce9e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Array(1.3310941, dtype=float32),\n",
              " PQCLayer(theta=f32[6,4,3], lmbd=f32[5,4], n_qubits=None, n_layers=None))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "x_train, x_test = x_train[..., np.newaxis] / 255.0, x_test[..., np.newaxis] / 255.0  # normalize the data\n",
        "\n",
        "def filter(x, y, a, b):\n",
        "    keep = (y == a) | (y == b)\n",
        "    x, y = x[keep], y[keep]\n",
        "    y = y == a\n",
        "    return x, y\n",
        "\n",
        "# Filter out classes 0 and 1\n",
        "x_train, y_train = filter(x_train, y_train, 0, 1)\n",
        "x_test, y_test = filter(x_test, y_test, 0, 1)\n",
        "\n",
        "def apply_pca(X, n_components):\n",
        "    X_flat = np.array([x.flatten() for x in X])\n",
        "    pca = PCA(n_components=n_components)\n",
        "    X_pca = pca.fit_transform(X_flat)\n",
        "    return X_pca\n",
        "\n",
        "n_components = 4\n",
        "x_train = apply_pca(x_train, n_components)\n",
        "x_test = apply_pca(x_test, n_components)\n",
        "\n",
        "x_train = jnp.array(x_train, dtype=DTYPE)\n",
        "x_test = jnp.array(x_test, dtype=DTYPE)\n",
        "y_train = jnp.array(y_train, dtype=DTYPE)\n",
        "y_test = jnp.array(y_test, dtype=DTYPE)\n",
        "\n",
        "\n",
        "\n",
        "class PQCLayer(eqx.Module):\n",
        "  theta: jax.Array = eqx.field(converter=jnp.asarray)\n",
        "  lmbd: jax.Array = eqx.field(converter=jnp.asarray)\n",
        "  n_qubits: int = eqx.field(static=True)\n",
        "  n_layers: int = eqx.field(static=True)\n",
        "\n",
        "  def __init__(self, n_qubits: int, n_layers: int, key: int):\n",
        "    key = jax.random.PRNGKey(key)\n",
        "    tkey, lkey = jax.random.split(key, num=2)\n",
        "    self.n_qubits = n_qubits\n",
        "    self.n_layers = n_layers\n",
        "    # rotation_params\n",
        "    self.theta = jax.random.uniform(key=tkey, shape=(n_layers + 1, n_qubits, 3),\n",
        "                                    minval=0.0, maxval=np.pi, dtype=DTYPE)\n",
        "    # input encoding params\n",
        "    # self.lmbd = jnp.ones(shape=(n_layers, n_qubits))\n",
        "    self.lmbd = jax.random.uniform(key=lkey, shape=(n_layers, n_qubits),\n",
        "                                    minval=0.0, maxval=np.pi, dtype=DTYPE)\n",
        "\n",
        "  def __call__(self, inputs):\n",
        "  # def __call__(self, X, n_qubits, depth):\n",
        "\n",
        "    # circuit = generate_circuit(self.n_qubits, self.n_layers, self.theta, self.lmbd, inputs)\n",
        "    circuit = tc.Circuit(self.n_qubits)\n",
        "\n",
        "    for l in range(self.n_layers):\n",
        "      # variational part\n",
        "      for qubit_idx in range(self.n_qubits):\n",
        "        circuit.rx(qubit_idx, theta=self.theta[l, qubit_idx, 0])\n",
        "        circuit.ry(qubit_idx, theta=self.theta[l, qubit_idx, 1])\n",
        "        circuit.rz(qubit_idx, theta=self.theta[l, qubit_idx, 2])\n",
        "\n",
        "      # entangling part\n",
        "      for qubit_idx in range(self.n_qubits - 1):\n",
        "        circuit.cnot(qubit_idx, qubit_idx + 1)\n",
        "      if self.n_qubits != 2:\n",
        "        circuit.cnot(self.n_qubits - 1, 0)\n",
        "\n",
        "      # encoding part\n",
        "      for qubit_idx in range(self.n_qubits):\n",
        "        linear_input = inputs[qubit_idx] * self.lmbd[l, qubit_idx]\n",
        "        circuit.rx(qubit_idx, theta=linear_input)\n",
        "\n",
        "    # last variational part\n",
        "    for qubit_idx in range(self.n_qubits):\n",
        "      circuit.rx(qubit_idx, theta=self.theta[self.n_layers, qubit_idx, 0])\n",
        "      circuit.ry(qubit_idx, theta=self.theta[self.n_layers, qubit_idx, 1])\n",
        "      circuit.rz(qubit_idx, theta=self.theta[self.n_layers, qubit_idx, 2])\n",
        "\n",
        "    return jnp.real(circuit.expectation_ps(z=[0,1,2,3]))\n",
        "\n",
        "re_uploadingpqc = PQCLayer(n_qubits=4, n_layers=5, key=600)\n",
        "t0 = re_uploadingpqc.theta\n",
        "l0 = re_uploadingpqc.lmbd\n"
      ],
      "metadata": {
        "id": "mP7y8LZcXDtO"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "re_uploadingpqc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AecjtFxXZl9t",
        "outputId": "7e853846-f704-473e-cb21-cf991a01aa68"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PQCLayer(theta=f64[6,4,3], lmbd=f64[5,4], n_qubits=4, n_layers=5)"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "param_spec = ('theta', 'lmbd')"
      ],
      "metadata": {
        "id": "YHU2_mILZv_Z"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optim = optax.multi_transform(\n",
        "    {'theta': optax.adam(1e-1), 'lmbd': optax.adam(1e-2)},\n",
        "    param_spec\n",
        ")"
      ],
      "metadata": {
        "id": "99mkNyUQjq1b"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optim.init(re_uploadingpqc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "Y1kHw0ptjzSk",
        "outputId": "907f6acf-3f0f-4c22-dc81-6a0aec110b46"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Expected tuple, got PQCLayer(theta=f64[6,4,3], lmbd=f64[5,4], n_qubits=4, n_layers=5).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-124-0b83f4590d44>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre_uploadingpqc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optax/_src/combine.py\u001b[0m in \u001b[0;36minit_fn\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m    212\u001b[0m                        f'Transforms keys: {list(sorted(transforms.keys()))} \\n')\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m     inner_states = {\n\u001b[0m\u001b[1;32m    215\u001b[0m         group: wrappers.masked(\n\u001b[1;32m    216\u001b[0m             \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmake_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optax/_src/combine.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    215\u001b[0m         group: wrappers.masked(\n\u001b[1;32m    216\u001b[0m             \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmake_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             mask_compatible_extra_args=mask_compatible_extra_args).init(params)\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     }\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optax/_src/wrappers.py\u001b[0m in \u001b[0;36minit_fn\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[0mmask_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m     \u001b[0mmasked_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask_pytree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_tree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    546\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mMaskedState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minner_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasked_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optax/_src/wrappers.py\u001b[0m in \u001b[0;36mmask_pytree\u001b[0;34m(pytree, mask_tree)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mmask_pytree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpytree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_tree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m     return jax.tree_util.tree_map(\n\u001b[0m\u001b[1;32m    510\u001b[0m         \u001b[0;32mlambda\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mMaskedNode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_tree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpytree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/tree_util.py\u001b[0m in \u001b[0;36mtree_map\u001b[0;34m(f, tree, is_leaf, *rest)\u001b[0m\n\u001b[1;32m    241\u001b[0m   \"\"\"\n\u001b[1;32m    242\u001b[0m   \u001b[0mleaves\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtreedef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_leaf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m   \u001b[0mall_leaves\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mleaves\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtreedef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_up_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtreedef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mxs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mall_leaves\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/tree_util.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    241\u001b[0m   \"\"\"\n\u001b[1;32m    242\u001b[0m   \u001b[0mleaves\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtreedef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_leaf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m   \u001b[0mall_leaves\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mleaves\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtreedef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_up_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtreedef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mxs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mall_leaves\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected tuple, got PQCLayer(theta=f64[6,4,3], lmbd=f64[5,4], n_qubits=4, n_layers=5)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# param_spec = eqx.tree_at(lambda actor: actor.lmbd, eqx.tree_at(lambda actor: actor.theta, eqx.filter(re_uploadingpqc, eqx.is_array), replace='group0'), replace='group1')\n",
        "\n",
        "optim = optax.multi_transform({\"group0\": optax.adam(1e-1),\n",
        "    \"group1\": optax.adam(1e-0),\n",
        "    },\n",
        "    param_spec\n",
        ")"
      ],
      "metadata": {
        "id": "jyiTrJIda-ac"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_spec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoeufzuWcvDN",
        "outputId": "574b6f01-b308-405d-f5e4-07cfb673ad72"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PQCLayer(theta='group0', lmbd='group1', n_qubits=4, n_layers=5)"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pt = tree_structure(re_uploadingpqc)"
      ],
      "metadata": {
        "id": "QpXo99g6cCxE"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMjhsR_bK-h1",
        "outputId": "4f77793f-5f90-4a46-de10-1ebb38aad66c"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PyTreeDef(CustomNode(PQCLayer[('theta', 'lmbd'), ('n_qubits', 'n_layers'), (4, 5)], [*, *]))"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pt.Custo"
      ],
      "metadata": {
        "id": "pwYBTjYzpaiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optim1 = optax.adam(1e-2)\n",
        "# optim1_state = optim1.init(re_uploadingpqc.theta)\n",
        "\n",
        "# optim2 = optax.adam(1e-3)\n",
        "# optim2_state = optim2.init(re_uploadingpqc.lmbd)\n",
        "\n",
        "optim = optax.adam(1e-2)\n",
        "optim = closure_to_pytree(optim)\n",
        "opt_state = optim.init(re_uploadingpqc)\n",
        "\n",
        "\n",
        "batch_idx = np.random.randint(0, len(x_train), 32)\n",
        "x_train_batch = jnp.array([x_train[i] for i in batch_idx], dtype=DTYPE)\n",
        "y_train_batch = jnp.array([y_train[i] for i in batch_idx], dtype=DTYPE)\n",
        "y_train_batch = jnp.array([int(value) for value in y_train_batch], dtype=DTYPE)\n",
        "\n",
        "@eqx.filter_value_and_grad\n",
        "def compute_loss(model, x, y):\n",
        "    pred_y = jax.vmap(model)(x)\n",
        "    # pred_y = jnp.real(pred_y)\n",
        "    # pred_y = jnp.real(pred_y, dtype=DTYPE)\n",
        "    loss = jnp.maximum(0, 1 - (2.0 * y - 1.0) * pred_y)\n",
        "    # loss = y * jnp.log((pred_y) + 1e-8) + (1 - y) * jnp.log((1 - pred_y) + 1e-8)\n",
        "    # print(f'loss = {loss}')\n",
        "    # return jnp.mean(jnp.real(loss))\n",
        "    return jnp.mean(loss)\n",
        "\n",
        "loss, grads = compute_loss(re_uploadingpqc, x_train_batch, y_train_batch)"
      ],
      "metadata": {
        "id": "78SWK6y0J39T"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eDykoCGOUR1",
        "outputId": "1077e97e-cec7-413d-e5a3-671c3121fa91"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array(-0.86930787, dtype=float64)"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# optim = optax.adam(1e-2)\n",
        "# opt_state = optim.init(model)\n",
        "\n",
        "# param_spec = eqx.filter(re_uploadingpqc, eqx.is_inexact_array)\n",
        "# param_spec = eqx.tree_at(lambda re_uploadingpqc: re_uploadingpqc.theta, param_spec, replace='group0')\n",
        "# param_spec = eqx.tree_at(lambda re_uploadingpqc: re_uploadingpqc.lmbd, param_spec, replace='group1')\n",
        "\n",
        "# optim_param_spec = optax.multi_transform({\"group0\": optax.adam(1e-1),\n",
        "#     \"group1\": optax.adam(1e-2)},\n",
        "#     param_spec\n",
        "# )\n",
        "\n",
        "# opt_state_param_spec = optim_param_spec.init(re_uploadingpqc)\n",
        "\n",
        "\n",
        "re_uploadingpqc = PQCLayer(n_qubits=4, n_layers=5, key=600)\n",
        "t0 = re_uploadingpqc.theta\n",
        "l0 = re_uploadingpqc.lmbd\n",
        "\n",
        "\n",
        "\n",
        "# @eqx.filter_jit\n",
        "# def make_step(model, x, y, opt_state1, opt_state2):\n",
        "#     loss, grads = compute_loss(model, x, y)\n",
        "#     updates1, opt_state1 = optim1.update(grads, opt_state1)\n",
        "#     updates2, opt_state2 = optim2.update(grads, opt_state2)\n",
        "#     model = eqx.apply_updates(model, updates1)\n",
        "#     model  eqx.apply_updates(model, updates2)\n",
        "#     # model = eqx.apply_updates(model, updates1)\n",
        "#     return loss, updates1, updates2, opt_state1, opt_state2\n",
        "\n",
        "@eqx.filter_jit\n",
        "def make_step(model, x, y, opt_state):\n",
        "    loss, grads = compute_loss(model, x, y)\n",
        "    updates, opt_state = optim.update(grads, opt_state)\n",
        "    model = eqx.apply_updates(model, updates)\n",
        "    return loss, model, opt_state\n",
        "\n",
        "# optim1 = optax.adam(1e-2)\n",
        "# opt_state1 = optim1.init(re_uploadingpqc.theta)\n",
        "\n",
        "# optim2 = optax.adam(1e-1)\n",
        "# opt_state2 = optim2.init(re_uploadingpqc.lmbd)\n",
        "\n",
        "# l, u1, u2, os1, os2 = make_step(re_uploadingpqc, x_train_batch, y_train_batch, opt_state1, opt_state2)\n",
        "\n",
        "optim = optax.adam(1e-2)\n",
        "optim = closure_to_pytree(optim)\n",
        "opt_state = optim.init(re_uploadingpqc)\n",
        "\n",
        "steps = 100\n",
        "\n",
        "for step in range(steps):\n",
        "  batch_idx = np.random.randint(0, len(x_train), 32)\n",
        "  x_train_batch = np.array([x_train[i] for i in batch_idx])\n",
        "  y_train_batch = np.array([y_train[i] for i in batch_idx])\n",
        "  y_train_batch = np.array([int(value) for value in y_train_batch])\n",
        "\n",
        "  loss, re_uploadingpqc, opt_state = make_step(re_uploadingpqc, x_train_batch, y_train_batch, opt_state)\n",
        "  loss = loss.item()\n",
        "  print(f\"step={step}, loss={loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfeysEufXPkx",
        "outputId": "2efd57d9-0313-4192-fb7f-46697b669cfd"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step=0, loss=1.0402872562408447\n",
            "step=1, loss=0.9897873997688293\n",
            "step=2, loss=0.9744366407394409\n",
            "step=3, loss=0.9225523471832275\n",
            "step=4, loss=1.0374821424484253\n",
            "step=5, loss=0.9024378657341003\n",
            "step=6, loss=1.0267958641052246\n",
            "step=7, loss=0.9772196412086487\n",
            "step=8, loss=1.0545833110809326\n",
            "step=9, loss=0.9997799396514893\n",
            "step=10, loss=1.0414196252822876\n",
            "step=11, loss=0.9966075420379639\n",
            "step=12, loss=0.9725483655929565\n",
            "step=13, loss=0.9736224412918091\n",
            "step=14, loss=0.9616207480430603\n",
            "step=15, loss=0.8747859001159668\n",
            "step=16, loss=0.9293810129165649\n",
            "step=17, loss=0.9634508490562439\n",
            "step=18, loss=1.0112330913543701\n",
            "step=19, loss=0.9833810329437256\n",
            "step=20, loss=0.903491199016571\n",
            "step=21, loss=1.0887476205825806\n",
            "step=22, loss=0.9175393581390381\n",
            "step=23, loss=0.9397738575935364\n",
            "step=24, loss=1.0100486278533936\n",
            "step=25, loss=0.9872901439666748\n",
            "step=26, loss=0.9821288585662842\n",
            "step=27, loss=0.9969692826271057\n",
            "step=28, loss=0.9673295617103577\n",
            "step=29, loss=1.0428285598754883\n",
            "step=30, loss=1.033631682395935\n",
            "step=31, loss=1.0037131309509277\n",
            "step=32, loss=0.9157374501228333\n",
            "step=33, loss=0.9951900243759155\n",
            "step=34, loss=0.9472622871398926\n",
            "step=35, loss=1.005519151687622\n",
            "step=36, loss=0.9673923254013062\n",
            "step=37, loss=0.998974621295929\n",
            "step=38, loss=0.9652993083000183\n",
            "step=39, loss=1.0490479469299316\n",
            "step=40, loss=0.9839930534362793\n",
            "step=41, loss=1.0045790672302246\n",
            "step=42, loss=0.922998309135437\n",
            "step=43, loss=1.0205841064453125\n",
            "step=44, loss=0.9009436368942261\n",
            "step=45, loss=0.9502221345901489\n",
            "step=46, loss=0.9949309229850769\n",
            "step=47, loss=0.9580475091934204\n",
            "step=48, loss=0.9065070152282715\n",
            "step=49, loss=1.0493522882461548\n",
            "step=50, loss=1.0068385601043701\n",
            "step=51, loss=0.9726564288139343\n",
            "step=52, loss=0.9937011003494263\n",
            "step=53, loss=0.9358514547348022\n",
            "step=54, loss=1.0016475915908813\n",
            "step=55, loss=0.979462206363678\n",
            "step=56, loss=0.8942812085151672\n",
            "step=57, loss=1.0001493692398071\n",
            "step=58, loss=0.9658193588256836\n",
            "step=59, loss=0.9024196863174438\n",
            "step=60, loss=0.9758437871932983\n",
            "step=61, loss=0.9914939403533936\n",
            "step=62, loss=0.7763037085533142\n",
            "step=63, loss=0.9474449753761292\n",
            "step=64, loss=0.9874740839004517\n",
            "step=65, loss=0.932282567024231\n",
            "step=66, loss=0.8900112509727478\n",
            "step=67, loss=0.9475851655006409\n",
            "step=68, loss=0.9265668392181396\n",
            "step=69, loss=0.8193671107292175\n",
            "step=70, loss=0.8835009932518005\n",
            "step=71, loss=0.9057010412216187\n",
            "step=72, loss=0.9800806045532227\n",
            "step=73, loss=0.9565341472625732\n",
            "step=74, loss=0.9560089111328125\n",
            "step=75, loss=0.9334991574287415\n",
            "step=76, loss=0.9164900779724121\n",
            "step=77, loss=0.9286918044090271\n",
            "step=78, loss=1.0174793004989624\n",
            "step=79, loss=0.9471933841705322\n",
            "step=80, loss=0.9061698317527771\n",
            "step=81, loss=0.8857598900794983\n",
            "step=82, loss=0.9480575323104858\n",
            "step=83, loss=0.9107357263565063\n",
            "step=84, loss=0.8995936512947083\n",
            "step=85, loss=0.932047963142395\n",
            "step=86, loss=0.8455585241317749\n",
            "step=87, loss=0.9013233184814453\n",
            "step=88, loss=0.9499658942222595\n",
            "step=89, loss=0.9464052319526672\n",
            "step=90, loss=0.9392849206924438\n",
            "step=91, loss=0.8906499147415161\n",
            "step=92, loss=0.9014909863471985\n",
            "step=93, loss=1.0016846656799316\n",
            "step=94, loss=0.8805160522460938\n",
            "step=95, loss=0.8950778841972351\n",
            "step=96, loss=0.9512810707092285\n",
            "step=97, loss=0.9372930526733398\n",
            "step=98, loss=0.8922765851020813\n",
            "step=99, loss=0.9325352907180786\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "re_uploadingpqc.theta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWsQunIzIyF0",
        "outputId": "4cfe8d6e-dff4-4db7-afbb-cbacbe307cd8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([[[2.0968297 , 0.30206835, 1.5739384 ],\n",
              "        [0.5251049 , 0.8741471 , 0.1987042 ],\n",
              "        [0.9864484 , 0.33099848, 1.0756977 ],\n",
              "        [0.6223653 , 1.4808354 , 0.58915305]],\n",
              "\n",
              "       [[2.457237  , 3.003884  , 0.1932554 ],\n",
              "        [1.5892024 , 1.8448769 , 3.0597703 ],\n",
              "        [2.842626  , 1.5531948 , 2.5688837 ],\n",
              "        [1.5581336 , 0.359943  , 3.0613048 ]],\n",
              "\n",
              "       [[0.8601422 , 0.07625703, 1.7691804 ],\n",
              "        [1.4836102 , 1.4857876 , 2.8845708 ],\n",
              "        [1.5571223 , 1.6005467 , 1.021268  ],\n",
              "        [2.1542861 , 0.5064244 , 3.7014155 ]],\n",
              "\n",
              "       [[1.2142535 , 1.1960316 , 1.8997414 ],\n",
              "        [2.949342  , 1.8271488 , 0.54395926],\n",
              "        [1.8761944 , 2.7566683 , 2.8122594 ],\n",
              "        [1.0667378 , 1.5274174 , 0.22443488]],\n",
              "\n",
              "       [[2.8497846 , 0.4646812 , 3.2288227 ],\n",
              "        [0.15543303, 2.2204597 , 2.7831235 ],\n",
              "        [2.110044  , 3.1186335 , 2.1053314 ],\n",
              "        [0.45254955, 1.1799093 , 1.566354  ]],\n",
              "\n",
              "       [[2.815995  , 2.980661  , 1.3049458 ],\n",
              "        [0.39323446, 1.8495985 , 0.8633565 ],\n",
              "        [1.4552405 , 1.5356699 , 1.9271975 ],\n",
              "        [1.3471947 , 3.1967936 , 0.43188518]]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(y_true, y_pred):\n",
        "    # Computed with uniform distribution\n",
        "\n",
        "    y_true = y_true > 0.0\n",
        "    y_pred = y_pred >= 0.0\n",
        "    result = y_true == y_pred\n",
        "\n",
        "    return jnp.sum(result)/y_true.shape[0]"
      ],
      "metadata": {
        "id": "52MsqdsR1cBV"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_ys = jax.vmap(re_uploadingpqc)(x_test)\n",
        "# num_correct = jnp.sum((pred_ys > 0.5) == y_test)\n",
        "# final_accuracy = (num_correct / x_test.shape[0]).item()\n",
        "print(f\"final_accuracy={accuracy(y_test, pred_ys)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulHg3Kad0Kf4",
        "outputId": "8fe45ed3-9075-48e4-e3ed-f126acdc6cef"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "final_accuracy=0.5485\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, model, opt_state = make_step(re_uploadingpqc, x_train_batch, y_train_batch, opt_state)"
      ],
      "metadata": {
        "id": "exWyiuc4s6lz"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkOEt4-Hb_z9",
        "outputId": "05eeaecd-05ff-4532-f06f-528f5c646768"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12000, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SyGXc3xxbbR",
        "outputId": "d3812d9c-1565-4443-cc5f-9b42bd628505"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([[[0.84259273, 0.94874143, 0.90688467],\n",
              "        [2.6667356 , 0.76266714, 1.52313363],\n",
              "        [1.06774572, 2.69004876, 1.00810876],\n",
              "        [2.45738273, 2.02423163, 1.71877908]],\n",
              "\n",
              "       [[2.68148044, 0.39126363, 0.20946645],\n",
              "        [0.70720533, 0.3897383 , 1.57548773],\n",
              "        [0.0434015 , 1.72885032, 2.6853669 ],\n",
              "        [3.06993527, 1.55593812, 1.80930546]],\n",
              "\n",
              "       [[0.79809019, 2.51548217, 3.08320155],\n",
              "        [0.66879265, 1.53784092, 2.43956667],\n",
              "        [2.25907099, 2.24584914, 2.13072184],\n",
              "        [2.84810735, 2.66760658, 2.59974501]],\n",
              "\n",
              "       [[0.67692702, 0.98194887, 2.8746463 ],\n",
              "        [1.54491374, 1.59811206, 0.95195518],\n",
              "        [1.65929299, 1.70987385, 1.79819793],\n",
              "        [2.21201573, 0.95468079, 1.83464712]],\n",
              "\n",
              "       [[2.71668027, 1.03779524, 2.34367556],\n",
              "        [2.83988658, 1.66392737, 1.57174783],\n",
              "        [2.08979208, 2.66964127, 0.54374638],\n",
              "        [0.03565717, 0.21567255, 1.88023176]],\n",
              "\n",
              "       [[0.68780493, 0.15546439, 2.48979877],\n",
              "        [1.87962935, 1.77782177, 2.84671802],\n",
              "        [1.14221602, 2.23558458, 0.38788242],\n",
              "        [2.82405449, 2.84739362, 2.15755409]]], dtype=float64)"
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.theta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIg24dNdeUa5",
        "outputId": "11eb4f63-29d3-4f0b-a2bc-031dde1ee052"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([[[0.94259273, 0.84874144, 0.80688467],\n",
              "        [2.76673558, 0.66266714, 1.62313362],\n",
              "        [0.96774575, 2.59004876, 1.10810876],\n",
              "        [2.55738272, 1.92423164, 1.61877908]],\n",
              "\n",
              "       [[2.78148044, 0.49126352, 0.10946645],\n",
              "        [0.80720533, 0.28973831, 1.47548774],\n",
              "        [0.14340149, 1.82885023, 2.5853669 ],\n",
              "        [3.16993527, 1.65593811, 1.70930546]],\n",
              "\n",
              "       [[0.6980902 , 2.41548217, 3.18320155],\n",
              "        [0.56879265, 1.43784092, 2.53956666],\n",
              "        [2.15907101, 2.14584914, 2.23072183],\n",
              "        [2.74810735, 2.76760657, 2.49974502]],\n",
              "\n",
              "       [[0.57692702, 0.88194888, 2.9746463 ],\n",
              "        [1.44491378, 1.49811211, 1.05195513],\n",
              "        [1.559293  , 1.80987383, 1.89819791],\n",
              "        [2.31201572, 1.05468078, 1.93464707]],\n",
              "\n",
              "       [[2.61668031, 1.13779524, 2.44367555],\n",
              "        [2.73988658, 1.76392737, 1.67174783],\n",
              "        [2.18979207, 2.56964128, 0.4437464 ],\n",
              "        [0.13565716, 0.11567256, 1.98023175]],\n",
              "\n",
              "       [[0.58780493, 0.25546439, 2.56467461],\n",
              "        [1.77962936, 1.87782177, 2.76499839],\n",
              "        [1.04221603, 2.33558458, 0.47179337],\n",
              "        [2.72405449, 2.74739363, 2.24749444]]], dtype=float64)"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "opt_state = optim.init(eqx.filter(re_uploadingpqc, eqx.is_inexact_array))"
      ],
      "metadata": {
        "id": "W4E7KnMzX2tC"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "brt1J8opJ6SQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_idx = np.random.randint(0, len(x_train), 32)\n",
        "x_train_batch = np.array([x_train[i] for i in batch_idx])\n",
        "y_train_batch = np.array([y_train[i] for i in batch_idx])\n",
        "y_train_batch = np.array([int(value) for value in y_train_batch])"
      ],
      "metadata": {
        "id": "ksmErc76LAaY"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_batch.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WavQytPhLSHq",
        "outputId": "b510e1a0-fe26-49a8-d876-7c3ec7c06822"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(32, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "actor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBevSvhpMX6j",
        "outputId": "3ff5bcfb-e0c4-4227-ea92-6fd1ff93d7d5"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Actor(n_qubits=4, n_layers=5, beta=1.0, n_actions=2, key=42)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from jaxtyping import Float, Array, Int\n",
        "\n",
        "# class CNN(eqx.Module):\n",
        "#     layers: list\n",
        "\n",
        "#     def __init__(self, key):\n",
        "#         key1, key2, key3, key4 = jax.random.split(key, 4)\n",
        "#         # Standard CNN setup: convolutional layer, followed by flattening,\n",
        "#         # with a small MLP on top.\n",
        "#         self.layers = [\n",
        "#             eqx.nn.Conv2d(1, 3, kernel_size=4, key=key1),\n",
        "#             eqx.nn.MaxPool2d(kernel_size=2),\n",
        "#             jax.nn.relu,\n",
        "#             jnp.ravel,\n",
        "#             eqx.nn.Linear(1728, 512, key=key2),\n",
        "#             jax.nn.sigmoid,\n",
        "#             eqx.nn.Linear(512, 64, key=key3),\n",
        "#             jax.nn.relu,\n",
        "#             eqx.nn.Linear(64, 10, key=key4),\n",
        "#             jax.nn.log_softmax,\n",
        "#         ]\n",
        "\n",
        "#     def __call__(self, x: Float[Array, \"1 28 28\"]) -> Float[Array, \"10\"]:\n",
        "#         for layer in self.layers:\n",
        "#             x = layer(x)\n",
        "#         return x\n",
        "\n",
        "# SEED = 5678\n",
        "\n",
        "# key = jax.random.PRNGKey(SEED)\n",
        "# key, subkey = jax.random.split(key, 2)\n",
        "# model = CNN(subkey)"
      ],
      "metadata": {
        "id": "AYUT7nOvBv6H"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params, static = eqx.partition(re_uploadingpqc, eqx.is_array)"
      ],
      "metadata": {
        "id": "ON-_raXnE0RH"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBsBg_iiFBCT",
        "outputId": "d1b6faf6-3377-4d88-da18-7894461ee38c"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PQCLayer(theta=f64[6,4,3], lmbd=f64[5,4], n_qubits=4, n_layers=5)"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Any, Callable\n",
        "import optax\n",
        "from flax import core, struct\n",
        "from flax.linen.fp8_ops import OVERWRITE_WITH_GRADIENT\n",
        "\n",
        "class TrainState(struct.PyTreeNode):\n",
        "    \"\"\"Train state supporting multiple optimizers.\n",
        "\n",
        "    Example usage::\n",
        "\n",
        "    # Example usage is similar to the previous one with additional optimizers.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    step: int\n",
        "    apply_fn: Callable = struct.field(pytree_node=False)\n",
        "    params: core.FrozenDict[str, Any] = struct.field(pytree_node=True)\n",
        "    tx: optax.GradientTransformation = struct.field(pytree_node=False)\n",
        "    opt_state: optax.OptState = struct.field(pytree_node=True)\n",
        "\n",
        "    # New fields for additional optimizers and optimizer states\n",
        "    tx2: optax.GradientTransformation = struct.field(pytree_node=False)\n",
        "    opt_state2: optax.OptState = struct.field(pytree_node=True)\n",
        "\n",
        "    def apply_gradients(self, *, grads, **kwargs):\n",
        "        \"\"\"Updates ``step``, ``params``, ``opt_state`` and ``**kwargs`` in return value.\"\"\"\n",
        "        if OVERWRITE_WITH_GRADIENT in grads:\n",
        "            grads_with_opt = grads['params']\n",
        "            params_with_opt = self.params['params']\n",
        "        else:\n",
        "            grads_with_opt = grads\n",
        "            params_with_opt = self.params\n",
        "\n",
        "        # Update parameters and optimizer state for the first optimizer\n",
        "        updates, new_opt_state = self.tx.update(\n",
        "            grads_with_opt, self.opt_state, params_with_opt\n",
        "        )\n",
        "        new_params_with_opt = optax.apply_updates(params_with_opt, updates)\n",
        "\n",
        "        # Update parameters and optimizer state for the second optimizer\n",
        "        updates2, new_opt_state2 = self.tx2.update(\n",
        "            grads_with_opt, self.opt_state2, params_with_opt\n",
        "        )\n",
        "        new_params_with_opt2 = optax.apply_updates(params_with_opt, updates2)\n",
        "\n",
        "        # As implied by the OWG name, the gradients are used directly to update the\n",
        "        # parameters.\n",
        "        if OVERWRITE_WITH_GRADIENT in grads:\n",
        "            new_params = {\n",
        "                'params': new_params_with_opt,\n",
        "                'params2': new_params_with_opt2,\n",
        "                OVERWRITE_WITH_GRADIENT: grads[OVERWRITE_WITH_GRADIENT],\n",
        "            }\n",
        "        else:\n",
        "            new_params = new_params_with_opt\n",
        "        return self.replace(\n",
        "            step=self.step + 1,\n",
        "            params=new_params,\n",
        "            opt_state=new_opt_state,\n",
        "            opt_state2=new_opt_state2,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "    @classmethod\n",
        "    def create(cls, *, apply_fn, params, tx, tx2, **kwargs):\n",
        "        \"\"\"Creates a new instance with ``step=0`` and initialized ``opt_state``.\"\"\"\n",
        "        # We exclude OWG params when present because they do not need opt states.\n",
        "        params_with_opt = (\n",
        "            params['params'] if OVERWRITE_WITH_GRADIENT in params else params\n",
        "        )\n",
        "        opt_state = tx.init(params_with_opt)\n",
        "        opt_state2 = tx2.init(params_with_opt)\n",
        "        return cls(\n",
        "            step=0,\n",
        "            apply_fn=apply_fn,\n",
        "            params=params,\n",
        "            tx=tx,\n",
        "            opt_state=opt_state,\n",
        "            tx2=tx2,\n",
        "            opt_state2=opt_state2,\n",
        "            **kwargs,\n",
        "        )\n"
      ],
      "metadata": {
        "id": "rb8mOSLCBUbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import equinox as eqx\n",
        "import jax\n",
        "import jax.random as jr\n",
        "import optax\n",
        "\n",
        "key1, key2 = jr.split(jr.PRNGKey(0))\n",
        "mlp1 = eqx.nn.MLP(2, 2, 2, 2, key=key1)\n",
        "mlp2 = eqx.nn.MLP(2, 2, 2, 2, key=key2)\n",
        "# Example model. In its interaction with `optax.multi_transform`, all that matters\n",
        "# is that it is some PyTree of parameters.\n",
        "model = (mlp1, mlp2)"
      ],
      "metadata": {
        "id": "5XgDcbFneNpj"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_spec = (\"group1\", \"group2\")\n"
      ],
      "metadata": {
        "id": "xF-hQ_VMeN92"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_spec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcK_UALvePgH",
        "outputId": "e813ef8f-bcff-4d51-caf3-6884413e7498"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('group1', 'group2')"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "param_spec = jax.tree_map(lambda _: \"group1\", model)\n",
        "# Use group2 for biases\n",
        "has_bias = lambda x: hasattr(x, \"bias\")\n",
        "where_bias = lambda m: tuple(x.bias for x in jax.tree_leaves(m, is_leaf=has_bias) if has_bias(x))\n",
        "param_spec = eqx.tree_at(where_bias, param_spec, replace_fn=lambda _: \"group2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PofqnlW1eXHQ",
        "outputId": "8d3d09eb-9e76-40e0-b955-f3ff428a2a8b"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-130-c15a47def01e>:4: DeprecationWarning: jax.tree_leaves is deprecated: use jax.tree_util.tree_leaves.\n",
            "  where_bias = lambda m: tuple(x.bias for x in jax.tree_leaves(m, is_leaf=has_bias) if has_bias(x))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "param_spec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2m7bBTFfV3V",
        "outputId": "54d9bca5-3936-4b6c-d943-5ad895aafe2b"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(MLP(\n",
              "   layers=(\n",
              "     Linear(\n",
              "       weight='group1',\n",
              "       bias='group2',\n",
              "       in_features=2,\n",
              "       out_features=2,\n",
              "       use_bias=True\n",
              "     ),\n",
              "     Linear(\n",
              "       weight='group1',\n",
              "       bias='group2',\n",
              "       in_features=2,\n",
              "       out_features=2,\n",
              "       use_bias=True\n",
              "     ),\n",
              "     Linear(\n",
              "       weight='group1',\n",
              "       bias='group2',\n",
              "       in_features=2,\n",
              "       out_features=2,\n",
              "       use_bias=True\n",
              "     )\n",
              "   ),\n",
              "   activation='group1',\n",
              "   final_activation='group1',\n",
              "   use_bias=True,\n",
              "   use_final_bias=True,\n",
              "   in_size=2,\n",
              "   out_size=2,\n",
              "   width_size=2,\n",
              "   depth=2\n",
              " ),\n",
              " MLP(\n",
              "   layers=(\n",
              "     Linear(\n",
              "       weight='group1',\n",
              "       bias='group2',\n",
              "       in_features=2,\n",
              "       out_features=2,\n",
              "       use_bias=True\n",
              "     ),\n",
              "     Linear(\n",
              "       weight='group1',\n",
              "       bias='group2',\n",
              "       in_features=2,\n",
              "       out_features=2,\n",
              "       use_bias=True\n",
              "     ),\n",
              "     Linear(\n",
              "       weight='group1',\n",
              "       bias='group2',\n",
              "       in_features=2,\n",
              "       out_features=2,\n",
              "       use_bias=True\n",
              "     )\n",
              "   ),\n",
              "   activation='group1',\n",
              "   final_activation='group1',\n",
              "   use_bias=True,\n",
              "   use_final_bias=True,\n",
              "   in_size=2,\n",
              "   out_size=2,\n",
              "   width_size=2,\n",
              "   depth=2\n",
              " ))"
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optim = optax.multi_transform(\n",
        "    {\"group1\": optax.adam(1e-1), \"group2\": optax.adam(1e-2)},\n",
        "    param_spec\n",
        ")\n",
        "opt_state = optim.init(eqx.filter(model, eqx.is_inexact_array))"
      ],
      "metadata": {
        "id": "gNfc6oXmfDFO"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# param_spec = eqx.filter(actor, eqx.is_inexact_array)\n",
        "param_spec1 = jax.tree_map(lambda _: \"NT\", re_uploadingpqc)\n",
        "# param_spec = eqx.tree_at(lambda actor: actor.theta, param_spec, replace='group0')\n",
        "# param_spec = eqx.tree_at(lambda actor: actor.lmbd, param_spec, replace='group1')\n",
        "# param_spec = eqx.tree_at(lambda actor: actor.w, param_spec, replace='group2')\n",
        "\n",
        "# optim = optax.multi_transform({\"group0\": optax.adam(1e-1),\n",
        "#     \"group1\": optax.adam(1e-0),\n",
        "#     \"group2\": optax.adam(1e-6),\n",
        "#     },\n",
        "#     param_spec\n",
        "# )\n",
        "param_spec1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFMZ72vWfFpb",
        "outputId": "e078360e-71b7-4505-d816-16060a9c606b"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PQCLayer(theta='NT', lmbd='NT', n_qubits=4, n_layers=5)"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSgEiFByfLtA",
        "outputId": "0db4d9b5-d8cc-4294-c9c5-8570b6f257b3"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GradientTransformationExtraArgs(init=<function multi_transform.<locals>.init_fn at 0x7b2a6f60f910>, update=<function multi_transform.<locals>.update_fn at 0x7b2a6f60f5b0>)"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eqx.tree_pprint(eqx.filter(param_spec, eqx.is_array))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cggoRQDeUJ6w",
        "outputId": "6b1be89a-2ac4-4261-9d54-a1134fd7fc9d"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(\n",
            "  MLP(\n",
            "    layers=(\n",
            "      Linear(\n",
            "        weight=None,\n",
            "        bias=None,\n",
            "        in_features=2,\n",
            "        out_features=2,\n",
            "        use_bias=True\n",
            "      ),\n",
            "      Linear(\n",
            "        weight=None,\n",
            "        bias=None,\n",
            "        in_features=2,\n",
            "        out_features=2,\n",
            "        use_bias=True\n",
            "      ),\n",
            "      Linear(\n",
            "        weight=None,\n",
            "        bias=None,\n",
            "        in_features=2,\n",
            "        out_features=2,\n",
            "        use_bias=True\n",
            "      )\n",
            "    ),\n",
            "    activation=None,\n",
            "    final_activation=None,\n",
            "    use_bias=True,\n",
            "    use_final_bias=True,\n",
            "    in_size=2,\n",
            "    out_size=2,\n",
            "    width_size=2,\n",
            "    depth=2\n",
            "  ),\n",
            "  MLP(\n",
            "    layers=(\n",
            "      Linear(\n",
            "        weight=None,\n",
            "        bias=None,\n",
            "        in_features=2,\n",
            "        out_features=2,\n",
            "        use_bias=True\n",
            "      ),\n",
            "      Linear(\n",
            "        weight=None,\n",
            "        bias=None,\n",
            "        in_features=2,\n",
            "        out_features=2,\n",
            "        use_bias=True\n",
            "      ),\n",
            "      Linear(\n",
            "        weight=None,\n",
            "        bias=None,\n",
            "        in_features=2,\n",
            "        out_features=2,\n",
            "        use_bias=True\n",
            "      )\n",
            "    ),\n",
            "    activation=None,\n",
            "    final_activation=None,\n",
            "    use_bias=True,\n",
            "    use_final_bias=True,\n",
            "    in_size=2,\n",
            "    out_size=2,\n",
            "    width_size=2,\n",
            "    depth=2\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_layers = 5\n",
        "n_qubits = 4\n",
        "key = 42\n",
        "key = jax.random.PRNGKey(key)\n",
        "tkey, lkey = jax.random.split(key, num=2)\n",
        "params = {'thetas': jax.random.uniform(key=tkey, shape=(n_layers + 1, n_qubits, 3),\n",
        "                                    minval=0.0, maxval=np.pi, dtype=DTYPE),\n",
        "          'lmbds': jnp.ones(shape=(n_layers, n_qubits), dtype=DTYPE)}\n",
        "\n",
        "\n",
        "model = PQCLayer(n_qubits=n_qubits,\n",
        "                 n_layers=n_layers,\n",
        "                 params=params)\n",
        "\n",
        "\n",
        "def map_nested_fn(fn):\n",
        "  '''Recursively apply `fn` to the key-value pairs of a nested dict'''\n",
        "  def map_fn(nested_dict):\n",
        "    return {k: (map_fn(v) if isinstance(v, dict) else fn(k, v))\n",
        "            for k, v in nested_dict.items()}\n",
        "  return map_fn\n",
        "\n",
        "# gradients = jax.tree_util.tree_map(jnp.ones_like, params)  # dummy gradients\n",
        "\n",
        "label_fn = map_nested_fn(lambda k, _: k)\n",
        "optim = optax.multi_transform({'thetas': optax.adam(0.01), 'lmbds': optax.adam(0.001)},\n",
        "                           label_fn)\n",
        "\n",
        "# optim = closure_to_pytree(optim)\n",
        "opt_state = optim.init(params)\n",
        "\n",
        "@eqx.filter_value_and_grad\n",
        "def compute_loss(model, x, y):\n",
        "    pred_y = jax.vmap(model)(x)\n",
        "    loss = jnp.maximum(0, 1 - (2.0 * y - 1.0) * pred_y)\n",
        "    return jnp.mean(loss)\n",
        "\n",
        "def apply_updates_to_model(model, new_params):\n",
        "  # this function is specific and works for this example only. one can think of\n",
        "  # generalizing it to work for updating any given attribute/params of the model.\n",
        "\n",
        "  model_new = eqx.tree_at(where=lambda model: model.theta, pytree=model, replace=new_params['thetas'])\n",
        "  model_new = eqx.tree_at(where=lambda model: model.lmbd, pytree=model_new, replace=new_params['lmbds'])\n",
        "\n",
        "  return model_new\n",
        "\n",
        "\n",
        "@eqx.filter_jit\n",
        "def make_step(model, x, y, opt_state, params):\n",
        "    loss, grads = compute_loss(model, x, y)\n",
        "    grads = {'thetas': grads.theta, 'lmbds': grads.lmbd}\n",
        "    updates, opt_state = optim.update(grads, opt_state, params)\n",
        "    new_params = optax.apply_updates(params, updates)\n",
        "    model_new = apply_updates_to_model(model, new_params)\n",
        "    # model_new = eqx.tree_at(where=lambda model: model.theta, pytree=model, replace=new_params['thetas'])\n",
        "    # model_new = eqx.tree_at(where=lambda model: model.lmbd, pytree=model_new, replace=new_params['lmbds'])\n",
        "    # model = eqx.apply_updates(model, updates)\n",
        "    return loss, model_new, opt_state\n",
        "\n",
        "for step in range(100):\n",
        "  batch_idx = np.random.randint(0, len(x_train), 32)\n",
        "  x_train_batch = np.array([x_train[i] for i in batch_idx])\n",
        "  y_train_batch = np.array([y_train[i] for i in batch_idx])\n",
        "  y_train_batch = np.array([int(value) for value in y_train_batch])\n",
        "\n",
        "  loss, model, opt_state = make_step(model, x_train_batch, y_train_batch, opt_state, params)\n",
        "  loss = loss.item()\n",
        "  print(f\"step={step}, loss={loss}\")\n",
        "\n",
        "# opt_init, opt_update = optax.adam(0.01)\n",
        "# opt_state = opt_init(eqx.filter(model, eqx.is_array))\n",
        "# flat_model, treedef_model = jtu.tree_flatten(model)\n",
        "# flat_opt_state, treedef_opt_state = jtu.tree_flatten(opt_state)"
      ],
      "metadata": {
        "id": "2njeTYIrUe_q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}