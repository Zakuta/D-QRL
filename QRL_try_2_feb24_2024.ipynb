{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPBE21VgNWn9dyaI5JTjiy+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zakuta/D-QRL/blob/main/QRL_try_2_feb24_2024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "f1fTvAQcbQBT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fbdbc58-af64-4a82-f851-53c63554f205"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flax in /usr/local/lib/python3.10/dist-packages (0.8.1)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/dist-packages (from flax) (1.25.2)\n",
            "Requirement already satisfied: jax>=0.4.19 in /usr/local/lib/python3.10/dist-packages (from flax) (0.4.23)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from flax) (1.0.7)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.10/dist-packages (from flax) (0.1.9)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.10/dist-packages (from flax) (0.4.4)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.10/dist-packages (from flax) (0.1.45)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.10/dist-packages (from flax) (13.7.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.10/dist-packages (from flax) (4.9.0)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from flax) (6.0.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.19->flax) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.19->flax) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.19->flax) (1.11.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax) (2.16.1)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from optax->flax) (1.4.0)\n",
            "Requirement already satisfied: chex>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from optax->flax) (0.1.85)\n",
            "Requirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.10/dist-packages (from optax->flax) (0.4.23+cuda12.cudnn89)\n",
            "Requirement already satisfied: etils[epath,epy] in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax) (1.7.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax) (1.6.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax) (3.20.3)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.7->optax->flax) (0.12.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax) (0.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax) (2023.6.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax) (6.1.1)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax) (3.17.0)\n"
          ]
        }
      ],
      "source": [
        "# !pip install equinox\n",
        "# !pip install tensorcircuit\n",
        "# !pip install -U qiskit\n",
        "# !pip install tensorcircuit\n",
        "# !pip install cirq\n",
        "# !pip install openfermion\n",
        "# !pip install gymnax\n",
        "# !pip install brax\n",
        "# !pip install distrax\n",
        "# !pip install flax"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "from jax import config\n",
        "\n",
        "config.update(\"jax_debug_nans\", True)\n",
        "config.update(\"jax_enable_x64\", True)\n",
        "\n",
        "import jax.numpy as jnp\n",
        "DTYPE=jnp.float64\n",
        "\n",
        "import chex\n",
        "import numpy as np\n",
        "import optax\n",
        "from flax import struct\n",
        "from functools import partial\n",
        "import tensorcircuit as tc\n",
        "\n",
        "import tensorflow as tf\n",
        "from sklearn.decomposition import PCA\n",
        "import equinox as eqx\n",
        "import types\n",
        "from jaxtyping import Array, PRNGKeyArray\n",
        "from typing import Union, Sequence, List, NamedTuple, Optional, Tuple, Any, Literal, TypeVar\n",
        "import jax.tree_util as jtu\n",
        "import gymnax\n",
        "import distrax\n",
        "from gymnax.environments import environment, spaces\n",
        "from brax import envs\n",
        "from brax.envs.wrappers.training import EpisodeWrapper, AutoResetWrapper\n",
        "\n",
        "K = tc.set_backend(\"jax\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPpDluwWbm-a",
        "outputId": "04990114-badd-46b5-82f1-f3f1a6a04bb0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorcircuit.translation:Please first ``pip install -U qiskit`` to enable related functionality in translation module\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# shamelessly taken from purejaxrl: https://github.com/luchris429/purejaxrl/blob/main/purejaxrl/wrappers.py\n",
        "\n",
        "class GymnaxWrapper(object):\n",
        "    \"\"\"Base class for Gymnax wrappers.\"\"\"\n",
        "\n",
        "    def __init__(self, env):\n",
        "        self._env = env\n",
        "\n",
        "    # provide proxy access to regular attributes of wrapped object\n",
        "    def __getattr__(self, name):\n",
        "        return getattr(self._env, name)\n",
        "\n",
        "\n",
        "class FlattenObservationWrapper(GymnaxWrapper):\n",
        "    \"\"\"Flatten the observations of the environment.\"\"\"\n",
        "\n",
        "    def __init__(self, env: environment.Environment):\n",
        "        super().__init__(env)\n",
        "\n",
        "    def observation_space(self, params) -> spaces.Box:\n",
        "        assert isinstance(\n",
        "            self._env.observation_space(params), spaces.Box\n",
        "        ), \"Only Box spaces are supported for now.\"\n",
        "        return spaces.Box(\n",
        "            low=self._env.observation_space(params).low,\n",
        "            high=self._env.observation_space(params).high,\n",
        "            shape=(np.prod(self._env.observation_space(params).shape),),\n",
        "            dtype=self._env.observation_space(params).dtype,\n",
        "        )\n",
        "\n",
        "    @partial(jax.jit, static_argnums=(0,))\n",
        "    def reset(\n",
        "        self, key: chex.PRNGKey, params: Optional[environment.EnvParams] = None\n",
        "    ) -> Tuple[chex.Array, environment.EnvState]:\n",
        "        obs, state = self._env.reset(key, params)\n",
        "        obs = jnp.reshape(obs, (-1,))\n",
        "        return obs, state\n",
        "\n",
        "    @partial(jax.jit, static_argnums=(0,))\n",
        "    def step(\n",
        "        self,\n",
        "        key: chex.PRNGKey,\n",
        "        state: environment.EnvState,\n",
        "        action: Union[int, float],\n",
        "        params: Optional[environment.EnvParams] = None,\n",
        "    ) -> Tuple[chex.Array, environment.EnvState, float, bool, dict]:\n",
        "        obs, state, reward, done, info = self._env.step(key, state, action, params)\n",
        "        obs = jnp.reshape(obs, (-1,))\n",
        "        return obs, state, reward, done, info\n",
        "\n",
        "\n",
        "@struct.dataclass\n",
        "class LogEnvState:\n",
        "    env_state: environment.EnvState\n",
        "    episode_returns: float\n",
        "    episode_lengths: int\n",
        "    returned_episode_returns: float\n",
        "    returned_episode_lengths: int\n",
        "    timestep: int\n",
        "\n",
        "\n",
        "class LogWrapper(GymnaxWrapper):\n",
        "    \"\"\"Log the episode returns and lengths.\"\"\"\n",
        "\n",
        "    def __init__(self, env: environment.Environment):\n",
        "        super().__init__(env)\n",
        "\n",
        "    @partial(jax.jit, static_argnums=(0,))\n",
        "    def reset(\n",
        "        self, key: chex.PRNGKey, params: Optional[environment.EnvParams] = None\n",
        "    ) -> Tuple[chex.Array, environment.EnvState]:\n",
        "        obs, env_state = self._env.reset(key, params)\n",
        "        state = LogEnvState(env_state, 0, 0, 0, 0, 0)\n",
        "        return obs, state\n",
        "\n",
        "    @partial(jax.jit, static_argnums=(0,))\n",
        "    def step(\n",
        "        self,\n",
        "        key: chex.PRNGKey,\n",
        "        state: environment.EnvState,\n",
        "        action: Union[int, float],\n",
        "        params: Optional[environment.EnvParams] = None,\n",
        "    ) -> Tuple[chex.Array, environment.EnvState, float, bool, dict]:\n",
        "        obs, env_state, reward, done, info = self._env.step(\n",
        "            key, state.env_state, action, params\n",
        "        )\n",
        "        new_episode_return = state.episode_returns + reward\n",
        "        new_episode_length = state.episode_lengths + 1\n",
        "        state = LogEnvState(\n",
        "            env_state=env_state,\n",
        "            episode_returns=new_episode_return * (1 - done),\n",
        "            episode_lengths=new_episode_length * (1 - done),\n",
        "            returned_episode_returns=state.returned_episode_returns * (1 - done)\n",
        "            + new_episode_return * done,\n",
        "            returned_episode_lengths=state.returned_episode_lengths * (1 - done)\n",
        "            + new_episode_length * done,\n",
        "            timestep=state.timestep + 1,\n",
        "        )\n",
        "        info[\"returned_episode_returns\"] = state.returned_episode_returns\n",
        "        info[\"returned_episode_lengths\"] = state.returned_episode_lengths\n",
        "        info[\"timestep\"] = state.timestep\n",
        "        info[\"returned_episode\"] = done\n",
        "        return obs, state, reward, done, info"
      ],
      "metadata": {
        "id": "eEXoWKLccJjF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reuploading_circuit(n_qubits, n_layers, rot_params, input_params, X):\n",
        "  circuit = tc.Circuit(n_qubits)\n",
        "\n",
        "  for l in range(n_layers):\n",
        "    # variational part\n",
        "    for qubit_idx in range(n_qubits):\n",
        "      circuit.rx(qubit_idx, theta=rot_params[l, qubit_idx, 0])\n",
        "      circuit.ry(qubit_idx, theta=rot_params[l, qubit_idx, 1])\n",
        "      circuit.rz(qubit_idx, theta=rot_params[l, qubit_idx, 2])\n",
        "\n",
        "    # entangling part\n",
        "    for qubit_idx in range(n_qubits - 1):\n",
        "      circuit.cnot(qubit_idx, qubit_idx + 1)\n",
        "    if n_qubits != 2:\n",
        "      circuit.cnot(n_qubits - 1, 0)\n",
        "\n",
        "    # encoding part\n",
        "    for qubit_idx in range(n_qubits):\n",
        "      input = X[qubit_idx] * input_params[l, qubit_idx]\n",
        "      # print(input)\n",
        "      circuit.rx(qubit_idx, theta=input)\n",
        "\n",
        "  # last variational part\n",
        "  for qubit_idx in range(n_qubits):\n",
        "    circuit.rx(qubit_idx, theta=rot_params[n_layers, qubit_idx, 0])\n",
        "    circuit.ry(qubit_idx, theta=rot_params[n_layers, qubit_idx, 1])\n",
        "    circuit.rz(qubit_idx, theta=rot_params[n_layers, qubit_idx, 2])\n",
        "\n",
        "  return circuit\n",
        "\n",
        "\n",
        "# class PQCLayer(eqx.Module):\n",
        "#   theta: Array\n",
        "#   lmbd: Array\n",
        "#   n_qubits: int = eqx.field(static=True)\n",
        "#   n_layers: int = eqx.field(static=True)\n",
        "\n",
        "#   def __init__(self, n_qubits: int, n_layers: int, params: Optional, key: PRNGKeyArray):\n",
        "\n",
        "#     key = jax.random.PRNGKey(key)\n",
        "#     tkey, lkey = jax.random.split(key, num=2)\n",
        "\n",
        "#     if params is None:\n",
        "#       self.theta = params['thetas']\n",
        "#       self.lmbd = params['lmbds']\n",
        "#     else:\n",
        "#       self.theta = jax.random.uniform(key=tkey, shape=(n_layers + 1, n_qubits, 3),\n",
        "#                                     minval=0.0, maxval=np.pi, dtype=DTYPE)\n",
        "#       self.lmbd = jnp.ones(shape=(n_layers, n_qubits), dtype=DTYPE)\n",
        "\n",
        "#     self.n_qubits = n_qubits\n",
        "#     self.n_layers = n_layers\n",
        "\n",
        "#   def __call__(self, inputs):\n",
        "#     circuit = tc.Circuit(self.n_qubits)\n",
        "\n",
        "#     for l in range(self.n_layers):\n",
        "#       # variational part\n",
        "#       for qubit_idx in range(self.n_qubits):\n",
        "#         circuit.rx(qubit_idx, theta=self.theta[l, qubit_idx, 0])\n",
        "#         circuit.ry(qubit_idx, theta=self.theta[l, qubit_idx, 1])\n",
        "#         circuit.rz(qubit_idx, theta=self.theta[l, qubit_idx, 2])\n",
        "\n",
        "#       # entangling part\n",
        "#       for qubit_idx in range(self.n_qubits - 1):\n",
        "#         circuit.cnot(qubit_idx, qubit_idx + 1)\n",
        "#       if self.n_qubits != 2:\n",
        "#         circuit.cnot(self.n_qubits - 1, 0)\n",
        "\n",
        "#       # encoding part\n",
        "#       for qubit_idx in range(self.n_qubits):\n",
        "#         linear_input = inputs[qubit_idx] * self.lmbd[l, qubit_idx]\n",
        "#         circuit.rx(qubit_idx, theta=linear_input)\n",
        "\n",
        "#     # last variational part\n",
        "#     for qubit_idx in range(self.n_qubits):\n",
        "#       circuit.rx(qubit_idx, theta=self.theta[self.n_layers, qubit_idx, 0])\n",
        "#       circuit.ry(qubit_idx, theta=self.theta[self.n_layers, qubit_idx, 1])\n",
        "#       circuit.rz(qubit_idx, theta=self.theta[self.n_layers, qubit_idx, 2])\n",
        "\n",
        "#     return jnp.real(circuit.expectation_ps(z=jnp.arange(len(self.n_qubits))))\n",
        "\n",
        "\n",
        "# class PQCLayer(eqx.Module):\n",
        "#   theta: jax.Array = eqx.field(converter=jnp.asarray)\n",
        "#   lmbd: jax.Array = eqx.field(converter=jnp.asarray)\n",
        "#   n_qubits: int = eqx.field(static=True)\n",
        "#   n_layers: int = eqx.field(static=True)\n",
        "\n",
        "#   def __init__(self, n_qubits: int, n_layers: int, key: int):\n",
        "#     key = jax.random.PRNGKey(key)\n",
        "#     tkey, lkey = jax.random.split(key, num=2)\n",
        "#     self.n_qubits = n_qubits\n",
        "#     self.n_layers = n_layers\n",
        "#     # rotation_params\n",
        "#     self.theta = jax.random.uniform(key=tkey, shape=(n_layers + 1, n_qubits, 3),\n",
        "#                                     minval=0.0, maxval=np.pi, dtype=DTYPE)\n",
        "#     # input encoding params\n",
        "#     # self.lmbd = jnp.ones(shape=(n_layers, n_qubits))\n",
        "#     self.lmbd = jax.random.uniform(key=lkey, shape=(n_layers, n_qubits),\n",
        "#                                     minval=0.0, maxval=np.pi, dtype=DTYPE)\n",
        "\n",
        "#   def __call__(self, inputs):\n",
        "#   # def __call__(self, X, n_qubits, depth):\n",
        "\n",
        "#     circuit = generate_circuit(self.n_qubits, self.n_layers, self.theta, self.lmbd, inputs)\n",
        "#     # state = circuit.state()\n",
        "#     # return state\n",
        "#     return K.real(circuit.expectation_ps(z=[0,1,2,3]))\n",
        "\n",
        "# class Alternating(eqx.Module):\n",
        "#   w: jax.Array = eqx.field(converter=jnp.asarray)\n",
        "\n",
        "#   def __init__(self, output_dim):\n",
        "#     self.w = jnp.array([[(-1.) ** i for i in range(output_dim)]])\n",
        "\n",
        "#   def __call__(self, inputs):\n",
        "#     return jnp.matmul(inputs, self.w)\n",
        "\n",
        "\n",
        "# class Actor(eqx.Module):\n",
        "#   n_qubits: int\n",
        "#   n_layers: int\n",
        "#   beta: float\n",
        "#   n_actions: Sequence[int]\n",
        "#   key: int\n",
        "\n",
        "#   def __call__(self, x):\n",
        "#     re_uploading_pqc = PQCLayer(n_qubits=self.n_qubits,\n",
        "#                                 n_layers=self.n_layers,\n",
        "#                                 key=self.key)(x)\n",
        "\n",
        "#     process = eqx.nn.Sequential([\n",
        "#         Alternating(self.n_actions),\n",
        "#         eqx.nn.Lambda(lambda x: x * self.beta),\n",
        "#         jax.nn.softmax()\n",
        "#     ])\n",
        "\n",
        "#     policy = process(re_uploading_pqc)\n",
        "\n",
        "#     return policy\n",
        "\n",
        "\n",
        "\n",
        "class QuantumActor(eqx.Module):\n",
        "  theta: jax.Array # trainable\n",
        "  lmbd: jax.Array # trainable\n",
        "  w: jax.Array # trainable\n",
        "  n_qubits: int = eqx.field(static=True)\n",
        "  n_layers: int = eqx.field(static=True)\n",
        "  beta: float = eqx.field(static=True)\n",
        "  n_actions: Sequence[int] = eqx.field(static=True)\n",
        "  # key: int\n",
        "\n",
        "  def __init__(self, n_qubits, n_layers, beta, n_actions, params: Optional, key = 42):\n",
        "\n",
        "    key = jax.random.PRNGKey(key)\n",
        "    key, _key = jax.random.split(key, num=2)\n",
        "\n",
        "    if params is None:\n",
        "      # rotation_params\n",
        "      self.theta = params['thetas']\n",
        "      # input encoding params\n",
        "      self.lmbd = params['lmbds']\n",
        "      # observable weights\n",
        "      self.w = params['ws']\n",
        "    else:\n",
        "      self.theta = jax.random.uniform(key=key, shape=(n_layers + 1, n_qubits, 3),\n",
        "                                    minval=0.0, maxval=np.pi, dtype=DTYPE)\n",
        "      self.lmbd = jnp.ones(shape=(n_layers, n_qubits), dtype=DTYPE)\n",
        "      self.w = jnp.array([[(-1.) ** i for i in range(n_actions)]])\n",
        "\n",
        "\n",
        "    self.n_qubits = n_qubits\n",
        "    self.n_layers = n_layers\n",
        "    self.beta = beta\n",
        "    self.n_actions = n_actions\n",
        "\n",
        "  def quantum_policy_circuit(self, inputs):\n",
        "\n",
        "    # this can be any PQC of the user's choice. hence, I made the decision to make a separate function within this class\n",
        "    circuit = reuploading_circuit(self.n_qubits, self.n_layers, self.theta, self.lmbd, inputs)\n",
        "\n",
        "    return K.real(circuit.expectation_ps(z=np.arange(self.n_qubits)))\n",
        "\n",
        "  def alternating(self, inputs):\n",
        "    return jnp.matmul(inputs, self.w)\n",
        "\n",
        "  def get_params(self):\n",
        "    return {'thetas': self.theta, 'lmbds': self.lmbd, 'ws': self.w}\n",
        "\n",
        "  def __call__(self, x):\n",
        "\n",
        "    pqc = self.quantum_policy_circuit(x)\n",
        "    # print(pqc)\n",
        "    alt = self.alternating(jnp.array([pqc], dtype=DTYPE))\n",
        "\n",
        "    # process = eqx.nn.Sequential([\n",
        "    #     alt,\n",
        "    #     eqx.nn.Lambda(lambda x: x * self.beta),\n",
        "    #     jax.nn.softmax()\n",
        "    # ])\n",
        "    # process = eqx.nn.Sequential([\n",
        "    #     self.alternating,\n",
        "    #     eqx.nn.Lambda(lambda x: x * self.beta),\n",
        "    #     distrax.Softmax\n",
        "    # ])\n",
        "\n",
        "\n",
        "    # policy = process(jnp.array([pqc], dtype=DTYPE))\n",
        "\n",
        "    actor_mean = eqx.nn.Lambda(lambda x: x * self.beta)(alt)\n",
        "    # policy = jax.nn.softmax(actor_mean)\n",
        "    policy = distrax.Softmax(actor_mean)\n",
        "\n",
        "    return policy\n",
        "\n",
        "\n",
        "# class Actor(eqx.Module):\n",
        "#   n_qubits: int\n",
        "#   n_layers: int\n",
        "#   beta: float\n",
        "#   n_actions: Sequence[int]\n",
        "#   pqc: eqx.Module\n",
        "#   alt: eqx.Module\n",
        "#   key: int\n",
        "\n",
        "#   def __init__(self, n_qubits, n_layers, beta, n_actions, key):\n",
        "#     self.n_qubits = n_qubits\n",
        "#     self.n_layers = n_layers\n",
        "#     self.beta = beta\n",
        "#     self.n_actions = n_actions\n",
        "#     self.key = key\n",
        "\n",
        "#     self.pqc = PQCLayer(n_qubits=self.n_qubits,\n",
        "#                         n_layers=self.n_layers,\n",
        "#                         key=self.key)\n",
        "\n",
        "#     self.alt = Alternating(self.n_actions)\n",
        "\n",
        "#   def __call__(self, x):\n",
        "#     re_uploading_pqc = self.pqc(x)\n",
        "\n",
        "#     process = eqx.nn.Sequential([\n",
        "#         self.alt,\n",
        "#         eqx.nn.Lambda(lambda x: x * self.beta),\n",
        "#         jax.nn.softmax()\n",
        "#     ])\n",
        "\n",
        "#     policy = process(re_uploading_pqc)\n",
        "\n",
        "#     return policy\n",
        "\n",
        "\n",
        "class TrainState(eqx.Module):\n",
        "  model: eqx.Module\n",
        "  optimizer: optax.GradientTransformation = eqx.field(static=True)\n",
        "  opt_state: optax.OptState\n",
        "\n",
        "  def __init__(self, model, optimizer, opt_state):\n",
        "    self.model = model\n",
        "    self.optimizer = optimizer\n",
        "    self.opt_state = opt_state\n",
        "\n",
        "  def apply_updates_to_model(self, new_params):\n",
        "    # this function is specific and works for this example only. one can think of\n",
        "    # generalizing it to work for updating any given attribute/params of the model.\n",
        "\n",
        "    model_new = eqx.tree_at(where=lambda model: model.theta, pytree=self.model, replace=new_params['thetas'])\n",
        "    model_new = eqx.tree_at(where=lambda model: model.lmbd, pytree=model_new, replace=new_params['lmbds'])\n",
        "    model_new = eqx.tree_at(where=lambda model: model.w, pytree=model_new, replace=new_params['ws'])\n",
        "\n",
        "    return model_new\n",
        "\n",
        "  def apply_gradients(self, params, grads):\n",
        "    grads = {'thetas': grads.theta, 'lmbds': grads.lmbd, 'ws': grads.w}\n",
        "    # if type(grads) == dict: # this is if we actually apply value_and_grad functionality of JAX, might be useful if we apply PPO for instance\n",
        "    #   grads = {'thetas': grads.theta, 'lmbds': grads.lmbd, 'ws': grads.w}\n",
        "    # else: # for monte-carlo estimation of grads which REINFORCE applies\n",
        "    #   grads = grads\n",
        "    updates, opt_state = self.optimizer.update(grads, self.opt_state, params)\n",
        "    new_params = optax.apply_updates(params, updates)\n",
        "    model_new = self.apply_updates_to_model(new_params)\n",
        "    # model = eqx.apply_updates(self.model, updates)\n",
        "    new_train_state = self.__class__(model=model_new, optimizer=self.optimizer, opt_state=opt_state)\n",
        "    return new_train_state\n",
        "\n",
        "class Transition(NamedTuple):\n",
        "  done: jnp.ndarray\n",
        "  action: jnp.ndarray\n",
        "  reward: jnp.ndarray\n",
        "  log_prob: jnp.ndarray\n",
        "  obs: jnp.ndarray\n",
        "  info: jnp.ndarray"
      ],
      "metadata": {
        "id": "1ZofBx0yccWb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lw0YYniPceHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@eqx.filter_jit\n",
        "def train_cp(conf):\n",
        "\n",
        "  conf['n_updates'] = (\n",
        "          conf['total_timesteps'] // conf['n_steps'] // conf['n_envs']\n",
        "      )\n",
        "\n",
        "  conf['mini_batchsize'] = (\n",
        "          conf['n_envs'] * conf['n_steps'] // conf['n_minibatches']\n",
        "      )\n",
        "\n",
        "  env, env_params = gymnax.make(conf['env_name'])\n",
        "  env = FlattenObservationWrapper(env)\n",
        "  env = LogWrapper(env)\n",
        "\n",
        "  n_actions = env.action_space(env_params).n\n",
        "\n",
        "  rng = jax.random.PRNGKey(conf['rng'])\n",
        "  rng, _rng = jax.random.split(rng)\n",
        "  params = {'thetas': jax.random.uniform(\n",
        "      key=_rng, shape=(conf['n_layers'] + 1, conf['n_qubits'], 3),\n",
        "      minval=0.0, maxval=np.pi, dtype=DTYPE\n",
        "      ),\n",
        "            'lmbds': jnp.ones(shape=(conf['n_layers'],\n",
        "                                    conf['n_qubits']), dtype=DTYPE\n",
        "                              ),\n",
        "            'ws': jnp.array([[(-1.) ** i for i in range(n_actions)]], dtype=DTYPE)\n",
        "            }\n",
        "\n",
        "  def train(rng):\n",
        "    actor = QuantumActor(\n",
        "        n_qubits=conf['n_qubits'], n_layers=conf['n_layers'],\n",
        "        beta=conf['beta'], n_actions=n_actions, params=params\n",
        "        )\n",
        "\n",
        "    state_bounds = jnp.array([2.4, 2.5, 0.21, 2.5], dtype=DTYPE)\n",
        "\n",
        "    def map_nested_fn(fn):\n",
        "      '''Recursively apply `fn` to the key-value pairs of a nested dict'''\n",
        "      def map_fn(nested_dict):\n",
        "        return {k: (map_fn(v) if isinstance(v, dict) else fn(k, v))\n",
        "                for k, v in nested_dict.items()}\n",
        "      return map_fn\n",
        "\n",
        "    label_fn = map_nested_fn(lambda k, _: k)\n",
        "    optim = optax.multi_transform({'thetas': optax.amsgrad(conf['lr_theta']),\n",
        "                                  'lmbds': optax.amsgrad(conf['lr_lmbd']),\n",
        "                                  'ws': optax.amsgrad(conf['lr_w'])},\n",
        "                              label_fn)\n",
        "\n",
        "    opt_state = optim.init(params)\n",
        "\n",
        "    train_state = TrainState(model=actor, optimizer=optim, opt_state=opt_state)\n",
        "\n",
        "    rng, rng_reset = jax.random.split(rng)\n",
        "    #vmappable\n",
        "    obs, env_state = env.reset(rng_reset, env_params)\n",
        "\n",
        "    def update_episode(runner_state, ununsed):\n",
        "      def env_step(runner_state, ununsed):\n",
        "\n",
        "        last_obs, env_state, train_state, rng = runner_state\n",
        "        rng, rng_step, rng_net = jax.random.split(rng, 3)\n",
        "\n",
        "        actor = train_state.model\n",
        "        policy = actor(last_obs.reshape(-1))\n",
        "        # policy = actor(last_obs.reshape(-1) / state_bounds)\n",
        "        action = policy.sample(seed=rng_net)\n",
        "        log_prob = policy.log_prob(action)\n",
        "\n",
        "        #vmappable\n",
        "        obs, env_state, reward, done, info = env.step(rng_step, env_state, action, env_params)\n",
        "\n",
        "        transition = Transition(\n",
        "            done, action, reward, log_prob, obs, info)\n",
        "\n",
        "        runner_state = (obs, env_state, train_state, rng)\n",
        "\n",
        "        return runner_state, transition\n",
        "\n",
        "      runner_state, traj_batch = jax.lax.scan(env_step, runner_state, None, conf['n_steps'])\n",
        "\n",
        "      return runner_state, traj_batch\n",
        "\n",
        "      obs, env_state, train_state, rng = runner_state\n",
        "\n",
        "      def calculate_returns(traj_batch):\n",
        "          def _compute_discounted_sum(carry, transition):\n",
        "              rewards_to_go = carry\n",
        "              reward = transition.reward\n",
        "              rewards_to_go = reward + conf['gamma'] * rewards_to_go\n",
        "              baseline = 0\n",
        "              return rewards_to_go, rewards_to_go\n",
        "\n",
        "          init_carry = jnp.zeros_like(0, dtype=DTYPE)\n",
        "\n",
        "          _, returns = jax.lax.scan(\n",
        "              _compute_discounted_sum,\n",
        "              init_carry,\n",
        "              traj_batch,\n",
        "              reverse=True,\n",
        "          )\n",
        "          return returns\n",
        "\n",
        "      returns = calculate_returns(traj_batch)\n",
        "\n",
        "      def update_epoch(update_state, ununsed):\n",
        "        @eqx.filter_value_and_grad\n",
        "        def reinforce_update(actor, batch_info):\n",
        "          traj_batch, returns = batch_info\n",
        "          policy =\n",
        "\n",
        "\n",
        "      # UPDATE ACTOR\n",
        "      def _update_epoch(update_state, unused):\n",
        "        def _update_minbatch(train_state, batch_info):\n",
        "          traj_batch, discounted_rewards = batch_info\n",
        "\n",
        "          @eqx.filter_value_and_grad\n",
        "          def _loss_fn(actor, traj_batch):\n",
        "            #TODO: can I use vmap here?\n",
        "            # RERUN ACTOR\n",
        "            print(traj_batch.obs.shape)\n",
        "            policy = actor(traj_batch.obs / state_bounds)\n",
        "            log_prob = policy.log_prob(traj_batch.action)\n",
        "            # for stability while training and less variability\n",
        "            returns = (discounted_rewards - jnp.mean(discounted_rewards)) / (jnp.std(discounted_rewards) + 1e-8)\n",
        "            loss = -jnp.mean(log_prob * returns)\n",
        "\n",
        "            return loss\n",
        "\n",
        "          loss, grads = _loss_fn(train_state.model, traj_batch)\n",
        "          print(grads)\n",
        "          train_state = train_state.apply_gradients(train_state.model.get_params(), grads)\n",
        "\n",
        "          return train_state, loss\n",
        "\n",
        "        train_state, traj_batch, discounted_rewards, rng = update_state\n",
        "        rng, _rng = jax.random.split(rng)\n",
        "\n",
        "        # Mini-batch Updates\n",
        "        batch_size = conf['mini_batchsize'] * conf['n_minibatches']\n",
        "        assert (batch_size == conf['n_steps'] * conf['n_envs']\n",
        "        ), 'batch size must be equal to number of steps * number of envs'\n",
        "        permutation = jax.random.permutation(_rng, batch_size)\n",
        "        batch = (traj_batch, discounted_rewards)\n",
        "        # print(batch)\n",
        "\n",
        "        #TODO: @Yash in future. We only need this if we are using VMAP. for now commenting it out!!\n",
        "        # batch = jax.tree_util.tree_map(\n",
        "        #                 lambda x: x.reshape((batch_size,) + x.shape[2:]), batch)\n",
        "        shuffled_batch = jax.tree_util.tree_map(\n",
        "                        lambda x: jnp.take(x, permutation, axis=0), batch\n",
        "                    )\n",
        "\n",
        "        minibatches = jax.tree_util.tree_map(\n",
        "            lambda x: jnp.reshape(\n",
        "                x, [conf['n_minibatches'], -1] + list(x.shape[1:])\n",
        "            ),\n",
        "            shuffled_batch,\n",
        "        )\n",
        "        # print(train_state)\n",
        "        # print(minibatches)\n",
        "\n",
        "        train_state, loss = jax.lax.scan(\n",
        "            _update_minbatch, train_state, minibatches\n",
        "        )\n",
        "        update_state = (train_state, traj_batch, discounted_rewards, rng)\n",
        "        return update_state, loss\n",
        "\n",
        "      # Updating training state and metrics\n",
        "      update_state = (train_state, traj_batch, discounted_rewards, rng)\n",
        "      update_state, loss_info = jax.lax.scan(\n",
        "          _update_epoch, update_state, None, conf['update_epochs']\n",
        "          )\n",
        "      train_state = update_state[0]\n",
        "      metric = traj_batch.info\n",
        "      rng = update_state[-1]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# def rollout(rng_input, policy_params, env_params, steps_in_episode):\n",
        "#     \"\"\"Rollout a jitted gymnax episode with lax.scan.\"\"\"\n",
        "#     # Reset the environment\n",
        "#     rng_reset, rng_episode = jax.random.split(rng_input)\n",
        "#     obs, state = env.reset(rng_reset, env_params)\n",
        "\n",
        "#     def policy_step(state_input, tmp):\n",
        "#         \"\"\"lax.scan compatible step transition in jax env.\"\"\"\n",
        "#         obs, state, policy_params, rng = state_input\n",
        "#         rng, rng_step, rng_net = jax.random.split(rng, 3)\n",
        "#         action = model.apply(policy_params, obs, rng_net)\n",
        "#         next_obs, next_state, reward, done, _ = env.step(\n",
        "#           rng_step, state, action, env_params\n",
        "#         )\n",
        "#         carry = [next_obs, next_state, policy_params, rng]\n",
        "#         return carry, [obs, action, reward, next_obs, done]\n",
        "\n",
        "#     # Scan over episode step loop\n",
        "#     _, scan_out = jax.lax.scan(\n",
        "#       policy_step,\n",
        "#       [obs, state, policy_params, rng_episode],\n",
        "#       (),\n",
        "#       steps_in_episode\n",
        "#     )\n",
        "#     # Return masked sum of rewards accumulated by agent in episode\n",
        "#     obs, action, reward, next_obs, done = scan_out\n",
        "#     return obs, action, reward, next_obs, done\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ma1upKSkce4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HfQDJRmkRPIe"
      },
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: in the tfq, tutorial the state was normalized using state_bounds = np.array([2.4, 2.5, 0.21, 2.5])\n",
        "# Need to double check whether it is necessary for our iteration of the code.\n",
        "# - vmapping over n_envs is throwing a weird error which needs further investigation, workaround right now is the set n_envs in conf to 1 and commenting out all vmap comments\n",
        "\n",
        "\n",
        "@eqx.filter_jit\n",
        "def make_train(conf):\n",
        "\n",
        "  conf['n_updates'] = (\n",
        "          conf['total_timesteps'] // conf['n_steps'] // conf['n_envs']\n",
        "      )\n",
        "\n",
        "  conf['mini_batchsize'] = (\n",
        "          conf['n_envs'] * conf['n_steps'] // conf['n_minibatches']\n",
        "      )\n",
        "\n",
        "  env, env_params = gymnax.make(conf['env_name'])\n",
        "  env = FlattenObservationWrapper(env)\n",
        "  env = LogWrapper(env)\n",
        "\n",
        "  n_actions = env.action_space(env_params).n\n",
        "\n",
        "  rng = jax.random.PRNGKey(conf['rng'])\n",
        "  rng, _rng = jax.random.split(rng)\n",
        "  params = {'thetas': jax.random.uniform(\n",
        "      key=_rng, shape=(conf['n_layers'] + 1, conf['n_qubits'], 3),\n",
        "      minval=0.0, maxval=np.pi, dtype=DTYPE\n",
        "      ),\n",
        "            'lmbds': jnp.ones(shape=(conf['n_layers'],\n",
        "                                    conf['n_qubits']), dtype=DTYPE\n",
        "                              ),\n",
        "            'ws': jnp.array([[(-1.) ** i for i in range(n_actions)]], dtype=DTYPE)\n",
        "            }\n",
        "\n",
        "  def train(rng):\n",
        "    actor = QuantumActor(\n",
        "        n_qubits=conf['n_qubits'], n_layers=conf['n_layers'],\n",
        "        beta=conf['beta'], n_actions=n_actions, params=params\n",
        "        )\n",
        "\n",
        "    state_bounds = jnp.array([2.4, 2.5, 0.21, 2.5], dtype=DTYPE)\n",
        "\n",
        "    def map_nested_fn(fn):\n",
        "      '''Recursively apply `fn` to the key-value pairs of a nested dict'''\n",
        "      def map_fn(nested_dict):\n",
        "        return {k: (map_fn(v) if isinstance(v, dict) else fn(k, v))\n",
        "                for k, v in nested_dict.items()}\n",
        "      return map_fn\n",
        "\n",
        "    label_fn = map_nested_fn(lambda k, _: k)\n",
        "    optim = optax.multi_transform({'thetas': optax.amsgrad(conf['lr_theta']),\n",
        "                                  'lmbds': optax.amsgrad(conf['lr_lmbd']),\n",
        "                                  'ws': optax.amsgrad(conf['lr_w'])},\n",
        "                              label_fn)\n",
        "\n",
        "    opt_state = optim.init(params)\n",
        "\n",
        "    train_state = TrainState(model=actor, optimizer=optim, opt_state=opt_state)\n",
        "\n",
        "    rng, _rng = jax.random.split(rng)\n",
        "    # reset_rng = jax.random.split(_rng, conf['n_envs'])\n",
        "    # obs, env_state = jax.vmap(env.reset, in_axes=(0, None))(reset_rng, env_params)\n",
        "    obs, env_state = env.reset(_rng, env_params)\n",
        "\n",
        "    def _update_step(runner_state, unused):\n",
        "      # COLLECT TRAJECTORIES\n",
        "      def _env_step(runner_state, ununsed):\n",
        "        train_state, env_state, last_obs, rng = runner_state\n",
        "        # print(last_obs)\n",
        "        rng, _rng = jax.random.split(rng)\n",
        "        actor = train_state.model\n",
        "        policy = actor(last_obs.reshape(-1) / state_bounds)\n",
        "        action = policy.sample(seed=_rng)\n",
        "        log_prob = policy.log_prob(action)\n",
        "        # print(action, log_prob)\n",
        "\n",
        "        rng, _rng = jax.random.split(rng)\n",
        "        # rng_step = jax.random.split(_rng, conf['n_envs'])\n",
        "\n",
        "        obs, env_state, reward, done, info = env.step(_rng, env_state, action, env_params)\n",
        "\n",
        "        # obs, env_state, reward, done, info = jax.vmap(\n",
        "        #     env.step, in_axes=(0, 0, 0, None)\n",
        "        #     )(rng_step, env_state, action, env_params)\n",
        "\n",
        "        transition = Transition(\n",
        "            done, action, reward, log_prob, last_obs, info)\n",
        "\n",
        "        runner_state = (train_state, env_state, obs, rng)\n",
        "\n",
        "        return runner_state, transition\n",
        "\n",
        "      train_state, traj_batch = jax.lax.scan(_env_step, runner_state, None, conf['n_steps'])\n",
        "\n",
        "      train_state, env_state, last_obs, rng = runner_state\n",
        "\n",
        "      def compute_rewards(traj_batch, gamma):\n",
        "        returns = []\n",
        "        discounted_sum = 0\n",
        "        for r in traj_batch.reward[::-1]:\n",
        "          discounted_sum = r + gamma * discounted_sum\n",
        "          returns.insert(0, discounted_sum)\n",
        "        # returns = (returns - jnp.mean(returns)) / (jnp.std(returns) + 1e-8)\n",
        "\n",
        "        return jnp.array(returns, dtype=DTYPE)\n",
        "      discounted_rewards = compute_rewards(traj_batch, conf['gamma'])\n",
        "      # print(discounted_rewards.shape)\n",
        "\n",
        "      # UPDATE ACTOR\n",
        "      def _update_epoch(update_state, unused):\n",
        "        def _update_minbatch(train_state, batch_info):\n",
        "          traj_batch, discounted_rewards = batch_info\n",
        "\n",
        "          @eqx.filter_value_and_grad\n",
        "          def _loss_fn(actor, traj_batch):\n",
        "            #TODO: can I use vmap here?\n",
        "            # RERUN ACTOR\n",
        "            print(traj_batch.obs.shape)\n",
        "            policy = actor(traj_batch.obs / state_bounds)\n",
        "            log_prob = policy.log_prob(traj_batch.action)\n",
        "            # for stability while training and less variability\n",
        "            returns = (discounted_rewards - jnp.mean(discounted_rewards)) / (jnp.std(discounted_rewards) + 1e-8)\n",
        "            loss = -jnp.mean(log_prob * returns)\n",
        "\n",
        "            return loss\n",
        "\n",
        "          loss, grads = _loss_fn(train_state.model, traj_batch)\n",
        "          print(grads)\n",
        "          train_state = train_state.apply_gradients(train_state.model.get_params(), grads)\n",
        "\n",
        "          return train_state, loss\n",
        "\n",
        "        train_state, traj_batch, discounted_rewards, rng = update_state\n",
        "        rng, _rng = jax.random.split(rng)\n",
        "\n",
        "        # Mini-batch Updates\n",
        "        batch_size = conf['mini_batchsize'] * conf['n_minibatches']\n",
        "        assert (batch_size == conf['n_steps'] * conf['n_envs']\n",
        "        ), 'batch size must be equal to number of steps * number of envs'\n",
        "        permutation = jax.random.permutation(_rng, batch_size)\n",
        "        batch = (traj_batch, discounted_rewards)\n",
        "        # print(batch)\n",
        "\n",
        "        #TODO: @Yash in future. We only need this if we are using VMAP. for now commenting it out!!\n",
        "        # batch = jax.tree_util.tree_map(\n",
        "        #                 lambda x: x.reshape((batch_size,) + x.shape[2:]), batch)\n",
        "        shuffled_batch = jax.tree_util.tree_map(\n",
        "                        lambda x: jnp.take(x, permutation, axis=0), batch\n",
        "                    )\n",
        "\n",
        "        minibatches = jax.tree_util.tree_map(\n",
        "            lambda x: jnp.reshape(\n",
        "                x, [conf['n_minibatches'], -1] + list(x.shape[1:])\n",
        "            ),\n",
        "            shuffled_batch,\n",
        "        )\n",
        "        # print(train_state)\n",
        "        # print(minibatches)\n",
        "\n",
        "        train_state, loss = jax.lax.scan(\n",
        "            _update_minbatch, train_state, minibatches\n",
        "        )\n",
        "        update_state = (train_state, traj_batch, discounted_rewards, rng)\n",
        "        return update_state, loss\n",
        "\n",
        "      # Updating training state and metrics\n",
        "      update_state = (train_state, traj_batch, discounted_rewards, rng)\n",
        "      update_state, loss_info = jax.lax.scan(\n",
        "          _update_epoch, update_state, None, conf['update_epochs']\n",
        "          )\n",
        "      train_state = update_state[0]\n",
        "      metric = traj_batch.info\n",
        "      rng = update_state[-1]\n",
        "\n",
        "      # Debugging mode\n",
        "      if conf['debug']:\n",
        "        def callback(info):\n",
        "          return_values = info['returned_episode_returns'][info['returned_episode']]\n",
        "          timesteps = info['timestep'][info['returned_episode']] * conf['n_envs']\n",
        "          for t in range(len(timesteps)):\n",
        "            print(f\"global step={timesteps[t]}, episodic return={return_values[t]}\")\n",
        "        jax.debug.callback(callback, metric)\n",
        "\n",
        "      runner_state = (train_state, env_state, last_obs, rng)\n",
        "\n",
        "      return runner_state, metric\n",
        "\n",
        "    rng, _rng = jax.random.split(rng)\n",
        "    runner_state = (train_state, env_state, obs, _rng)\n",
        "    runner_state, metric = jax.lax.scan(\n",
        "        _update_step, runner_state, None, conf['n_updates']\n",
        "        )\n",
        "    return {'runner_state': runner_state, 'metrics': metric}\n",
        "\n",
        "  return train"
      ],
      "metadata": {
        "id": "OiOjp2XJrQZ0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conf = {'n_layers': 5,\n",
        "        'n_qubits': 4,\n",
        "        'beta': 1.0,\n",
        "        'n_envs': 1,\n",
        "        'total_timesteps': 125000,\n",
        "        'n_steps': 64,\n",
        "        'gamma': 0.99,\n",
        "        'n_minibatches': 4,\n",
        "        'update_epochs': 4,\n",
        "        'debug': True,\n",
        "        'env_name': 'CartPole-v1',\n",
        "        'lr_theta': 0.001,\n",
        "        'lr_lmbd': 0.1,\n",
        "        'lr_w': 0.1,\n",
        "        'rng': 42}\n",
        "\n",
        "out = make_train(conf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVhMAzG4sgjS",
        "outputId": "b13377b2-cb58-4c78-babc-fa9b746430fd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-8921c385755d>:25: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'>  is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
            "  params = {'thetas': jax.random.uniform(\n",
            "<ipython-input-10-8921c385755d>:29: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in ones is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
            "  'lmbds': jnp.ones(shape=(conf['n_layers'],\n",
            "<ipython-input-10-8921c385755d>:32: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in array is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
            "  'ws': jnp.array([[(-1.) ** i for i in range(n_actions)]], dtype=DTYPE)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out(jax.random.PRNGKey(42))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 795
        },
        "id": "NqTp5XhMsoMQ",
        "outputId": "f85d2f7a-8465-4e48-9088-389af08796bd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-b421d98321bc>:168: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'>  is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
            "  self.theta = jax.random.uniform(key=key, shape=(n_layers + 1, n_qubits, 3),\n",
            "<ipython-input-3-b421d98321bc>:170: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in ones is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
            "  self.lmbd = jnp.ones(shape=(n_layers, n_qubits), dtype=DTYPE)\n",
            "<ipython-input-10-8921c385755d>:41: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in array is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
            "  state_bounds = jnp.array([2.4, 2.5, 0.21, 2.5], dtype=DTYPE)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-b421d98321bc>:196: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in array is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
            "  alt = self.alternating(jnp.array([pqc], dtype=DTYPE))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-b421d98321bc>:196: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in array is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
            "  alt = self.alternating(jnp.array([pqc], dtype=DTYPE))\n",
            "<ipython-input-10-8921c385755d>:145: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in array is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
            "  return jnp.array(returns, dtype=DTYPE)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(16, 4)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "div got incompatible shapes for broadcasting: (64,), (4,).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-704becf01d87>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPRNGKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-8921c385755d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(rng)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_rng\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0mrunner_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_rng\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m     runner_state, metric = jax.lax.scan(\n\u001b[0m\u001b[1;32m    231\u001b[0m         \u001b[0m_update_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunner_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'n_updates'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         )\n",
            "    \u001b[0;31m[... skipping hidden 9 frame]\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-8921c385755d>\u001b[0m in \u001b[0;36m_update_step\u001b[0;34m(runner_state, unused)\u001b[0m\n\u001b[1;32m    206\u001b[0m       \u001b[0;31m# Updating training state and metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m       \u001b[0mupdate_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraj_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscounted_rewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m       update_state, loss_info = jax.lax.scan(\n\u001b[0m\u001b[1;32m    209\u001b[0m           \u001b[0m_update_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'update_epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m           )\n",
            "    \u001b[0;31m[... skipping hidden 9 frame]\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-8921c385755d>\u001b[0m in \u001b[0;36m_update_epoch\u001b[0;34m(update_state, unused)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;31m# print(minibatches)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         train_state, loss = jax.lax.scan(\n\u001b[0m\u001b[1;32m    201\u001b[0m             \u001b[0m_update_minbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminibatches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         )\n",
            "    \u001b[0;31m[... skipping hidden 9 frame]\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-8921c385755d>\u001b[0m in \u001b[0;36m_update_minbatch\u001b[0;34m(train_state, batch_info)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m           \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_loss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraj_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m           \u001b[0mtrain_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "    \u001b[0;31m[... skipping hidden 10 frame]\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-8921c385755d>\u001b[0m in \u001b[0;36m_loss_fn\u001b[0;34m(actor, traj_batch)\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0;31m# RERUN ACTOR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraj_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             \u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraj_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mstate_bounds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m             \u001b[0mlog_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraj_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0;31m# for stability while training and less variability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/numpy/array_methods.py\u001b[0m in \u001b[0;36mop\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    741\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_forward_operator_to_aval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 743\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"_{name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    744\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/numpy/array_methods.py\u001b[0m in \u001b[0;36mdeferring_binary_op\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mswap\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_accepted_binop_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mbinary_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m     \u001b[0;31m# Note: don't use isinstance here, because we don't want to raise for\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[0;31m# subclasses, e.g. NamedTuple objects that may override operators.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/numpy/ufuncs.py\u001b[0m in \u001b[0;36mtrue_divide\u001b[0;34m(x1, x2)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrue_divide\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mArrayLike\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mArrayLike\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m   \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpromote_args_inexact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"true_divide\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0mdivide\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrue_divide\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "    \u001b[0;31m[... skipping hidden 7 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/lax/lax.py\u001b[0m in \u001b[0;36mbroadcasting_shape_rule\u001b[0;34m(name, *avals)\u001b[0m\n\u001b[1;32m   1597\u001b[0m         \u001b[0mresult_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnon_1s\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1598\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1599\u001b[0;31m         raise TypeError(f'{name} got incompatible shapes for broadcasting: '\n\u001b[0m\u001b[1;32m   1600\u001b[0m                         f'{\", \".join(map(str, map(tuple, shapes)))}.')\n\u001b[1;32m   1601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: div got incompatible shapes for broadcasting: (64,), (4,)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env, env_params = gymnax.make(conf['env_name'])\n",
        "env = FlattenObservationWrapper(env)\n",
        "env = LogWrapper(env)"
      ],
      "metadata": {
        "id": "nP8th2o6xLm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env, env_params = gymnax.make(conf['env_name'])\n",
        "env = FlattenObservationWrapper(env)\n",
        "env = LogWrapper(env)\n",
        "\n",
        "n_actions = env.action_space(env_params).n"
      ],
      "metadata": {
        "id": "00kaDGfv0ltW"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rng, _rng = jax.random.split(jax.random.PRNGKey(42))\n",
        "reset_rng = jax.random.split(_rng, 1)\n",
        "obs, env_state = jax.vmap(env.reset, in_axes=(0, None))(reset_rng, env_params)"
      ],
      "metadata": {
        "id": "rIRqZYKh0pEh"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "help(env.step)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwAQCwpg1CIV",
        "outputId": "9987cb1c-a60f-493b-ef0a-e4b0ef0d8e21"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on method step in module __main__:\n",
            "\n",
            "step(key: jax.Array, state: gymnax.environments.environment.EnvState, action: Union[int, float], params: Optional[gymnax.environments.environment.EnvParams] = None) -> Tuple[Union[jax.Array, numpy.ndarray, numpy.bool_, numpy.number], gymnax.environments.environment.EnvState, float, bool, dict] method of __main__.LogWrapper instance\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env_state.env_state."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb7i7VkN1zCw",
        "outputId": "210a69e7-526b-44db-9759-faa34b9e49ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method EnvState.__repr__ of EnvState(x=Array([0.04827876], dtype=float64), x_dot=Array([0.03837702], dtype=float64), theta=Array([0.00256703], dtype=float64), theta_dot=Array([-0.03793145], dtype=float64), time=Array([0], dtype=int64, weak_type=True))>"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "obs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_xudcMr4L26",
        "outputId": "ff0da4ae-9da9-45ec-b918-50935e2844eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([[ 0.04827876,  0.03837702,  0.00256703, -0.03793145]], dtype=float64)"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.array([[(-1.) ** i for i in range(2)]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQCextkE68As",
        "outputId": "28f3ed53-01ee-40a7-ea06-cdd97e760e9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1., -1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cirq\n",
        "from functools import reduce\n"
      ],
      "metadata": {
        "id": "00VqfL7aa7Is"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qubits = cirq.GridQubit.rect(1, 4)\n",
        "ops = [cirq.Z(q) for q in qubits]\n",
        "observables = [reduce((lambda x, y: x * y), ops)]"
      ],
      "metadata": {
        "id": "sG6SXcKlbOnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "observables"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKiSXYJ9bWO4",
        "outputId": "8d5c36ff-f4ac-48bc-e3d1-a0811b31c502"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Z(q(0, 0))*Z(q(0, 1))*Z(q(0, 2))*Z(q(0, 3))]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input = [tf.keras.Input(shape=(4,), dtype=tf.dtypes.float32, name='input')]\n",
        "\n",
        "tf.gather(tf.shape(input[0]), 0)\n",
        "\n",
        "import sympy"
      ],
      "metadata": {
        "id": "l_BH93xkbc1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import flax.linen as nn\n",
        "import numpy as np\n",
        "import optax\n",
        "from flax.linen.initializers import constant, orthogonal\n",
        "from typing import Sequence, NamedTuple, Any\n",
        "from flax.training.train_state import TrainState\n",
        "import distrax\n",
        "import gymnax\n",
        "# from purejaxrl.wrappers import LogWrapper, FlattenObservationWrapper\n",
        "\n",
        "\n",
        "class ActorCritic(nn.Module):\n",
        "    action_dim: Sequence[int]\n",
        "    activation: str = \"tanh\"\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        if self.activation == \"relu\":\n",
        "            activation = nn.relu\n",
        "        else:\n",
        "            activation = nn.tanh\n",
        "        actor_mean = nn.Dense(\n",
        "            64, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
        "        )(x)\n",
        "        actor_mean = activation(actor_mean)\n",
        "        actor_mean = nn.Dense(\n",
        "            64, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
        "        )(actor_mean)\n",
        "        actor_mean = activation(actor_mean)\n",
        "        actor_mean = nn.Dense(\n",
        "            self.action_dim, kernel_init=orthogonal(0.01), bias_init=constant(0.0)\n",
        "        )(actor_mean)\n",
        "        pi = distrax.Categorical(logits=actor_mean)\n",
        "\n",
        "        critic = nn.Dense(\n",
        "            64, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
        "        )(x)\n",
        "        critic = activation(critic)\n",
        "        critic = nn.Dense(\n",
        "            64, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
        "        )(critic)\n",
        "        critic = activation(critic)\n",
        "        critic = nn.Dense(1, kernel_init=orthogonal(1.0), bias_init=constant(0.0))(\n",
        "            critic\n",
        "        )\n",
        "\n",
        "        return pi, jnp.squeeze(critic, axis=-1)\n",
        "\n",
        "\n",
        "class Transition(NamedTuple):\n",
        "    done: jnp.ndarray\n",
        "    action: jnp.ndarray\n",
        "    value: jnp.ndarray\n",
        "    reward: jnp.ndarray\n",
        "    log_prob: jnp.ndarray\n",
        "    obs: jnp.ndarray\n",
        "    info: jnp.ndarray\n",
        "\n",
        "\n",
        "def make_train(config):\n",
        "    config[\"NUM_UPDATES\"] = (\n",
        "        config[\"TOTAL_TIMESTEPS\"] // config[\"NUM_STEPS\"] // config[\"NUM_ENVS\"]\n",
        "    )\n",
        "    config[\"MINIBATCH_SIZE\"] = (\n",
        "        config[\"NUM_ENVS\"] * config[\"NUM_STEPS\"] // config[\"NUM_MINIBATCHES\"]\n",
        "    )\n",
        "    env, env_params = gymnax.make(config[\"ENV_NAME\"])\n",
        "    env = FlattenObservationWrapper(env)\n",
        "    env = LogWrapper(env)\n",
        "\n",
        "    def linear_schedule(count):\n",
        "        frac = (\n",
        "            1.0\n",
        "            - (count // (config[\"NUM_MINIBATCHES\"] * config[\"UPDATE_EPOCHS\"]))\n",
        "            / config[\"NUM_UPDATES\"]\n",
        "        )\n",
        "        return config[\"LR\"] * frac\n",
        "\n",
        "    def train(rng):\n",
        "        # INIT NETWORK\n",
        "        network = ActorCritic(\n",
        "            env.action_space(env_params).n, activation=config[\"ACTIVATION\"]\n",
        "        )\n",
        "        rng, _rng = jax.random.split(rng)\n",
        "        init_x = jnp.zeros(env.observation_space(env_params).shape)\n",
        "        print(f'init_x {init_x.shape}')\n",
        "        network_params = network.init(_rng, init_x)\n",
        "        if config[\"ANNEAL_LR\"]:\n",
        "            tx = optax.chain(\n",
        "                optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
        "                optax.adam(learning_rate=linear_schedule, eps=1e-5),\n",
        "            )\n",
        "        else:\n",
        "            tx = optax.chain(\n",
        "                optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
        "                optax.adam(config[\"LR\"], eps=1e-5),\n",
        "            )\n",
        "        train_state = TrainState.create(\n",
        "            apply_fn=network.apply,\n",
        "            params=network_params,\n",
        "            tx=tx,\n",
        "        )\n",
        "\n",
        "        # INIT ENV\n",
        "        rng, _rng = jax.random.split(rng)\n",
        "        reset_rng = jax.random.split(_rng, config[\"NUM_ENVS\"])\n",
        "        obsv, env_state = jax.vmap(env.reset, in_axes=(0, None))(reset_rng, env_params)\n",
        "\n",
        "        # TRAIN LOOP\n",
        "        def _update_step(runner_state, unused):\n",
        "            # COLLECT TRAJECTORIES\n",
        "            def _env_step(runner_state, unused):\n",
        "                train_state, env_state, last_obs, rng = runner_state\n",
        "\n",
        "                # SELECT ACTION\n",
        "                rng, _rng = jax.random.split(rng)\n",
        "                pi, value = network.apply(train_state.params, last_obs)\n",
        "                action = pi.sample(seed=_rng)\n",
        "                log_prob = pi.log_prob(action)\n",
        "\n",
        "                # STEP ENV\n",
        "                rng, _rng = jax.random.split(rng)\n",
        "                rng_step = jax.random.split(_rng, config[\"NUM_ENVS\"])\n",
        "\n",
        "                print(rng_step.shape)\n",
        "                print(action.shape)\n",
        "                obsv, env_state, reward, done, info = jax.vmap(\n",
        "                    env.step, in_axes=(0, 0, 0, None)\n",
        "                )(rng_step, env_state, action, env_params)\n",
        "                transition = Transition(\n",
        "                    done, action, value, reward, log_prob, last_obs, info\n",
        "                )\n",
        "                runner_state = (train_state, env_state, obsv, rng)\n",
        "                return runner_state, transition\n",
        "\n",
        "            runner_state, traj_batch = jax.lax.scan(\n",
        "                _env_step, runner_state, None, config[\"NUM_STEPS\"]\n",
        "            )\n",
        "\n",
        "            # CALCULATE ADVANTAGE\n",
        "            train_state, env_state, last_obs, rng = runner_state\n",
        "            _, last_val = network.apply(train_state.params, last_obs)\n",
        "\n",
        "            def _calculate_gae(traj_batch, last_val):\n",
        "                def _get_advantages(gae_and_next_value, transition):\n",
        "                    gae, next_value = gae_and_next_value\n",
        "                    done, value, reward = (\n",
        "                        transition.done,\n",
        "                        transition.value,\n",
        "                        transition.reward,\n",
        "                    )\n",
        "                    delta = reward + config[\"GAMMA\"] * next_value * (1 - done) - value\n",
        "                    gae = (\n",
        "                        delta\n",
        "                        + config[\"GAMMA\"] * config[\"GAE_LAMBDA\"] * (1 - done) * gae\n",
        "                    )\n",
        "                    return (gae, value), gae\n",
        "\n",
        "                _, advantages = jax.lax.scan(\n",
        "                    _get_advantages,\n",
        "                    (jnp.zeros_like(last_val), last_val),\n",
        "                    traj_batch,\n",
        "                    reverse=True,\n",
        "                    unroll=16,\n",
        "                )\n",
        "                return advantages, advantages + traj_batch.value\n",
        "\n",
        "            advantages, targets = _calculate_gae(traj_batch, last_val)\n",
        "            print(traj_batch.reward.shape)\n",
        "            print(advantages.shape)\n",
        "\n",
        "            # UPDATE NETWORK\n",
        "            def _update_epoch(update_state, unused):\n",
        "                def _update_minbatch(train_state, batch_info):\n",
        "                    traj_batch, advantages, targets = batch_info\n",
        "\n",
        "                    def _loss_fn(params, traj_batch, gae, targets):\n",
        "                        # RERUN NETWORK\n",
        "                        pi, value = network.apply(params, traj_batch.obs)\n",
        "                        log_prob = pi.log_prob(traj_batch.action)\n",
        "\n",
        "                        # CALCULATE VALUE LOSS\n",
        "                        value_pred_clipped = traj_batch.value + (\n",
        "                            value - traj_batch.value\n",
        "                        ).clip(-config[\"CLIP_EPS\"], config[\"CLIP_EPS\"])\n",
        "                        value_losses = jnp.square(value - targets)\n",
        "                        value_losses_clipped = jnp.square(value_pred_clipped - targets)\n",
        "                        value_loss = (\n",
        "                            0.5 * jnp.maximum(value_losses, value_losses_clipped).mean()\n",
        "                        )\n",
        "\n",
        "                        # CALCULATE ACTOR LOSS\n",
        "                        ratio = jnp.exp(log_prob - traj_batch.log_prob)\n",
        "                        gae = (gae - gae.mean()) / (gae.std() + 1e-8)\n",
        "                        loss_actor1 = ratio * gae\n",
        "                        loss_actor2 = (\n",
        "                            jnp.clip(\n",
        "                                ratio,\n",
        "                                1.0 - config[\"CLIP_EPS\"],\n",
        "                                1.0 + config[\"CLIP_EPS\"],\n",
        "                            )\n",
        "                            * gae\n",
        "                        )\n",
        "                        loss_actor = -jnp.minimum(loss_actor1, loss_actor2)\n",
        "                        loss_actor = loss_actor.mean()\n",
        "                        entropy = pi.entropy().mean()\n",
        "\n",
        "                        total_loss = (\n",
        "                            loss_actor\n",
        "                            + config[\"VF_COEF\"] * value_loss\n",
        "                            - config[\"ENT_COEF\"] * entropy\n",
        "                        )\n",
        "                        return total_loss, (value_loss, loss_actor, entropy)\n",
        "\n",
        "                    grad_fn = jax.value_and_grad(_loss_fn, has_aux=True)\n",
        "                    total_loss, grads = grad_fn(\n",
        "                        train_state.params, traj_batch, advantages, targets\n",
        "                    )\n",
        "                    train_state = train_state.apply_gradients(grads=grads)\n",
        "                    return train_state, total_loss\n",
        "\n",
        "                train_state, traj_batch, advantages, targets, rng = update_state\n",
        "                rng, _rng = jax.random.split(rng)\n",
        "                # Batching and Shuffling\n",
        "                batch_size = config[\"MINIBATCH_SIZE\"] * config[\"NUM_MINIBATCHES\"]\n",
        "                assert (\n",
        "                    batch_size == config[\"NUM_STEPS\"] * config[\"NUM_ENVS\"]\n",
        "                ), \"batch size must be equal to number of steps * number of envs\"\n",
        "                permutation = jax.random.permutation(_rng, batch_size)\n",
        "                batch = (traj_batch, advantages, targets)\n",
        "                # print(advantages.shape)\n",
        "                # print(targets.shape)\n",
        "                print(batch)\n",
        "                # print(traj_batch.shape)\n",
        "                batch = jax.tree_util.tree_map(\n",
        "                    lambda x: x.reshape((batch_size,) + x.shape[2:]), batch\n",
        "                )\n",
        "                print(batch)\n",
        "                # print(traj_batch.reshape((batch_size,) + traj_batch.shape[2:]))\n",
        "                # batch[]\n",
        "                # print(traj_batch.reshape((batch_size,) + traj_batch.shape[2:]).shape)\n",
        "\n",
        "                shuffled_batch = jax.tree_util.tree_map(\n",
        "                    lambda x: jnp.take(x, permutation, axis=0), batch\n",
        "                )\n",
        "                # Mini-batch Updates\n",
        "                minibatches = jax.tree_util.tree_map(\n",
        "                    lambda x: jnp.reshape(\n",
        "                        x, [config[\"NUM_MINIBATCHES\"], -1] + list(x.shape[1:])\n",
        "                    ),\n",
        "                    shuffled_batch,\n",
        "                )\n",
        "                train_state, total_loss = jax.lax.scan(\n",
        "                    _update_minbatch, train_state, minibatches\n",
        "                )\n",
        "                update_state = (train_state, traj_batch, advantages, targets, rng)\n",
        "                return update_state, total_loss\n",
        "            # Updating Training State and Metrics:\n",
        "            update_state = (train_state, traj_batch, advantages, targets, rng)\n",
        "            update_state, loss_info = jax.lax.scan(\n",
        "                _update_epoch, update_state, None, config[\"UPDATE_EPOCHS\"]\n",
        "            )\n",
        "            train_state = update_state[0]\n",
        "            metric = traj_batch.info\n",
        "            rng = update_state[-1]\n",
        "\n",
        "            # Debugging mode\n",
        "            if config.get(\"DEBUG\"):\n",
        "                def callback(info):\n",
        "                    return_values = info[\"returned_episode_returns\"][info[\"returned_episode\"]]\n",
        "                    timesteps = info[\"timestep\"][info[\"returned_episode\"]] * config[\"NUM_ENVS\"]\n",
        "                    for t in range(len(timesteps)):\n",
        "                        print(f\"global step={timesteps[t]}, episodic return={return_values[t]}\")\n",
        "                jax.debug.callback(callback, metric)\n",
        "\n",
        "            runner_state = (train_state, env_state, last_obs, rng)\n",
        "            return runner_state, metric\n",
        "\n",
        "        rng, _rng = jax.random.split(rng)\n",
        "        runner_state = (train_state, env_state, obsv, _rng)\n",
        "        runner_state, metric = jax.lax.scan(\n",
        "            _update_step, runner_state, None, config[\"NUM_UPDATES\"]\n",
        "        )\n",
        "        return {\"runner_state\": runner_state, \"metrics\": metric}\n",
        "\n",
        "    return train\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  # conf = {'n_layers': 5,\n",
        "  #       'n_qubits': 4,\n",
        "  #       'beta': 1.0,\n",
        "  #       'n_envs': 1,\n",
        "  #       'total_timesteps': 125000,\n",
        "  #       'n_steps': 64,\n",
        "  #       'gamma': 0.99,\n",
        "  #       'n_minibatches': 4,\n",
        "  #       'update_epochs': 4,\n",
        "  #       'debug': True,\n",
        "  #       'env_name': 'CartPole-v1',\n",
        "  #       'lr_theta': 0.001,\n",
        "  #       'lr_lmbd': 0.1,\n",
        "  #       'lr_w': 0.1,\n",
        "  #       'rng': 42}\n",
        "\n",
        "\n",
        "    config = {\n",
        "        \"LR\": 2.5e-4,\n",
        "        \"NUM_ENVS\": 1,\n",
        "        \"NUM_STEPS\": 64,\n",
        "        \"TOTAL_TIMESTEPS\": 125000,\n",
        "        \"UPDATE_EPOCHS\": 4,\n",
        "        \"NUM_MINIBATCHES\": 4,\n",
        "        \"GAMMA\": 0.99,\n",
        "        \"GAE_LAMBDA\": 0.95,\n",
        "        \"CLIP_EPS\": 0.2,\n",
        "        \"ENT_COEF\": 0.01,\n",
        "        \"VF_COEF\": 0.5,\n",
        "        \"MAX_GRAD_NORM\": 0.5,\n",
        "        \"ACTIVATION\": \"tanh\",\n",
        "        \"ENV_NAME\": \"CartPole-v1\",\n",
        "        \"ANNEAL_LR\": True,\n",
        "        \"DEBUG\": True,\n",
        "    }\n",
        "    rng = jax.random.PRNGKey(30)\n",
        "    train_jit = jax.jit(make_train(config))\n",
        "    out = train_jit(rng)"
      ],
      "metadata": {
        "id": "T3lUu-Scnd6J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 653
        },
        "outputId": "8042b864-9bb7-44cb-a260-adc4835da06b"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "init_x (4,)\n",
            "(1, 2)\n",
            "(1,)\n",
            "(1, 2)\n",
            "(1,)\n",
            "(64, 1)\n",
            "(64, 1)\n",
            "(Transition(done=Traced<ShapedArray(bool[64,1])>with<DynamicJaxprTrace(level=3/0)>, action=Traced<ShapedArray(int64[64,1])>with<DynamicJaxprTrace(level=3/0)>, value=Traced<ShapedArray(float64[64,1])>with<DynamicJaxprTrace(level=3/0)>, reward=Traced<ShapedArray(float64[64,1], weak_type=True)>with<DynamicJaxprTrace(level=3/0)>, log_prob=Traced<ShapedArray(float64[64,1])>with<DynamicJaxprTrace(level=3/0)>, obs=Traced<ShapedArray(float64[64,1,4])>with<DynamicJaxprTrace(level=3/0)>, info={'discount': Traced<ShapedArray(float64[64,1], weak_type=True)>with<DynamicJaxprTrace(level=3/0)>, 'returned_episode': Traced<ShapedArray(bool[64,1])>with<DynamicJaxprTrace(level=3/0)>, 'returned_episode_lengths': Traced<ShapedArray(int64[64,1], weak_type=True)>with<DynamicJaxprTrace(level=3/0)>, 'returned_episode_returns': Traced<ShapedArray(float64[64,1])>with<DynamicJaxprTrace(level=3/0)>, 'timestep': Traced<ShapedArray(int64[64,1], weak_type=True)>with<DynamicJaxprTrace(level=3/0)>}), Traced<ShapedArray(float64[64,1])>with<DynamicJaxprTrace(level=3/0)>, Traced<ShapedArray(float64[64,1])>with<DynamicJaxprTrace(level=3/0)>)\n",
            "(Transition(done=Traced<ShapedArray(bool[64])>with<DynamicJaxprTrace(level=3/0)>, action=Traced<ShapedArray(int64[64])>with<DynamicJaxprTrace(level=3/0)>, value=Traced<ShapedArray(float64[64])>with<DynamicJaxprTrace(level=3/0)>, reward=Traced<ShapedArray(float64[64], weak_type=True)>with<DynamicJaxprTrace(level=3/0)>, log_prob=Traced<ShapedArray(float64[64])>with<DynamicJaxprTrace(level=3/0)>, obs=Traced<ShapedArray(float64[64,4])>with<DynamicJaxprTrace(level=3/0)>, info={'discount': Traced<ShapedArray(float64[64], weak_type=True)>with<DynamicJaxprTrace(level=3/0)>, 'returned_episode': Traced<ShapedArray(bool[64])>with<DynamicJaxprTrace(level=3/0)>, 'returned_episode_lengths': Traced<ShapedArray(int64[64], weak_type=True)>with<DynamicJaxprTrace(level=3/0)>, 'returned_episode_returns': Traced<ShapedArray(float64[64])>with<DynamicJaxprTrace(level=3/0)>, 'timestep': Traced<ShapedArray(int64[64], weak_type=True)>with<DynamicJaxprTrace(level=3/0)>}), Traced<ShapedArray(float64[64])>with<DynamicJaxprTrace(level=3/0)>, Traced<ShapedArray(float64[64])>with<DynamicJaxprTrace(level=3/0)>)\n",
            "(1, 2)\n",
            "(1,)\n",
            "(64, 1)\n",
            "(64, 1)\n",
            "(Transition(done=Traced<ShapedArray(bool[64,1])>with<DynamicJaxprTrace(level=3/0)>, action=Traced<ShapedArray(int64[64,1])>with<DynamicJaxprTrace(level=3/0)>, value=Traced<ShapedArray(float64[64,1])>with<DynamicJaxprTrace(level=3/0)>, reward=Traced<ShapedArray(float64[64,1], weak_type=True)>with<DynamicJaxprTrace(level=3/0)>, log_prob=Traced<ShapedArray(float64[64,1])>with<DynamicJaxprTrace(level=3/0)>, obs=Traced<ShapedArray(float64[64,1,4])>with<DynamicJaxprTrace(level=3/0)>, info={'discount': Traced<ShapedArray(float64[64,1], weak_type=True)>with<DynamicJaxprTrace(level=3/0)>, 'returned_episode': Traced<ShapedArray(bool[64,1])>with<DynamicJaxprTrace(level=3/0)>, 'returned_episode_lengths': Traced<ShapedArray(int64[64,1], weak_type=True)>with<DynamicJaxprTrace(level=3/0)>, 'returned_episode_returns': Traced<ShapedArray(float64[64,1])>with<DynamicJaxprTrace(level=3/0)>, 'timestep': Traced<ShapedArray(int64[64,1], weak_type=True)>with<DynamicJaxprTrace(level=3/0)>}), Traced<ShapedArray(float64[64,1])>with<DynamicJaxprTrace(level=3/0)>, Traced<ShapedArray(float64[64,1])>with<DynamicJaxprTrace(level=3/0)>)\n",
            "(Transition(done=Traced<ShapedArray(bool[64])>with<DynamicJaxprTrace(level=3/0)>, action=Traced<ShapedArray(int64[64])>with<DynamicJaxprTrace(level=3/0)>, value=Traced<ShapedArray(float64[64])>with<DynamicJaxprTrace(level=3/0)>, reward=Traced<ShapedArray(float64[64], weak_type=True)>with<DynamicJaxprTrace(level=3/0)>, log_prob=Traced<ShapedArray(float64[64])>with<DynamicJaxprTrace(level=3/0)>, obs=Traced<ShapedArray(float64[64,4])>with<DynamicJaxprTrace(level=3/0)>, info={'discount': Traced<ShapedArray(float64[64], weak_type=True)>with<DynamicJaxprTrace(level=3/0)>, 'returned_episode': Traced<ShapedArray(bool[64])>with<DynamicJaxprTrace(level=3/0)>, 'returned_episode_lengths': Traced<ShapedArray(int64[64], weak_type=True)>with<DynamicJaxprTrace(level=3/0)>, 'returned_episode_returns': Traced<ShapedArray(float64[64])>with<DynamicJaxprTrace(level=3/0)>, 'timestep': Traced<ShapedArray(int64[64], weak_type=True)>with<DynamicJaxprTrace(level=3/0)>}), Traced<ShapedArray(float64[64])>with<DynamicJaxprTrace(level=3/0)>, Traced<ShapedArray(float64[64])>with<DynamicJaxprTrace(level=3/0)>)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-121-321a8270d26c>\u001b[0m in \u001b[0;36m<cell line: 291>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0mrng\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPRNGKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0mtrain_jit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmake_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_jit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/pjit.py\u001b[0m in \u001b[0;36mcache_miss\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    255\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mapi_boundary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcache_miss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m     outs, out_flat, out_tree, args_flat, jaxpr = _python_pjit_helper(\n\u001b[0m\u001b[1;32m    258\u001b[0m         fun, infer_params_fn, *args, **kwargs)\n\u001b[1;32m    259\u001b[0m     \u001b[0mexecutable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_most_recent_pjit_call_executable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjaxpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/pjit.py\u001b[0m in \u001b[0;36m_python_pjit_helper\u001b[0;34m(fun, infer_params_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0mdispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_arg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m     \u001b[0mout_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpjit_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mpxla\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDeviceAssignmentMismatchError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0mfails\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/core.py\u001b[0m in \u001b[0;36mbind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m   2738\u001b[0m     top_trace = (top_trace if not axis_main or axis_main.level < top_trace.level\n\u001b[1;32m   2739\u001b[0m                  else axis_main.with_cur_sublevel())\n\u001b[0;32m-> 2740\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind_with_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_trace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/core.py\u001b[0m in \u001b[0;36mbind_with_trace\u001b[0;34m(self, trace, args, params)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbind_with_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_primitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_raise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_lower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiple_results\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfull_lower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/core.py\u001b[0m in \u001b[0;36mprocess_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess_primitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 935\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    936\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/pjit.py\u001b[0m in \u001b[0;36m_pjit_call_impl\u001b[0;34m(jaxpr, in_shardings, out_shardings, resource_env, donated_invars, name, keep_unused, inline, *args)\u001b[0m\n\u001b[1;32m   1243\u001b[0m   has_explicit_sharding = _pjit_explicit_sharding(\n\u001b[1;32m   1244\u001b[0m       in_shardings, out_shardings, None, None)\n\u001b[0;32m-> 1245\u001b[0;31m   return xc._xla.pjit(name, f, call_impl_cache_miss, [], [], donated_argnums,\n\u001b[0m\u001b[1;32m   1246\u001b[0m                       \u001b[0mtree_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_registry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1247\u001b[0m                       _get_cpp_global_cache(has_explicit_sharding))(*args)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/pjit.py\u001b[0m in \u001b[0;36mcall_impl_cache_miss\u001b[0;34m(*args_, **kwargs_)\u001b[0m\n\u001b[1;32m   1227\u001b[0m                     donated_invars, name, keep_unused, inline):\n\u001b[1;32m   1228\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_impl_cache_miss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1229\u001b[0;31m     out_flat, compiled = _pjit_call_impl_python(\n\u001b[0m\u001b[1;32m   1230\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjaxpr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjaxpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_shardings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0min_shardings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m         \u001b[0mout_shardings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_shardings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresource_env\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresource_env\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/pjit.py\u001b[0m in \u001b[0;36m_pjit_call_impl_python\u001b[0;34m(jaxpr, in_shardings, out_shardings, resource_env, donated_invars, name, keep_unused, inline, *args)\u001b[0m\n\u001b[1;32m   1163\u001b[0m       \u001b[0mjaxpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_shardings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_shardings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresource_env\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1164\u001b[0m       \u001b[0mdonated_invars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_unused\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1165\u001b[0;31m       lowering_parameters=mlir.LoweringParameters()).compile()\n\u001b[0m\u001b[1;32m   1166\u001b[0m   \u001b[0m_most_recent_pjit_call_executable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweak_key_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjaxpr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m   \u001b[0;31m# This check is expensive so only do it if enable_checks is on.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/interpreters/pxla.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, compiler_options)\u001b[0m\n\u001b[1;32m   2217\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompiler_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mMeshExecutable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2218\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_executable\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcompiler_options\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2219\u001b[0;31m       executable = UnloadedMeshExecutable.from_hlo(\n\u001b[0m\u001b[1;32m   2220\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hlo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2221\u001b[0m           compiler_options=compiler_options)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/interpreters/pxla.py\u001b[0m in \u001b[0;36mfrom_hlo\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   2657\u001b[0m           \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2659\u001b[0;31m     xla_executable, compile_options = _cached_compilation(\n\u001b[0m\u001b[1;32m   2660\u001b[0m         \u001b[0mhlo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmesh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspmd_lowering\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2661\u001b[0m         \u001b[0mtuple_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauto_spmd_lowering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_prop_to_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/interpreters/pxla.py\u001b[0m in \u001b[0;36m_cached_compilation\u001b[0;34m(computation, name, mesh, spmd_lowering, tuple_args, auto_spmd_lowering, _allow_propagation_to_outputs, host_callbacks, backend, da, pmap_nreps, compiler_options_keys, compiler_options_values)\u001b[0m\n\u001b[1;32m   2526\u001b[0m       \u001b[0;34m\"Finished XLA compilation of {fun_name} in {elapsed_time} sec\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2527\u001b[0m       fun_name=name, event=dispatch.BACKEND_COMPILE_EVENT):\n\u001b[0;32m-> 2528\u001b[0;31m     xla_executable = compiler.compile_or_get_cached(\n\u001b[0m\u001b[1;32m   2529\u001b[0m         backend, computation, dev, compile_options, host_callbacks)\n\u001b[1;32m   2530\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mxla_executable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/compiler.py\u001b[0m in \u001b[0;36mcompile_or_get_cached\u001b[0;34m(backend, computation, devices, compile_options, host_callbacks)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0muse_compilation_cache\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m     return backend_compile(backend, computation, compile_options,\n\u001b[0m\u001b[1;32m    297\u001b[0m                            host_callbacks)\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/profiler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    334\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mTraceAnnotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdecorator_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/compiler.py\u001b[0m in \u001b[0;36mbackend_compile\u001b[0;34m(backend, module, options, host_callbacks)\u001b[0m\n\u001b[1;32m    249\u001b[0m   \u001b[0;31m# separately in Python profiling results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mhost_callbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     return backend.compile(built_c, compile_options=options,\n\u001b[0m\u001b[1;32m    252\u001b[0m                            host_callbacks=host_callbacks)\n\u001b[1;32m    253\u001b[0m   \u001b[0;31m# Some backends don't have `host_callbacks` option yet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from flax import linen as nn\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    \"\"\"Simple ReLU MLP.\"\"\"\n",
        "\n",
        "    num_hidden_units: int\n",
        "    num_hidden_layers: int\n",
        "    num_output_units: int\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x, rng):\n",
        "        for l in range(self.num_hidden_layers):\n",
        "            x = nn.Dense(features=self.num_hidden_units)(x)\n",
        "            x = nn.relu(x)\n",
        "        x = nn.Dense(features=self.num_output_units)(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "model = MLP(48, 1, 1)\n",
        "policy_params = model.init(jax.random.PRNGKey(0), jnp.zeros(3), None)\n",
        "print(type(policy_params))"
      ],
      "metadata": {
        "id": "lacJpatynvkI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64d4565c-1b63-4511-8cf3-aa0f22dac337"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'dict'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rewards = [1,2,3,4,5,6,7,10]\n",
        "gamma = 0.99\n",
        "\n",
        "\n",
        "returns = []\n",
        "discounted_sum = 0\n",
        "for r in rewards[::-1]:\n",
        "  discounted_sum = r + gamma * discounted_sum\n",
        "  returns.insert(0, discounted_sum)\n",
        "\n",
        "returns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRixLUg7drTc",
        "outputId": "e4e0a781-76fc-4ee2-d5ee-875cfcfe8aec"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[36.2214308742769,\n",
              " 35.577202903309995,\n",
              " 33.916366569,\n",
              " 31.2286531,\n",
              " 27.50369,\n",
              " 22.730999999999998,\n",
              " 16.9,\n",
              " 10.0]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "jnp.zeros_like(1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNm-c1YCr65N",
        "outputId": "81a96a17-427b-4bb4-ec4b-e22af83a2ac9"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array(0, dtype=int32, weak_type=True)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "returns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6UrucnqrqBN",
        "outputId": "b8efdfbe-a59c-41bd-f1fd-06d0ed82e6b9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[23.472496000000003,\n",
              " 24.969440000000002,\n",
              " 25.521600000000003,\n",
              " 25.024,\n",
              " 23.36,\n",
              " 20.4,\n",
              " 16.0,\n",
              " 10.0]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#     def policy_step(state_input, tmp):\n",
        "#         \"\"\"lax.scan compatible step transition in jax env.\"\"\"\n",
        "#         obs, state, policy_params, rng = state_input\n",
        "#         rng, rng_step, rng_net = jax.random.split(rng, 3)\n",
        "#         action = model.apply(policy_params, obs, rng_net)\n",
        "#         next_obs, next_state, reward, done, _ = env.step(\n",
        "#           rng_step, state, action, env_params\n",
        "#         )\n",
        "#         carry = [next_obs, next_state, policy_params, rng]\n",
        "#         return carry, [obs, action, reward, next_obs, done]\n",
        "\n",
        "def _calculate_gae(traj_batch, last_val):\n",
        "    def _get_advantages(gae_and_next_value, transition):\n",
        "        gae, next_value = gae_and_next_value\n",
        "        done, value, reward = (\n",
        "            transition.done,\n",
        "            transition.value,\n",
        "            transition.reward,\n",
        "        )\n",
        "        delta = reward + config[\"GAMMA\"] * next_value * (1 - done) - value\n",
        "        gae = (\n",
        "            delta\n",
        "            + config[\"GAMMA\"] * config[\"GAE_LAMBDA\"] * (1 - done) * gae\n",
        "        )\n",
        "        return (gae, value), gae\n",
        "\n",
        "    _, advantages = jax.lax.scan(\n",
        "        _get_advantages,\n",
        "        (jnp.zeros_like(last_val), last_val),\n",
        "        traj_batch,\n",
        "        reverse=True,\n",
        "        unroll=16,\n",
        "    )\n",
        "    return advantages, advantages + traj_batch.value\n",
        "\n",
        "\n",
        "def _calculate_returns(traj_batch):\n",
        "  def _discounted_sum(return_and_next_sum, transition):\n",
        "    discounted_sum = return_and_next_sum\n",
        "    done, reward =\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HI7lHglYrtIk"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Trans(NamedTuple):\n",
        "  reward: jnp.ndarray\n",
        "\n",
        "def calculate_returns(traj_batch):\n",
        "    def _compute_discounted_sum(carry, transition):\n",
        "        rewards_to_go = carry\n",
        "        reward = transition.reward\n",
        "        rewards_to_go = reward + conf['gamma'] * rewards_to_go\n",
        "        baseline = 0\n",
        "        return rewards_to_go, rewards_to_go\n",
        "\n",
        "    init_carry = jnp.zeros_like(0, dtype=DTYPE)\n",
        "\n",
        "    _, returns = jax.lax.scan(\n",
        "        _compute_discounted_sum,\n",
        "        init_carry,\n",
        "        traj_batch,\n",
        "        reverse=True,\n",
        "    )\n",
        "    return returns\n",
        "\n",
        "traj_batch = Trans(reward=jnp.array([1,2,3,4,5,6,7,10], dtype=DTYPE))\n",
        "calculate_returns(traj_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyK_uVHP0CO5",
        "outputId": "b88607cb-fca5-4cad-eac9-a9f78daee05a"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-43-c4be01b3cdec>:22: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in array is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
            "  traj_batch = Trans(reward=jnp.array([1,2,3,4,5,6,7,10], dtype=DTYPE))\n",
            "<ipython-input-43-c4be01b3cdec>:12: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in zeros_like is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
            "  init_carry = jnp.zeros_like(0, dtype=DTYPE)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([36.22143 , 35.5772  , 33.916367, 31.228653, 27.503689, 22.730999,\n",
              "       16.9     , 10.      ], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def rcal(rewards_history, ununsed):\n",
        "  discounted_sum, reward = rewards_history\n",
        "\n",
        "  new_discounted_sum = reward + 0.99 * discounted_sum\n",
        "\n",
        "  carry = (new_discounted_sum, reward)\n",
        "\n",
        "  return carry, new_discounted_sum\n",
        "\n",
        "_, ds = jax.lax.scan(\n",
        "        rcal,\n",
        "        (0.0, [1,2,3,4,5,6,7,10]),\n",
        "        None,\n",
        "        reverse=True,\n",
        "        length=8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "DOYcn44btpf6",
        "outputId": "f71fb7ac-b386-4633-f592-bd0abe8d90d9"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "unsupported operand type(s) for +: 'list' and 'DynamicJaxprTracer'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-27cf80889960>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mcarry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_discounted_sum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m _, ds = jax.lax.scan(\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mrcal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "    \u001b[0;31m[... skipping hidden 9 frame]\u001b[0m\n",
            "\u001b[0;32m<ipython-input-41-27cf80889960>\u001b[0m in \u001b[0;36mrcal\u001b[0;34m(rewards_history, ununsed)\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mdiscounted_sum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrewards_history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mnew_discounted_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.99\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdiscounted_sum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mcarry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnew_discounted_sum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/numpy/array_methods.py\u001b[0m in \u001b[0;36mop\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    741\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_forward_operator_to_aval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 743\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"_{name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    744\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/numpy/array_methods.py\u001b[0m in \u001b[0;36mdeferring_binary_op\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[0;31m# subclasses, e.g. NamedTuple objects that may override operators.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_rejected_binop_types\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m       raise TypeError(f\"unsupported operand type(s) for {opchar}: \"\n\u001b[0m\u001b[1;32m    276\u001b[0m                       f\"{type(args[0]).__name__!r} and {type(args[1]).__name__!r}\")\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'list' and 'DynamicJaxprTracer'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    def _discount_rewards(carry, reward):\n",
        "        running_add, discounted_rewards = carry\n",
        "        running_add = reward + config[\"GAMMA\"] * running_add\n",
        "        discounted_rewards = jax.ops.index_update(discounted_rewards, jax.ops.index[i], running_add)\n",
        "        return (running_add, discounted_rewards), (running_add, discounted_rewards)\n",
        "\n",
        "#     def policy_step(state_input, tmp):\n",
        "#         \"\"\"lax.scan compatible step transition in jax env.\"\"\"\n",
        "#         obs, state, policy_params, rng = state_input\n",
        "#         rng, rng_step, rng_net = jax.random.split(rng, 3)\n",
        "#         action = model.apply(policy_params, obs, rng_net)\n",
        "#         next_obs, next_state, reward, done, _ = env.step(\n",
        "#           rng_step, state, action, env_params\n",
        "#         )\n",
        "#         carry = [next_obs, next_state, policy_params, rng]\n",
        "#         return carry, [obs, action, reward, next_obs, done]"
      ],
      "metadata": {
        "id": "MsWem5etsoYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _calculate_discounted_rewards(traj_batch, last_val):\n",
        "    def _discount_rewards(carry, reward):\n",
        "        running_add, discounted_rewards = carry\n",
        "        running_add = reward + config[\"GAMMA\"] * running_add\n",
        "        discounted_rewards = jax.ops.index_update(discounted_rewards, jax.ops.index[i], running_add)\n",
        "        return (running_add, discounted_rewards), (running_add, discounted_rewards)\n",
        "\n",
        "    init_carry = (last_val, jnp.zeros_like(traj_batch.reward))\n",
        "\n",
        "    _, (_, discounted_rewards) = jax.lax.scan(\n",
        "        _discount_rewards,\n",
        "        init_carry,\n",
        "        traj_batch.reward,\n",
        "        reverse=True\n",
        "    )\n",
        "\n",
        "    return discounted_rewards, discounted_rewards + last_val"
      ],
      "metadata": {
        "id": "aOupviQZtIqZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}