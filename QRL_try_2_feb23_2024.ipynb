{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMAUwWHuhZTr7FCpXtThOqf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zakuta/D-QRL/blob/main/QRL_try_2_feb23_2024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1fTvAQcbQBT"
      },
      "outputs": [],
      "source": [
        "# !pip install equinox\n",
        "# !pip install tensorcircuit\n",
        "# !pip install -U qiskit\n",
        "# !pip install tensorcircuit\n",
        "# !pip install cirq\n",
        "# !pip install openfermion\n",
        "# !pip install gymnax\n",
        "# !pip install brax\n",
        "# !pip install distrax"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "from jax import config\n",
        "\n",
        "config.update(\"jax_debug_nans\", True)\n",
        "config.update(\"jax_enable_x64\", True)\n",
        "\n",
        "import jax.numpy as jnp\n",
        "DTYPE=jnp.float64\n",
        "\n",
        "import chex\n",
        "import numpy as np\n",
        "import optax\n",
        "from flax import struct\n",
        "from functools import partial\n",
        "import tensorcircuit as tc\n",
        "\n",
        "import tensorflow as tf\n",
        "from sklearn.decomposition import PCA\n",
        "import equinox as eqx\n",
        "import types\n",
        "from jaxtyping import Array, PRNGKeyArray\n",
        "from typing import Union, Sequence, List, NamedTuple, Optional, Tuple, Any, Literal, TypeVar\n",
        "import jax.tree_util as jtu\n",
        "import gymnax\n",
        "import distrax\n",
        "from gymnax.environments import environment, spaces\n",
        "from brax import envs\n",
        "from brax.envs.wrappers.training import EpisodeWrapper, AutoResetWrapper\n",
        "\n",
        "K = tc.set_backend(\"jax\")"
      ],
      "metadata": {
        "id": "pPpDluwWbm-a"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# shamelessly taken from purejaxrl: https://github.com/luchris429/purejaxrl/blob/main/purejaxrl/wrappers.py\n",
        "\n",
        "class GymnaxWrapper(object):\n",
        "    \"\"\"Base class for Gymnax wrappers.\"\"\"\n",
        "\n",
        "    def __init__(self, env):\n",
        "        self._env = env\n",
        "\n",
        "    # provide proxy access to regular attributes of wrapped object\n",
        "    def __getattr__(self, name):\n",
        "        return getattr(self._env, name)\n",
        "\n",
        "\n",
        "class FlattenObservationWrapper(GymnaxWrapper):\n",
        "    \"\"\"Flatten the observations of the environment.\"\"\"\n",
        "\n",
        "    def __init__(self, env: environment.Environment):\n",
        "        super().__init__(env)\n",
        "\n",
        "    def observation_space(self, params) -> spaces.Box:\n",
        "        assert isinstance(\n",
        "            self._env.observation_space(params), spaces.Box\n",
        "        ), \"Only Box spaces are supported for now.\"\n",
        "        return spaces.Box(\n",
        "            low=self._env.observation_space(params).low,\n",
        "            high=self._env.observation_space(params).high,\n",
        "            shape=(np.prod(self._env.observation_space(params).shape),),\n",
        "            dtype=self._env.observation_space(params).dtype,\n",
        "        )\n",
        "\n",
        "    @partial(jax.jit, static_argnums=(0,))\n",
        "    def reset(\n",
        "        self, key: chex.PRNGKey, params: Optional[environment.EnvParams] = None\n",
        "    ) -> Tuple[chex.Array, environment.EnvState]:\n",
        "        obs, state = self._env.reset(key, params)\n",
        "        obs = jnp.reshape(obs, (-1,))\n",
        "        return obs, state\n",
        "\n",
        "    @partial(jax.jit, static_argnums=(0,))\n",
        "    def step(\n",
        "        self,\n",
        "        key: chex.PRNGKey,\n",
        "        state: environment.EnvState,\n",
        "        action: Union[int, float],\n",
        "        params: Optional[environment.EnvParams] = None,\n",
        "    ) -> Tuple[chex.Array, environment.EnvState, float, bool, dict]:\n",
        "        obs, state, reward, done, info = self._env.step(key, state, action, params)\n",
        "        obs = jnp.reshape(obs, (-1,))\n",
        "        return obs, state, reward, done, info\n",
        "\n",
        "\n",
        "@struct.dataclass\n",
        "class LogEnvState:\n",
        "    env_state: environment.EnvState\n",
        "    episode_returns: float\n",
        "    episode_lengths: int\n",
        "    returned_episode_returns: float\n",
        "    returned_episode_lengths: int\n",
        "    timestep: int\n",
        "\n",
        "\n",
        "class LogWrapper(GymnaxWrapper):\n",
        "    \"\"\"Log the episode returns and lengths.\"\"\"\n",
        "\n",
        "    def __init__(self, env: environment.Environment):\n",
        "        super().__init__(env)\n",
        "\n",
        "    @partial(jax.jit, static_argnums=(0,))\n",
        "    def reset(\n",
        "        self, key: chex.PRNGKey, params: Optional[environment.EnvParams] = None\n",
        "    ) -> Tuple[chex.Array, environment.EnvState]:\n",
        "        obs, env_state = self._env.reset(key, params)\n",
        "        state = LogEnvState(env_state, 0, 0, 0, 0, 0)\n",
        "        return obs, state\n",
        "\n",
        "    @partial(jax.jit, static_argnums=(0,))\n",
        "    def step(\n",
        "        self,\n",
        "        key: chex.PRNGKey,\n",
        "        state: environment.EnvState,\n",
        "        action: Union[int, float],\n",
        "        params: Optional[environment.EnvParams] = None,\n",
        "    ) -> Tuple[chex.Array, environment.EnvState, float, bool, dict]:\n",
        "        obs, env_state, reward, done, info = self._env.step(\n",
        "            key, state.env_state, action, params\n",
        "        )\n",
        "        new_episode_return = state.episode_returns + reward\n",
        "        new_episode_length = state.episode_lengths + 1\n",
        "        state = LogEnvState(\n",
        "            env_state=env_state,\n",
        "            episode_returns=new_episode_return * (1 - done),\n",
        "            episode_lengths=new_episode_length * (1 - done),\n",
        "            returned_episode_returns=state.returned_episode_returns * (1 - done)\n",
        "            + new_episode_return * done,\n",
        "            returned_episode_lengths=state.returned_episode_lengths * (1 - done)\n",
        "            + new_episode_length * done,\n",
        "            timestep=state.timestep + 1,\n",
        "        )\n",
        "        info[\"returned_episode_returns\"] = state.returned_episode_returns\n",
        "        info[\"returned_episode_lengths\"] = state.returned_episode_lengths\n",
        "        info[\"timestep\"] = state.timestep\n",
        "        info[\"returned_episode\"] = done\n",
        "        return obs, state, reward, done, info"
      ],
      "metadata": {
        "id": "eEXoWKLccJjF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reuploading_circuit(n_qubits, n_layers, rot_params, input_params, X):\n",
        "  circuit = tc.Circuit(n_qubits)\n",
        "  # params = np.random.normal(size=(n_layers + 1, n_qubits, 3))\n",
        "  # inputs = np.random.normal(size=(n_layers, n_qubits))\n",
        "\n",
        "  for l in range(n_layers):\n",
        "    # variational part\n",
        "    for qubit_idx in range(n_qubits):\n",
        "      circuit.rx(qubit_idx, theta=rot_params[l, qubit_idx, 0])\n",
        "      circuit.ry(qubit_idx, theta=rot_params[l, qubit_idx, 1])\n",
        "      circuit.rz(qubit_idx, theta=rot_params[l, qubit_idx, 2])\n",
        "\n",
        "    # entangling part\n",
        "    for qubit_idx in range(n_qubits - 1):\n",
        "      circuit.cnot(qubit_idx, qubit_idx + 1)\n",
        "    if n_qubits != 2:\n",
        "      circuit.cnot(n_qubits - 1, 0)\n",
        "\n",
        "    # encoding part\n",
        "    for qubit_idx in range(n_qubits):\n",
        "      input = X[qubit_idx] * input_params[l, qubit_idx]\n",
        "      circuit.rx(qubit_idx, theta=input)\n",
        "\n",
        "  # last variational part\n",
        "  for qubit_idx in range(n_qubits):\n",
        "    circuit.rx(qubit_idx, theta=rot_params[n_layers, qubit_idx, 0])\n",
        "    circuit.ry(qubit_idx, theta=rot_params[n_layers, qubit_idx, 1])\n",
        "    circuit.rz(qubit_idx, theta=rot_params[n_layers, qubit_idx, 2])\n",
        "\n",
        "  return circuit\n",
        "\n",
        "\n",
        "class PQCLayer(eqx.Module):\n",
        "  theta: Array\n",
        "  lmbd: Array\n",
        "  n_qubits: int = eqx.field(static=True)\n",
        "  n_layers: int = eqx.field(static=True)\n",
        "\n",
        "  def __init__(self, n_qubits: int, n_layers: int, params: Optional, key: PRNGKeyArray):\n",
        "\n",
        "    key = jax.random.PRNGKey(key)\n",
        "    tkey, lkey = jax.random.split(key, num=2)\n",
        "\n",
        "    if params is None:\n",
        "      self.theta = params['thetas']\n",
        "      self.lmbd = params['lmbds']\n",
        "    else:\n",
        "      self.theta = jax.random.uniform(key=tkey, shape=(n_layers + 1, n_qubits, 3),\n",
        "                                    minval=0.0, maxval=np.pi, dtype=DTYPE)\n",
        "      self.lmbd = jnp.ones(shape=(n_layers, n_qubits), dtype=DTYPE)\n",
        "\n",
        "    self.n_qubits = n_qubits\n",
        "    self.n_layers = n_layers\n",
        "\n",
        "  def __call__(self, inputs):\n",
        "    circuit = tc.Circuit(self.n_qubits)\n",
        "\n",
        "    for l in range(self.n_layers):\n",
        "      # variational part\n",
        "      for qubit_idx in range(self.n_qubits):\n",
        "        circuit.rx(qubit_idx, theta=self.theta[l, qubit_idx, 0])\n",
        "        circuit.ry(qubit_idx, theta=self.theta[l, qubit_idx, 1])\n",
        "        circuit.rz(qubit_idx, theta=self.theta[l, qubit_idx, 2])\n",
        "\n",
        "      # entangling part\n",
        "      for qubit_idx in range(self.n_qubits - 1):\n",
        "        circuit.cnot(qubit_idx, qubit_idx + 1)\n",
        "      if self.n_qubits != 2:\n",
        "        circuit.cnot(self.n_qubits - 1, 0)\n",
        "\n",
        "      # encoding part\n",
        "      for qubit_idx in range(self.n_qubits):\n",
        "        linear_input = inputs[qubit_idx] * self.lmbd[l, qubit_idx]\n",
        "        circuit.rx(qubit_idx, theta=linear_input)\n",
        "\n",
        "    # last variational part\n",
        "    for qubit_idx in range(self.n_qubits):\n",
        "      circuit.rx(qubit_idx, theta=self.theta[self.n_layers, qubit_idx, 0])\n",
        "      circuit.ry(qubit_idx, theta=self.theta[self.n_layers, qubit_idx, 1])\n",
        "      circuit.rz(qubit_idx, theta=self.theta[self.n_layers, qubit_idx, 2])\n",
        "\n",
        "    return jnp.real(circuit.expectation_ps(z=jnp.arange(len(self.n_qubits))))\n",
        "\n",
        "\n",
        "# class PQCLayer(eqx.Module):\n",
        "#   theta: jax.Array = eqx.field(converter=jnp.asarray)\n",
        "#   lmbd: jax.Array = eqx.field(converter=jnp.asarray)\n",
        "#   n_qubits: int = eqx.field(static=True)\n",
        "#   n_layers: int = eqx.field(static=True)\n",
        "\n",
        "#   def __init__(self, n_qubits: int, n_layers: int, key: int):\n",
        "#     key = jax.random.PRNGKey(key)\n",
        "#     tkey, lkey = jax.random.split(key, num=2)\n",
        "#     self.n_qubits = n_qubits\n",
        "#     self.n_layers = n_layers\n",
        "#     # rotation_params\n",
        "#     self.theta = jax.random.uniform(key=tkey, shape=(n_layers + 1, n_qubits, 3),\n",
        "#                                     minval=0.0, maxval=np.pi, dtype=DTYPE)\n",
        "#     # input encoding params\n",
        "#     # self.lmbd = jnp.ones(shape=(n_layers, n_qubits))\n",
        "#     self.lmbd = jax.random.uniform(key=lkey, shape=(n_layers, n_qubits),\n",
        "#                                     minval=0.0, maxval=np.pi, dtype=DTYPE)\n",
        "\n",
        "#   def __call__(self, inputs):\n",
        "#   # def __call__(self, X, n_qubits, depth):\n",
        "\n",
        "#     circuit = generate_circuit(self.n_qubits, self.n_layers, self.theta, self.lmbd, inputs)\n",
        "#     # state = circuit.state()\n",
        "#     # return state\n",
        "#     return K.real(circuit.expectation_ps(z=[0,1,2,3]))\n",
        "\n",
        "# class Alternating(eqx.Module):\n",
        "#   w: jax.Array = eqx.field(converter=jnp.asarray)\n",
        "\n",
        "#   def __init__(self, output_dim):\n",
        "#     self.w = jnp.array([[(-1.) ** i for i in range(output_dim)]])\n",
        "\n",
        "#   def __call__(self, inputs):\n",
        "#     return jnp.matmul(inputs, self.w)\n",
        "\n",
        "\n",
        "# class Actor(eqx.Module):\n",
        "#   n_qubits: int\n",
        "#   n_layers: int\n",
        "#   beta: float\n",
        "#   n_actions: Sequence[int]\n",
        "#   key: int\n",
        "\n",
        "#   def __call__(self, x):\n",
        "#     re_uploading_pqc = PQCLayer(n_qubits=self.n_qubits,\n",
        "#                                 n_layers=self.n_layers,\n",
        "#                                 key=self.key)(x)\n",
        "\n",
        "#     process = eqx.nn.Sequential([\n",
        "#         Alternating(self.n_actions),\n",
        "#         eqx.nn.Lambda(lambda x: x * self.beta),\n",
        "#         jax.nn.softmax()\n",
        "#     ])\n",
        "\n",
        "#     policy = process(re_uploading_pqc)\n",
        "\n",
        "#     return policy\n",
        "\n",
        "\n",
        "\n",
        "class QuantumActor(eqx.Module):\n",
        "  theta: jax.Array # trainable\n",
        "  lmbd: jax.Array # trainable\n",
        "  w: jax.Array # trainable\n",
        "  n_qubits: int = eqx.field(static=True)\n",
        "  n_layers: int = eqx.field(static=True)\n",
        "  beta: float = eqx.field(static=True)\n",
        "  n_actions: Sequence[int] = eqx.field(static=True)\n",
        "  # key: int\n",
        "\n",
        "  def __init__(self, n_qubits, n_layers, beta, n_actions, params: Optional, key = 42):\n",
        "\n",
        "    key = jax.random.PRNGKey(key)\n",
        "    key, _key = jax.random.split(key, num=2)\n",
        "\n",
        "    if params is None:\n",
        "      # rotation_params\n",
        "      self.theta = params['thetas']\n",
        "      # input encoding params\n",
        "      self.lmbd = params['lmbds']\n",
        "      # observable weights\n",
        "      self.w = params['ws']\n",
        "    else:\n",
        "      self.theta = jax.random.uniform(key=key, shape=(n_layers + 1, n_qubits, 3),\n",
        "                                    minval=0.0, maxval=np.pi, dtype=DTYPE)\n",
        "      self.lmbd = jnp.ones(shape=(n_layers, n_qubits), dtype=DTYPE)\n",
        "      self.w = jnp.array([[(-1.) ** i for i in range(n_actions)]])\n",
        "\n",
        "\n",
        "    self.n_qubits = n_qubits\n",
        "    self.n_layers = n_layers\n",
        "    self.beta = beta\n",
        "    self.n_actions = n_actions\n",
        "\n",
        "  def quantum_policy_circuit(self, inputs):\n",
        "\n",
        "    # this can be any PQC of the user's choice. hence, I made the decision to make a separate function within this class\n",
        "    circuit = reuploading_circuit(self.n_qubits, self.n_layers, self.theta, self.lmbd, inputs)\n",
        "\n",
        "    return K.real(circuit.expectation_ps(z=np.arange(self.n_qubits)))\n",
        "\n",
        "  def alternating(self, inputs):\n",
        "    return jnp.matmul(inputs, self.w)\n",
        "\n",
        "  def get_params(self):\n",
        "    return {\"theta\": self.theta, \"lmbd\": self.lmbd, \"w\": self.w}\n",
        "\n",
        "  def __call__(self, x):\n",
        "\n",
        "    pqc = self.quantum_policy_circuit(x)\n",
        "    alt = self.alternating(pqc)\n",
        "\n",
        "    # process = eqx.nn.Sequential([\n",
        "    #     alt,\n",
        "    #     eqx.nn.Lambda(lambda x: x * self.beta),\n",
        "    #     jax.nn.softmax()\n",
        "    # ])\n",
        "    # policy = process(pqc)\n",
        "\n",
        "    actor_mean = eqx.nn.Lambda(lambda x: x * self.beta)(alt)\n",
        "    # policy = jax.nn.softmax(actor_mean)\n",
        "    policy = distrax.Softmax(actor_mean)\n",
        "\n",
        "    return policy\n",
        "\n",
        "\n",
        "# class Actor(eqx.Module):\n",
        "#   n_qubits: int\n",
        "#   n_layers: int\n",
        "#   beta: float\n",
        "#   n_actions: Sequence[int]\n",
        "#   pqc: eqx.Module\n",
        "#   alt: eqx.Module\n",
        "#   key: int\n",
        "\n",
        "#   def __init__(self, n_qubits, n_layers, beta, n_actions, key):\n",
        "#     self.n_qubits = n_qubits\n",
        "#     self.n_layers = n_layers\n",
        "#     self.beta = beta\n",
        "#     self.n_actions = n_actions\n",
        "#     self.key = key\n",
        "\n",
        "#     self.pqc = PQCLayer(n_qubits=self.n_qubits,\n",
        "#                         n_layers=self.n_layers,\n",
        "#                         key=self.key)\n",
        "\n",
        "#     self.alt = Alternating(self.n_actions)\n",
        "\n",
        "#   def __call__(self, x):\n",
        "#     re_uploading_pqc = self.pqc(x)\n",
        "\n",
        "#     process = eqx.nn.Sequential([\n",
        "#         self.alt,\n",
        "#         eqx.nn.Lambda(lambda x: x * self.beta),\n",
        "#         jax.nn.softmax()\n",
        "#     ])\n",
        "\n",
        "#     policy = process(re_uploading_pqc)\n",
        "\n",
        "#     return policy\n",
        "\n",
        "\n",
        "class TrainState(eqx.Module):\n",
        "    model: eqx.Module\n",
        "    optimizer: optax.GradientTransformation = eqx.field(static=True)\n",
        "    opt_state: optax.OptState\n",
        "\n",
        "    def __init__(self, model, optimizer, opt_state):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.opt_state = opt_state\n",
        "\n",
        "    def apply_updates_to_model(self, new_params):\n",
        "      # this function is specific and works for this example only. one can think of\n",
        "      # generalizing it to work for updating any given attribute/params of the model.\n",
        "\n",
        "      model_new = eqx.tree_at(where=lambda model: model.theta, pytree=self.model, replace=new_params['thetas'])\n",
        "      model_new = eqx.tree_at(where=lambda model: model.lmbd, pytree=model_new, replace=new_params['lmbds'])\n",
        "      model_new = eqx.tree_at(where=lambda model: model.w, pytree=model_new, replace=new_params['ws'])\n",
        "\n",
        "      return model_new\n",
        "\n",
        "    def apply_gradients(self, params, grads):\n",
        "      grads = {'thetas': grads.theta, 'lmbds': grads.lmbd, 'ws': grads.w}\n",
        "      # if type(grads) == dict: # this is if we actually apply value_and_grad functionality of JAX, might be useful if we apply PPO for instance\n",
        "      #   grads = {'thetas': grads.theta, 'lmbds': grads.lmbd, 'ws': grads.w}\n",
        "      # else: # for monte-carlo estimation of grads which REINFORCE applies\n",
        "      #   grads = grads\n",
        "      updates, opt_state = self.optimizer.update(grads, self.opt_state, params)\n",
        "      new_params = optax.apply_updates(params, updates)\n",
        "      model_new = self.apply_updates_to_model(new_params)\n",
        "      # model = eqx.apply_updates(self.model, updates)\n",
        "      new_train_state = self.__class__(model=model_new, optimizer=self.optimizer, opt_state=opt_state)\n",
        "      return new_train_state\n",
        "\n",
        "class Transition(NamedTuple):\n",
        "  done: jnp.ndarray\n",
        "  action: jnp.ndarray\n",
        "  reward: jnp.ndarray\n",
        "  log_prob: jnp.ndarray\n",
        "  obs: jnp.ndarray\n",
        "  info: jnp.ndarray"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "1ZofBx0yccWb",
        "outputId": "4a6c9b2e-6392-47ce-fce6-fcb0801fe874"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'eqx' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-5e589adfd3ac>\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mPQCLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meqx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m   \u001b[0mtheta\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m   \u001b[0mlmbd\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'eqx' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conf = {'n_layers': 5,\n",
        "        'n_qubits': 4,\n",
        "        'beta': 1.0,\n",
        "        'n_envs': 2,\n",
        "        'total_timesteps': 250000,\n",
        "        'n_steps': 64,\n",
        "        'gamma': 0.99,\n",
        "        'n_minibatches': 4,\n",
        "        'update_epochs': 4,\n",
        "        'debug': True,\n",
        "        'env_name': 'Cartpole-v1',\n",
        "        'lr_theta': 0.001,\n",
        "        'lr_lmbd': 0.1,\n",
        "        'lr_w': 0.1,\n",
        "        'rng': 42}\n",
        "\n",
        "\n",
        "def main_train(config):\n",
        "\n",
        "  conf['n_updates'] = (\n",
        "          conf['total_timesteps'] // conf['n_steps'] // conf['n_envs']\n",
        "      )\n",
        "\n",
        "  conf['mini_batchsize'] = (\n",
        "          conf['n_envs'] * conf['n_steps'] // conf['n_minibatches']\n",
        "      )\n",
        "\n",
        "  # n_layers = 5\n",
        "  # n_qubits = 4\n",
        "  # beta = 1.0\n",
        "\n",
        "  # n_envs = 2\n",
        "  # # batch_size = 10\n",
        "  # n_steps = 10\n",
        "  # gamma = 0.99\n",
        "  # mini_batch_size = 10\n",
        "  # n_mini_batches = 10\n",
        "  # update_epochs = 100\n",
        "  # debug = True\n",
        "  # n_updates = 1000\n",
        "  env, env_params = gymnax.make(conf['env_name'])\n",
        "  env = FlattenObservationWrapper(env)\n",
        "  env = LogWrapper(env)\n",
        "\n",
        "  n_actions = env.action_space(env_params).n\n",
        "\n",
        "  rng = jax.random.PRNGKey(conf['rng'])\n",
        "  rng, _rng = jax.random.split(rng)\n",
        "  params = {'thetas': jax.random.uniform(\n",
        "      key=_rng, shape=(conf['n_layers'] + 1, conf['n_qubits'], 3),\n",
        "      minval=0.0, maxval=np.pi, dtype=DTYPE\n",
        "      ),\n",
        "            'lmbds': jnp.ones(shape=(conf['n_layers'],\n",
        "                                    conf['n_qubits']), dtype=DTYPE\n",
        "                              ),\n",
        "            'ws': jnp.array([[(-1.) ** i for i in range(n_actions)]], dtype=DTYPE)\n",
        "            }\n",
        "\n",
        "  def train(rng):\n",
        "  actor = QuantumActor(\n",
        "      n_qubits=conf['n_qubits'], n_layers=conf['n_layers'],\n",
        "      beta=conf['beta'], n_actions=n_actions, params=params\n",
        "      )\n",
        "\n",
        "  def map_nested_fn(fn):\n",
        "    '''Recursively apply `fn` to the key-value pairs of a nested dict'''\n",
        "    def map_fn(nested_dict):\n",
        "      return {k: (map_fn(v) if isinstance(v, dict) else fn(k, v))\n",
        "              for k, v in nested_dict.items()}\n",
        "    return map_fn\n",
        "\n",
        "  label_fn = map_nested_fn(lambda k, _: k)\n",
        "  optim = optax.multi_transform({'thetas': optax.amsgrad(conf['lr_theta']),\n",
        "                                'lmbds': optax.amsgrad(conf['lr_lmbd']),\n",
        "                                'ws': optax.amsgrad(conf['lr_w'])},\n",
        "                            label_fn)\n",
        "\n",
        "  # optim = closure_to_pytree(optim)\n",
        "  opt_state = optim.init(params)\n",
        "\n",
        "  train_state = TrainState(model=actor, optimizer=optim, opt_state=opt_state)\n",
        "\n",
        "  rng, _rng = jax.random.split(rng)\n",
        "  reset_rng = jax.random.split(_rng, conf['n_envs'])\n",
        "  obs, env_state = jax.vmap(env.reset, in_axes=(0, None))(reset_rng, env_params)\n",
        "\n",
        "  def _update_step(runner_state, unused):\n",
        "    # COLLECT TRAJECTORIES\n",
        "    def _env_step(runner_state, ununsed):\n",
        "      train_state, env_state, last_obs, rng = runner_state\n",
        "\n",
        "      rng, _rng = jax.random.split(rng)\n",
        "      actor = train_state.model\n",
        "      policy = actor(last_obs)\n",
        "      action = policy.sample(seed=_rng)\n",
        "      log_prob = policy.log_prob(action)\n",
        "      # action_probs = actor(last_obs)\n",
        "      # action = jax.random.choice(key=_rng, a=n_actions, p=action_probs)\n",
        "\n",
        "      rng, _rng = jax.random.split(rng)\n",
        "      rng_step = jax.random.split(_rng, conf['n_envs'])\n",
        "      obs, env_state, reward, done, info = jax.vmap(\n",
        "          env.step, in_axes=(0, 0, 0, None)\n",
        "          )(rng_step, env_state, action, env_params)\n",
        "\n",
        "      transition = Transition(\n",
        "          done, action, reward, log_prob, last_obs, info)\n",
        "\n",
        "      runner_state = (train_state, env_state, obs, rng)\n",
        "\n",
        "      return runner_state, transition\n",
        "\n",
        "    train_state, traj_batch = jax.lax.scan(_env_step, runner_state, None, conf['n_steps'])\n",
        "\n",
        "    train_state, env_state, last_obs, rng = runner_state\n",
        "\n",
        "    # def _calculate_discounted_rewards(rewards, dones, gamma):\n",
        "    #   def scan_fn(carry, inputs):\n",
        "    #     discounted_rewards, running_add = carry\n",
        "    #     reward, done = inputs\n",
        "    #     running_add = running_add * gamma * (1 - done) + reward\n",
        "    #     return (discounted_rewards.at[t].set(running_add), running_add)\n",
        "\n",
        "    #   n = len(rewards)\n",
        "    #   initial_state = (jnp.zeros_like(rewards), 0.0)\n",
        "    #   _, discounted_rewards = jax.lax.scan(scan_fn, initial_state, (rewards, dones))\n",
        "    #   return discounted_rewards\n",
        "\n",
        "    # def _calculate_discounted_rewards(rewards, dones, gamma):\n",
        "    #   def scan_fn(carry, inputs):\n",
        "    #       discounted_rewards, running_add, t = carry\n",
        "    #       reward, done = inputs\n",
        "    #       running_add = running_add * gamma * (1 - done) + reward\n",
        "    #       return (discounted_rewards.at[t].set(running_add), running_add, t + 1)\n",
        "\n",
        "    #   n = len(rewards)\n",
        "    #   initial_state = (jnp.zeros_like(rewards), 0.0, 0)\n",
        "    #   _, discounted_rewards, _ = jax.lax.scan(scan_fn, initial_state, (rewards, dones))\n",
        "    #   return discounted_rewards\n",
        "\n",
        "    def _calculate_discounted_rewards(rewards, dones, gamma):\n",
        "      def scan_fn(carry, inputs):\n",
        "        discounted_rewards, running_add = carry\n",
        "        reward, done = inputs\n",
        "        running_add = running_add * gamma * (1 - done) + reward\n",
        "        return (discounted_rewards.at[len(discounted_rewards) - 1].set(running_add), running_add)\n",
        "\n",
        "      n = len(rewards)\n",
        "      initial_state = (jnp.zeros_like(rewards), 0.0)\n",
        "      _, discounted_rewards = jax.lax.scan(scan_fn, initial_state, (rewards, dones))\n",
        "      return discounted_rewards\n",
        "\n",
        "    rewards = traj_batch.reward\n",
        "    dones = traj_batch.done\n",
        "    discounted_rewards = _calculate_discounted_rewards(rewards, dones, gamma)\n",
        "\n",
        "    # UPDATE ACTOR\n",
        "    def _update_epoch(update_state, unused):\n",
        "      def _update_minbatch(train_state, batch_info):\n",
        "        traj_batch, discounted_rewards = batch_info\n",
        "\n",
        "        @eqx.filter_value_and_grad\n",
        "        def _loss_fn(traj_batch):\n",
        "          #TODO: can I use vmap here?\n",
        "          # RERUN ACTOR\n",
        "          actor = train_state.model\n",
        "          policy = actor(traj_batch.obs)\n",
        "          log_prob = policy.log_prob(traj_batch.action)\n",
        "          # for stability while training and less variability\n",
        "          returns = (discounted_rewards - jnp.mean(discounted_rewards)) / (jnp.std(discounted_rewards) + 1e-8)\n",
        "          loss = -jnp.mean(log_prob * returns)\n",
        "\n",
        "          return loss\n",
        "\n",
        "        loss, grads = _loss_fn(traj_batch)\n",
        "        train_state = train_state.apply_gradients(train_state.model.get_params, grads)\n",
        "\n",
        "        return train_state, loss\n",
        "\n",
        "      train_state, traj_batch, discounted_rewards, rng = update_state\n",
        "      rng, _rng = jax.random.split(rng)\n",
        "\n",
        "      # Mini-batch Updates\n",
        "      batch_size = conf['mini_batchsize'] * conf['n_minibatches']\n",
        "      assert (batch_size == conf['n_steps'] * conf['n_envs']\n",
        "      ), 'batch size must be equal to number of steps * number of envs'\n",
        "      permutation = jax.random.permutation(_rng, batch_size)\n",
        "      batch = (traj_batch, discounted_rewards)\n",
        "      batch = jax.tree_util.tree_map(\n",
        "                      lambda x: x.reshape((batch_size,) + x.shape[2:]), batch)\n",
        "      shuffled_batch = jax.tree_util.tree_map(\n",
        "                      lambda x: jnp.take(x, permutation, axis=0), batch\n",
        "                  )\n",
        "\n",
        "      minibatches = jax.tree_util.tree_map(\n",
        "          lambda x: jnp.reshape(\n",
        "              x, [conf['n_minibatches'], -1] + list(x.shape[1:])\n",
        "          ),\n",
        "          shuffled_batch,\n",
        "      )\n",
        "\n",
        "      train_state, loss = jax.lax.scan(\n",
        "          _update_minbatch, train_state, minibatches\n",
        "      )\n",
        "      update_state = (train_state, discounted_rewards, rng)\n",
        "      return update_state, loss\n",
        "\n",
        "    # Updating training state and metrics\n",
        "    update_state = (train_state, traj_batch, discounted_rewards, rng)\n",
        "    update_state, loss_info = jax.lax.scan(\n",
        "        _update_epoch, update_state, None, conf['update_epochs']\n",
        "        )\n",
        "    train_state = update_state[0]\n",
        "    metric = traj_batch.info\n",
        "    rng = update_state[-1]\n",
        "\n",
        "    # Debugging mode\n",
        "    if debug:\n",
        "      def callback(info):\n",
        "        return_values = info['returned_episode_returns'][info['returned_episode']]\n",
        "        timesteps = info['timestep'][info['returned_episode']] * conf['n_envs']\n",
        "        for t in range(len(timesteps)):\n",
        "          print(f\"global step={timesteps[t]}, episodic return={return_values[t]}\")\n",
        "      jax.debug.callback(callback, metric)\n",
        "\n",
        "    runner_state = (train_state, env_state, last_obs, rng)\n",
        "\n",
        "    return runner_state, metric\n",
        "\n",
        "  rng, _rng = jax.random.split(rng)\n",
        "  runner_state = (train_state, env_state, obs, _rng)\n",
        "  runner_state, metric = jax.lax.scan(\n",
        "      _update_step, runner_state, None, conf['n_updates']\n",
        "      )\n",
        "  return {'runner_state': runner_state, 'metrics': metric}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "OiOjp2XJrQZ0",
        "outputId": "97dd569e-66c9-45fe-d6c7-004b49658950"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'gymnax' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-e1d126859ce1>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mupdate_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mdebug\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgymnax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CartPole-v1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFlattenObservationWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'gymnax' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_"
      ],
      "metadata": {
        "id": "HVhMAzG4sgjS"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NqTp5XhMsoMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nP8th2o6xLm7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}